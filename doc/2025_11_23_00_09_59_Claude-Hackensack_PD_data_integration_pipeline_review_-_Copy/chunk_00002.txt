'))\n        self.exports_path = self.base_path / self.config.get('exports_directory', '05_EXPORTS')\n        self.output_path = self.base_path / self.config.get('output_directory', 'output')\n        self.reports_path = self.base_path / self.config.get('reports_directory', 'reports')\n        \n        # Ensure output directories exist\n        self.output_path.mkdir(parents=True, exist_ok=True)\n        self.reports_path.mkdir(parents=True, exist_ok=True)\n        \n        # Integration statistics\n        self.stats = {\n            'cad_records_loaded': 0,\n            'rms_records_loaded': 0,\n            'successful_matches': 0,\n            'failed_matches': 0,\n            'cad_only_records': 0,\n            'rms_only_records': 0,\n            'quality_fixes_applied': 0,\n            'processing_errors': 0,\n            'elapsed_time': 0,\n        }\n        \n        # Load schema registry\n        # Replace with actual schema mapper once implemented\n        # self.schema_mapper = SchemaMapper(self.schema_registry_path)\n        \n        logger.info(f\"Initialized integration pipeline with config: {self.config_path}\")\n    \n    def _load_config(self) -> Dict:\n        \"\"\"Load configuration from YAML file.\"\"\"\n        if not self.config_path.exists():\n            raise FileNotFoundError(f\"Configuration file not found: {self.config_path}\")\n        \n        try:\n            with open(self.config_path, 'r') as f:\n                config = yaml.safe_load(f)\n            return config\n        except Exception as e:\n            raise ValueError(f\"Failed to load configuration: {str(e)}\")\n    \n    def load_cad_data(self) -> pd.DataFrame:\n        \"\"\"Load and preprocess CAD data from configured sources.\"\"\"\n        logger.info(\"Loading CAD data...\")\n        \n        cad_files = []\n        for pattern in self.config.get('cad_file_patterns', ['*_CAD*.xlsx']):\n            cad_files.extend(list(self.exports_path.glob(pattern)))\n        \n        logger.info(f\"Found {len(cad_files)} CAD files: {[f.name for f in cad_files]}\")\n        \n        exclude_patterns = self.config.get('exclude_patterns', ['*CORRUPTED*', '*OLD*'])\n        for pattern in exclude_patterns:\n            cad_files = [f for f in cad_files if not f.match(pattern)]\n        \n        logger.info(f\"After exclusions: {len(cad_files)} CAD files\")\n        \n        combined_cad = []\n        \n        for file_path in sorted(cad_files):\n            try:\n                logger.info(f\"Processing {file_path.name}...\")\n                \n                # Use chunking for large files\n                chunk_size = self.config.get('chunk_size', 10000)\n                if 'chunkable_files' in self.config and any(file_path.match(pattern) for pattern in self.config['chunkable_files']):\n                    # Process in chunks\n                    chunks = []\n                    for chunk_idx, chunk in enumerate(pd.read_excel(file_path, engine='openpyxl', chunksize=chunk_size)):\n                        logger.info(f\"Processing chunk {chunk_idx+1} with {len(chunk)} rows\")\n                        processed_chunk = self._preprocess_cad_chunk(chunk, file_path.name)\n                        chunks.append(processed_chunk)\n                        \n                        # Force garbage collection\n                        del processed_chunk\n                        gc.collect()\n                    \n                    file_df = pd.concat(chunks, ignore_index=True)\n                else:\n                    # Process entire file at once\n                    df = pd.read_excel(file_path, engine='openpyxl')\n                    file_df = self._preprocess_cad_chunk(df, file_path.name)\n                \n                combined_cad.append(file_df)\n                logger.info(f\"Loaded {len(file_df):,} CAD records from {file_path.name}\")\n                \n                # Force garbage collection\n                del file_df\n                gc.collect()\n                \n            except Exception as e:\n                logger.error(f\"Error processing CAD file {file_path.name}: {str(e)}\")\n                self.stats['processing_errors'] += 1\n        \n        if combined_cad:\n            cad_data = pd.concat(combined_cad, ignore_index=True)\n            self.stats['cad_records_loaded'] = len(cad_data)\n            logger.info(f\"Total CAD records: {len(cad_data):,}\")\n            return cad_data\n        else:\n            logger.error(\"No CAD data could be loaded!\")\n            return pd.DataFrame()\n    \n    def _preprocess_cad_chunk(self, df: pd.DataFrame, source_file: str) -> pd.DataFrame:\n        \"\"\"Preprocess CAD data chunk with basic cleaning and standardization.\"\"\"\n        # Add source tracking\n        df['Data_Source'] = 'CAD'\n        df['Source_File'] = source_file\n        df['Load_Timestamp'] = datetime.now()\n        \n        # Extract case number using regex\n        if 'ReportNumberNew' in df.columns:\n            # Use configured regex pattern\n            case_pattern = self.config.get('case_number_regex', r'(\\d{2}-\\d{6})')\n            df['Case_Number'] = df['ReportNumberNew'].astype(str).str.extract(case_pattern, expand=False)\n        \n        # Fix 'BACKUP' call addresses\n        if 'FullAddress2' in df.columns and 'Incident' in df.columns:\n            backup_pattern = self.config.get('backup_pattern', r'BACKUP|Assist Own Agency')\n            backup_mask = df['Incident'].str.contains(backup_pattern, case=False, na=False)\n            missing_address_mask = df['FullAddress2'].isna() | (df['FullAddress2'] == '')\n            \n            fix_mask = backup_mask & missing_address_mask\n            if fix_mask.any():\n                df.loc[fix_mask, 'FullAddress2'] = 'Location Per CAD System'\n                self.stats['quality_fixes_applied'] += fix_mask.sum()\n                logger.info(f\"Fixed {fix_mask.sum()} backup addresses\")\n        \n        # Convert time fields to datetime\n        time_fields = ['Time of Call', 'Time Dispatched', 'Time Out', 'Time In']\n        for field in time_fields:\n            if field in df.columns:\n                df[field] = pd.to_datetime(df[field], errors='coerce')\n        \n        # Calculate response times\n        if all(field in df.columns for field in ['Time of Call', 'Time Dispatched']):\n            df['Response_Time_Seconds'] = (\n                df['Time Dispatched'] - df['Time of Call']\n            ).dt.total_seconds()\n        \n        if all(field in df.columns for field in ['Time Dispatched', 'Time Out']):\n            df['Travel_Time_Seconds'] = (\n                df['Time Out'] - df['Time Dispatched']\n            ).dt.total_seconds()\n        \n        # Use memory-efficient data types\n        df = self._optimize_dtypes(df)\n        \n        return df\n    \n    def load_rms_data(self) -> pd.DataFrame:\n        \"\"\"Load and preprocess RMS data from configured sources.\"\"\"\n        logger.info(\"Loading RMS data...\")\n        \n        rms_files = []\n        for pattern in self.config.get('rms_file_patterns', ['*_RMS*.xlsx']):\n            rms_files.extend(list(self.exports_path.glob(pattern)))\n        \n        logger.info(f\"Found {len(rms_files)} RMS files: {[f.name for f in rms_files]}\")\n        \n        exclude_patterns = self.config.get('exclude_patterns', ['*CORRUPTED*', '*OLD*'])\n        for pattern in exclude_patterns:\n            rms_files = [f for f in rms_files if not f.match(pattern)]\n        \n        logger.info(f\"After exclusions: {len(rms_files)} RMS files\")\n        \n        combined_rms = []\n        \n        for file_path in sorted(rms_files):\n            try:\n                logger.info(f\"Processing {file_path.name}...\")\n                \n                # Use chunking for large files\n                chunk_size = self.config.get('chunk_size', 10000)\n                if 'chunkable_files' in self.config and any(file_path.match(pattern) for pattern in self.config['chunkable_files']):\n                    # Process in chunks\n                    chunks = []\n                    for chunk_idx, chunk in enumerate(pd.read_excel(file_path, engine='openpyxl', chunksize=chunk_size)):\n                        logger.info(f\"Processing chunk {chunk_idx+1} with {len(chunk)} rows\")\n                        processed_chunk = self._preprocess_rms_chunk(chunk, file_path.name)\n                        chunks.append(processed_chunk)\n                        \n                        # Force garbage collection\n                        del processed_chunk\n                        gc.collect()\n                    \n                    file_df = pd.concat(chunks, ignore_index=True)\n                else:\n                    # Process entire file at once\n                    df = pd.read_excel(file_path, engine='openpyxl')\n                    file_df = self._preprocess_rms_chunk(df, file_path.name)\n                \n                combined_rms.append(file_df)\n                logger.info(f\"Loaded {len(file_df):,} RMS records from {file_path.name}\")\n                \n                # Force garbage collection\n                del file_df\n                gc.collect()\n                \n            except Exception as e:\n                logger.error(f\"Error processing RMS file {file_path.name}: {str(e)}\")\n                self.stats['processing_errors'] += 1\n        \n        if combined_rms:\n            rms_data = pd.concat(combined_rms, ignore_index=True)\n            self.stats['rms_records_loaded'] = len(rms_data)\n            logger.info(f\"Total RMS records: {len(rms_data):,}\")\n            return rms_data\n        else:\n            logger.warning(\"No RMS data could be loaded - proceeding with CAD-only pipeline\")\n            return pd.DataFrame()\n    \n    def _preprocess_rms_chunk(self, df: pd.DataFrame, source_file: str) -> pd.DataFrame:\n        \"\"\"Preprocess RMS data chunk with basic cleaning and standardization.\"\"\"\n        # Add source tracking\n        df['Data_Source'] = 'RMS'\n        df['Source_File'] = source_file\n        df['Load_Timestamp'] = datetime.now()\n        \n        # Find and extract case number\n        case_columns = ['CaseNumber', 'Case Number', 'Report Number', 'ReportNumber']\n        case_column = next((col for col in case_columns if col in df.columns), None)\n        \n        if case_column:\n            # Use configured regex pattern\n            case_pattern = self.config.get('case_number_regex', r'(\\d{2}-\\d{6})')\n            df['Case_Number'] = df[case_column].astype(str).str.extract(case_pattern, expand=False)\n            \n            # Identify supplements\n            supplement_pattern = self.config.get('supplement_pattern', r'[A-Z]$|S\\d+$')\n            df['Is_Supplement'] = df[case_column].astype(str).str.contains(supplement_pattern, na=False)\n            \n            # Store original case number for reference\n            df['Original_Case_Number'] = df[case_column]\n        \n        # Convert date fields to datetime\n        date_columns = ['ReportDate', 'Report Date', 'IncidentDate', 'Incident Date']\n        for column in date_columns:\n            if column in df.columns:\n                df[column] = pd.to_datetime(df[column], errors='coerce')\n        \n        # Fix time field issues - convert \"1\" to proper time\n        time_columns = [col for col in df.columns if 'time' in col.lower() or 'hour' in col.lower()]\n        for column in time_columns:\n            if column in df.columns and df[column].dtype == 'object':\n                # Check for improper time values\n                improper_time = df[column].astype(str).str.match(r'^\\d{1,2}$')\n                if improper_time.any():\n                    # Convert single digits to proper time format\n                    df.loc[improper_time, column] = df.loc[improper_time, column].astype(str) + ':00:00'\n                    self.stats['quality_fixes_applied'] += improper_time.sum()\n                    logger.info(f\"Fixed {improper_time.sum()} improper time values in {column}\")\n                \n                # Convert to datetime\n                df[column] = pd.to_datetime(df[column], errors='coerce')\n        \n        # Use memory-efficient data types\n        df = self._optimize_dtypes(df)\n        \n        return df\n    \n    def integrate_data(self, cad_data: pd.DataFrame, rms_data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Integrate CAD and RMS data using advanced matching strategies.\"\"\"\n        logger.info(\"Integrating CAD and RMS data...\")\n        \n        if rms_data.empty:\n            logger.warning(\"No RMS data available - continuing with CAD-only dataset\")\n            # Add integration tracking fields to CAD data\n            cad_data['Integration_Type'] = 'CAD_ONLY'\n            cad_data['Match_Confidence'] = 'NONE'\n            cad_data['RMS_Data_Available'] = False\n            \n            self.stats['cad_only_records'] = len(cad_data)\n            return cad_data\n        \n        # Strategy 1: Direct case number matching\n        logger.info(\"Performing direct case number matching...\")\n        \n        # Ensure case number column exists in both dataframes\n        if 'Case_Number' not in cad_data.columns:\n            logger.error(\"Case_Number field missing in CAD data\")\n            self.stats['processing_errors'] += 1\n            cad_data['Case_Number'] = ''\n        \n        if 'Case_Number' not in rms_data.columns:\n            logger.error(\"Case_Number field missing in RMS data\")\n            self.stats['processing_errors'] += 1\n            rms_data['Case_Number'] = ''\n        \n        # Filter out invalid case numbers\n        valid_cad = cad_data['Case_Number'].notna() & (cad_data['Case_Number'] != '')\n        valid_rms = rms_data['Case_Number'].notna() & (rms_data['Case_Number'] != '')\n        \n        logger.info(f\"Valid case numbers: {valid_cad.sum():,} in CAD, {valid_rms.sum():,} in RMS\")\n        \n        # Filter RMS supplements if configured\n        if self.config.get('exclude_supplements', True) and 'Is_Supplement' in rms_data.columns:\n            non_supplement = ~rms_data['Is_Supplement']\n            logger.info(f\"Filtering {(~non_supplement).sum():,} supplement cases from RMS\")\n            rms_match_data = rms_data[valid_rms & non_supplement].copy()\n        else:\n            rms_match_data = rms_data[valid_rms].copy()\n        \n        # Create RMS lookup dictionary for faster matching\n        rms_lookup = {}\n        \n        logger.info(f\"Creating RMS lookup from {len(rms_match_data):,} records...\")\n        \n        # Identify key RMS fields to include in lookup\n        rms_keys = ['Case_Number']\n        \n        # Add disposition field if available\n        disposition_fields = ['Disposition', 'Status', 'CaseStatus']\n        disposition_field = next((f for f in disposition_fields if f in rms_match_data.columns), None)\n        if disposition_field:\n            rms_keys.append(disposition_field)\n        \n        # Add officer field if available\n        officer_fields = ['Officer', 'Officer Name', 'Reporting Officer']\n        officer_field = next((f for f in officer_fields if f in rms_match_data.columns), None)\n        if officer_field:\n            rms_keys.append(officer_field)\n        \n        # Build lookup dictionary\n        for _, row in rms_match_data[rms_keys].iterrows():\n            case_num = row['Case_Number']\n            if pd.notna(case_num) and case_num != '':\n                # Store key fields in dictionary\n                rms_lookup[case_num] = {\n                    field: row[field] for field in rms_keys if field in row.index\n                }\n        \n        logger.info(f\"RMS lookup created with {len(rms_lookup):,} unique case numbers\")\n        \n        # Initialize integration fields\n        cad_data['Integration_Type'] = 'CAD_ONLY'\n        cad_data['Match_Confidence'] = 'NONE'\n        cad_data['RMS_Data_Available'] = False\n        \n        if disposition_field:\n            cad_data['RMS_Disposition'] = ''\n        \n        if officer_field:\n            cad_data['RMS_Officer'] = ''\n        \n        # Perform matching\n        logger.info(\"Performing case number matching...\")\n        \n        matched_count = 0\n        batch_size = self.config.get('processing_batch_size', 5000)\n        total_batches = (len(cad_data) + batch_size - 1) // batch_size\n        \n        for batch_idx in range(total_batches):\n            start_idx = batch_idx * batch_size\n            end_idx = min((batch_idx + 1) * batch_size, len(cad_data))\n            \n            if batch_idx % 10 == 0 or batch_idx == total_batches - 1:\n                logger.info(f\"Processing batch {batch_idx + 1}/{total_batches}\")\n            \n            batch_matches = 0\n            \n            for idx in range(start_idx, end_idx):\n                case_num = cad_data.at[idx, 'Case_Number']\n                \n                if pd.notna(case_num) and case_num != '' and case_num in rms_lookup:\n                    # Match found!\n                    rms_record = rms_lookup[case_num]\n                    \n                    # Update integration fields\n                    cad_data.at[idx, 'Integration_Type'] = 'CAD_RMS_MATCHED'\n                    cad_data.at[idx, 'Match_Confidence'] = 'HIGH'\n                    cad_data.at[idx, 'RMS_Data_Available'] = True\n                    \n                    # Copy relevant RMS fields\n                    if disposition_field and disposition_field in rms_record:\n                        cad_data.at[idx, 'RMS_Disposition'] = rms_record[disposition_field]\n                    \n                    if officer_field and officer_field in rms_record:\n                        cad_data.at[idx, 'RMS_Officer'] = rms_record[officer_field]\n                    \n                    batch_matches += 1\n            \n            matched_count += batch_matches\n        \n        # Update statistics\n        self.stats['successful_matches'] = matched_count\n        self.stats['failed_matches'] = len(cad_data) - matched_count\n        self.stats['cad_only_records'] = len(cad_data) - matched_count\n        \n        logger.info(f\"Matching complete:\")\n        logger.info(f\"  - Total CAD records: {len(cad_data):,}\")\n        logger.info(f\"  - Matched records: {matched_count:,} ({matched_count / len(cad_data) * 100:.1f}%)\")\n        logger.info(f\"  - CAD-only records: {len(cad_data) - matched_count:,}\")\n        \n        return cad_data\n    \n    def calculate_quality_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Calculate data quality scores based on field completeness.\"\"\"\n        logger.info(\"Calculating data quality scores...\")\n        \n        # Initialize quality score\n        df['Data_Quality_Score'] = 0\n        \n        # Score components (total 100 points possible)\n        \n        # Case number present (20 points)\n        has_case = df['Case_Number'].notna() & (df['Case_Number'] != '')\n        df.loc[has_case, 'Data_Quality_Score'] += 20\n        \n        # Address present (20 points)\n        if 'FullAddress2' in df.columns:\n            has_address = df['FullAddress2'].notna() & (df['FullAddress2'] != '')\n            df.loc[has_address, 'Data_Quality_Score'] += 20\n        \n        # Time data present (20 points total)\n        if 'Time of Call' in df.columns:\n            has_call_time = df['Time of Call'].notna()\n            df.loc[has_call_time, 'Data_Quality_Score'] += 10\n        \n        if 'Time Dispatched' in df.columns:\n            has_dispatch_time = df['Time Dispatched'].notna()\n            df.loc[has_dispatch_time, 'Data_Quality_Score'] += 10\n        \n        # Integration success (25 points)\n        is_matched = df['Integration_Type'] == 'CAD_RMS_MATCHED'\n        df.loc[is_matched, 'Data_Quality_Score'] += 25\n        \n        # Officer data (15 points)\n        if 'Officer' in df.columns:\n            has_officer = df['Officer'].notna() & (df['Officer'] != '')\n            df.loc[has_officer, 'Data_Quality_Score'] += 15\n        \n        # Log quality distribution\n        quality_bins = [0, 60, 80, 101]  # Upper bound is exclusive\n        quality_labels = ['Low', 'Medium', 'High']\n        \n        quality_dist = pd.cut(df['Data_Quality_Score'], bins=quality_bins, labels=quality_labels)\n        quality_counts = quality_dist.value_counts()\n        \n        logger.info(\"Data quality distribution:\")\n        for label in ['High', 'Medium', 'Low']:\n            if label in quality_counts:\n                count = quality_counts[label]\n                percentage = count / len(df) * 100\n                logger.info(f\"  {label}: {count:,} records ({percentage:.1f}%)\")\n        \n        logger.info(f\"Average quality score: {df['Data_Quality_Score'].mean():.1f}/100\")\n        \n        return df\n    \n    def _optimize_dtypes(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Optimize data types for memory efficiency.\"\"\"\n        for col in df.columns:\n            if df[col].dtype == 'object':\n                # Convert to category if low cardinality\n                nunique = df[col].nunique()\n                if nunique > 0 and nunique / len(df) < 0.5:\n                    df[col] = df[col].astype('category')\n            elif df[col].dtype == 'float64':\n                # Downcast floats\n                df[col] = pd.to_numeric(df[col], downcast='float')\n            elif df[col].dtype == 'int64':\n                # Downcast integers\n                df[col] = pd.to_numeric(df[col], downcast='integer')\n        \n        return df\n    \n    def save_results(self, df: pd.DataFrame) -> Dict[str, Path]:\n        \"\"\"Save results in multiple formats.\"\"\"\n        logger.info(\"Saving integration results...\")\n        \n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        \n        # Define output files\n        output_files = {\n            'csv': self.output_path / f\"integrated_cad_rms_{timestamp}.csv\",\n            'excel': self.output_path / f\"integrated_cad_rms_{timestamp}.xlsx\",\n            'powerbi': self.output_path / f\"powerbi_ready_{timestamp}.csv\"\n        }\n        \n        # Save full dataset\n        df.to_csv(output_files['csv'], index=False)\n        \n        # Save Excel with limited columns to avoid size issues\n        max_excel_cols = self.config.get('max_excel_columns', 50)\n        if len(df.columns) > max_excel_cols:\n            logger.warning(f\"Limiting Excel output to {max_excel_cols} columns (from {len(df.columns)})\")\n            priority_columns = self.config.get('priority_columns', [\n                'Case_Number', 'Integration_Type', 'Data_Quality_Score',\n                'Time of Call', 'Time Dispatched', 'FullAddress2', 'Incident', 'Officer'\n            ])\n            \n            # Ensure priority columns are included\n            excel_columns = [\n                col for col in priority_columns if col in df.columns\n            ]\n            \n            # Add remaining columns up to the limit\n            remaining_columns = [\n                col for col in df.columns if col not in excel_columns\n            ][:max_excel_cols - len(excel_columns)]\n            \n            excel_columns.extend(remaining_columns)\n            df[excel_columns].to_excel(output_files['excel'], index=False)\n        else:\n            df.to_excel(output_files['excel'], index=False)\n        \n        # Create PowerBI-optimized version\n        powerbi_columns = self.config.get('powerbi_columns', [\n            'Case_Number', 'Incident', 'Time of Call', 'Time Dispatched',\n            'Time Out', 'Time In', 'FullAddress2', 'PDZone', 'Grid',\n            'Officer', 'Integration_Type', 'Data_Quality_Score',\n            'RMS_Disposition', 'Data_Source', 'Source_File'\n        ])\n        \n        # Only include columns that exist\n        available_powerbi_cols = [col for col in powerbi_columns if col in df.columns]\n        \n        # Create clean PowerBI dataset\n        powerbi_df = df[available_powerbi_cols].copy()\n        \n        # Convert all object columns to strings with empty string for nulls\n        for col in powerbi_df.columns:\n            if powerbi_df[col].dtype == 'object' or powerbi_df[col].dtype.name == 'category':\n                powerbi_df[col] = powerbi_df[col].fillna('').astype(str)\n        \n        # Save PowerBI-ready CSV\n        powerbi_df.to_csv(output_files['powerbi'], index=False)\n        \n        # Log file sizes\n        for name, path in output_files.items():\n            size_mb = path.stat().st_size / (1024 * 1024)\n            logger.info(f\"  - {name.upper()}: {path.name} ({size_mb:.1f} MB)\")\n        \n        return output_files\n    \n    def generate_report(self, df: pd.DataFrame) -> Dict:\n        \"\"\"Generate comprehensive integration report.\"\"\"\n        logger.info(\"Generating integration report...\")\n        \n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        \n        # Calculate detailed statistics\n        total_records = len(df)\n        match_rate = 0\n        if 'Integration_Type' in df.columns:\n            matches = (df['Integration_Type'] == 'CAD_RMS_MATCHED').sum()\n            match_rate = matches / total_records if total_records > 0 else 0\n        \n        quality_stats = {}\n        if 'Data_Quality_Score' in df.columns:\n            quality_stats = {\n                'average_quality_score': float(df['Data_Quality_Score'].mean()),\n                'median_quality_score': float(df['Data_Quality_Score'].median()),\n                'high_quality_count': int((df['Data_Quality_Score'] >= 80).sum()),\n                'medium_quality_count': int(((df['Data_Quality_Score'] >= 60) & \n                                           (df['Data_Quality_Score'] < 80)).sum()),\n                'low_quality_count': int((df['Data_Quality_Score'] < 60).sum())\n            }\n        \n        # Create comprehensive report\n        report = {\n            'report_metadata': {\n                'timestamp': timestamp,\n                'pipeline_version': self.config.get('pipeline_version', '1.0.0'),\n                'schema_version': self.config.get('schema_version', 'unknown')\n            },\n            'execution_statistics': {\n                'start_time': self.stats.get('start_time', ''),\n                'end_time': self.stats.get('end_time', ''),\n                'elapsed_seconds': self.stats.get('elapsed_time', 0),\n                'cad_files_processed': len(self.config.get('cad_file_patterns', [])),\n                'rms_files_processed': len(self.config.get('rms_file_patterns', []))\n            },\n            'integration_statistics': {\n                'cad_records_processed': self.stats.get('cad_records_loaded', 0),\n                'rms_records_processed': self.stats.get('rms_records_loaded', 0),\n                'matched_records': self.stats.get('successful_matches', 0),\n                'cad_only_records': self.stats.get('cad_only_records', 0),\n                'match_rate_percentage': round(match_rate * 100, 2),\n                'processing_errors': self.stats.get('processing_errors', 0)\n            },\n            'data_quality_statistics': quality_stats,\n            'field_completeness': self._calculate_field_completeness(df),\n            'recommendations': self._generate_recommendations(df)\n        }\n        \n        # Save report\n        report_file = self.reports_path / f\"integration_report_{timestamp}.json\"\n        with open(report_file, 'w') as f:\n            json.dump(report, f, indent=2, default=str)\n        \n        logger.info(f\"Integration report saved: {report_file}\")\n        return report\n    \n    def _calculate_field_completeness(self, df: pd.DataFrame) -> Dict:\n        \"\"\"Calculate field completeness statistics.\"\"\"\n        completeness = {}\n        \n        key_fields = self.config.get('key_fields', [\n            'Case_Number', 'FullAddress2', 'Time of Call', 'Incident', 'Officer'\n        ])\n        \n        for field in key_fields:\n            if field in df.columns:\n                if df[field].dtype == 'datetime64[ns]':\n                    not_null = df[field].notna()\n                else:\n                    not_null = df[field].notna() & (df[field].astype(str) != '')\n                \n                completeness[field] = {\n                    'count': int(not_null.sum()),\n                    'percentage': float(not_null.mean() * 100),\n                    'null_count': int((~not_null).sum())\n                }\n        \n        return completeness\n    \n    def _generate_recommendations(self, df: pd.DataFrame) -> List[str]:\n        \"\"\"Generate recommendations based on data analysis.\"\"\"\n        recommendations = []\n        \n        # Check match rate\n        match_rate = 0\n        if 'Integration_Type' in df.columns:\n            match_rate = (df['Integration_Type'] == 'CAD_RMS_MATCHED').mean()\n        \n        if match_rate < 0.7:\n            recommendations.append(\n                f\"Low match rate ({match_rate:.1%}) detected. Consider reviewing case number formats \"\n                f\"and improving extraction patterns.\"\n            )\n        \n        # Check quality distribution\n        if 'Data_Quality_Score' in df.columns:\n            low_quality = (df['Data_Quality_Score'] < 60).mean()\n            if low_quality > 0.3:\n                recommendations.append(\n                    f\"High proportion of low-quality records ({low_quality:.1%}).