:2C|39)', incident_norm)\n            if prefix_match:\n                issues.append(\"Missing space before hyphen in statute format\")\n            \n            # Check for spaces within statute part\n            if ' ' in statute:\n                issues.append(\"Spaces in statute part\")\n        \n        # Add to anomalies if issues found\n        if issues:\n            anomaly_rows.append({\n                'Raw_Value': row['Raw_Value'],\n                'Current_Incident_Norm': incident_norm,\n                'Issues': '; '.join(issues),\n                'Suggested_Fix': clean_call_type(incident_norm),\n                'Source': row['Source']\n            })\n    \n    return pd.DataFrame(anomaly_rows)\n\ndef identify_unmapped_incidents(mapping_df, hist_counts):\n    \"\"\"Create a list of unmapped incidents from historical data.\"\"\"\n    # Get all raw values that have mappings\n    mapped_raw = set(mapping_df['Raw_Value'])\n    \n    # Find unmapped incidents\n    unmapped = []\n    for incident, count in hist_counts.items():\n        if incident not in mapped_raw and not pd.isna(incident):\n            unmapped.append({\n                'Raw_Value': incident,\n                'Suggested_Norm': clean_call_type(incident),\n                'Count': count,\n                'Status': 'Needs Mapping'\n            })\n    \n    # Create dataframe and sort by count\n    unmapped_df = pd.DataFrame(unmapped)\n    if not unmapped_df.empty:\n        unmapped_df = unmapped_df.sort_values('Count', ascending=False)\n    \n    return unmapped_df\n\ndef identify_mapping_changes(mapping_df):\n    \"\"\"Identify changes in mappings from original to new normalized values.\"\"\"\n    changes = []\n    \n    # Only look at 'Approved' status rows\n    approved_df = mapping_df[mapping_df['Status'] == 'Approved']\n    \n    for _, row in approved_df.iterrows():\n        raw = row['Raw_Value']\n        current_norm = row['Incident_Norm']\n        new_norm = clean_call_type(raw)\n        \n        # Check if normalization would change\n        if current_norm != new_norm:\n            changes.append({\n                'Raw_Value': raw,\n                'Old_Incident_Norm': current_norm,\n                'New_Incident_Norm': new_norm,\n                'Reason': 'Normalization rule change'\n            })\n    \n    return pd.DataFrame(changes)\n\ndef suggest_clean_edits(anomalies_df):\n    \"\"\"Suggest edits to the clean file based on anomalies.\"\"\"\n    if anomalies_df.empty:\n        return pd.DataFrame()\n    \n    edits = []\n    \n    for _, row in anomalies_df.iterrows():\n        edits.append({\n            'Current_Value': row['Current_Incident_Norm'],\n            'Suggested_Value': row['Suggested_Fix'],\n            'Issues': row['Issues']\n        })\n    \n    return pd.DataFrame(edits)\n\ndef main():\n    # Create master mapping\n    print(\"Creating master mapping...\")\n    mapping_df, hist_counts = create_master_mapping()\n    \n    # Identify duplicates\n    print(\"Identifying duplicates...\")\n    duplicates_df = identify_duplicates(mapping_df)\n    \n    # Identify anomalies\n    print(\"Identifying anomalies...\")\n    anomalies_df = identify_anomalies(mapping_df)\n    \n    # Identify unmapped incidents\n    print(\"Identifying unmapped incidents...\")\n    unmapped_df = identify_unmapped_incidents(mapping_df, hist_counts)\n    \n    # Identify mapping changes\n    print(\"Identifying mapping changes...\")\n    changes_df = identify_mapping_changes(mapping_df)\n    \n    # Suggest clean edits\n    print(\"Suggesting clean file edits...\")\n    edits_df = suggest_clean_edits(anomalies_df)\n    \n    # Save outputs\n    mapping_df.to_csv(OUTPUT_DIR / 'CallType_Master_Mapping.csv', index=False)\n    duplicates_df.to_csv(OUTPUT_DIR / 'duplicates_review.csv', index=False)\n    anomalies_df.to_csv(OUTPUT_DIR / 'anomalies.csv', index=False)\n    unmapped_df.to_csv(OUTPUT_DIR / 'unmapped_incidents.csv', index=False)\n    changes_df.to_csv(OUTPUT_DIR / 'mapping_changes.csv', index=False)\n    edits_df.to_csv(OUTPUT_DIR / 'suggested_clean_edits.csv', index=False)\n    \n    print(f\"Master mapping created with {len(mapping_df)} entries\")\n    print(f\"Found {len(duplicates_df)} potential duplicate sets\")\n    print(f\"Found {len(anomalies_df)} anomalies\")\n    print(f\"Found {len(unmapped_df)} unmapped incidents\")\n    print(f\"Found {len(changes_df)} mapping changes\")\n    print(f\"Suggested {len(edits_df)} edits to clean file\")\n    \n    print(\"\\nAll files saved to:\", OUTPUT_DIR)\n\nif __name__ == '__main__':\n    main()\n",
          "description": "Creating a script to build the master mapping"
        },
        "message": "Creating a script to build the master mapping",
        "display_content": {
          "type": "json_block",
          "json_block": "{\"language\": \"python\", \"code\": \"import pandas as pd\\nimport re\\nimport csv\\nfrom pathlib import Path\\nfrom collections import defaultdict\\n\\n# Paths\\nCLEAN_FILE_PATH = '/mnt/user-data/uploads/CallType_Categories_clean.xlsx'\\nHISTORICAL_DATA_PATH = '/mnt/user-data/uploads/2019_2025_10_Call_Types_Incidents_csv.txt'\\nOUTPUT_DIR = Path('/home/claude/work')\\n\\n# Small words to keep lowercase\\nSMALL_WORDS = {'a', 'an', 'and', 'as', 'at', 'but', 'by', 'for', 'if', 'in', \\n                'of', 'on', 'or', 'the', 'to', 'with', 'nor'}\\n\\n# Acronyms to preserve\\nACRONYMS = {'ABC', 'AD', 'ALPR', 'TAS', 'JV', 'A.T.R.A. ', 'A.C.O.R.N. ', 'DV', 'DWI', \\n            'MV', 'MVA', 'EMS', 'DPW', 'DOA', 'CDS', 'UCC', 'CPR', 'AED'}\\n\\n# Pattern for statute suffixes\\nSTATUTE_SUFFIX_RE = re.compile(\\n    r\\\"\\\\s*-\\\\s*((? :2C|39):[0-9A-Za-z.\\\\-]+)\\\\s*$\\\"\\n)\\n\\ndef smart_title_case(text):\\n    \\\"\\\"\\\"Apply smart title case with preservation of acronyms and small words.\\\"\\\"\\\"\\n    if not text:\\n        return \\\"\\\"\\n    \\n    # Special case for 9-1-1 Emergency\\n    text = re.sub(r'9-1-1[eE]mergency', '9-1-1 Emergency', text)\\n    \\n    # Split text into words\\n    words = text.split()\\n    result = []\\n    \\n    # Process each word\\n    for i, word in enumerate(words):\\n        # Check if it's an acronym (full match)\\n        if word.upper() in ACRONYMS:\\n            result.append(word.upper())\\n        # Check for acronym parts (partial match)\\n        elif any(acronym in word.upper() for acronym in ACRONYMS):\\n            # This is a complex case, handle it specifically\\n            for acronym in ACRONYMS:\\n                if acronym in word.upper():\\n                    word = word.replace(word.upper().replace(acronym, ''), \\n                                       word.upper().replace(acronym, '').lower())\\n                    word = word.replace(acronym.upper(), acronym)\\n            result.append(word)\\n        # First word or after a colon always capitalized\\n        elif i == 0 or (i > 0 and words[i-1].endswith(':')):\\n            result.append(word.capitalize())\\n        # Small words in the middle stay lowercase\\n        elif word.lower() in SMALL_WORDS and i != 0 and i != len(words) - 1:\\n            result.append(word.lower())\\n        # Other words get capitalized\\n        else:\\n            result.append(word.capitalize())\\n    \\n    return ' '.join(result)\\n\\ndef extract_statute(text):\\n    \\\"\\\"\\\"Extract statute information from call type.\\\"\\\"\\\"\\n    if not text:\\n        return None, None\\n    \\n    m = STATUTE_SUFFIX_RE.search(text)\\n    if not m:\\n        return None, None\\n    \\n    statute_text = m.group(1)\\n    statute_type = statute_text.split(':')[0] if ':' in statute_text else None\\n    \\n    return statute_type, statute_text\\n\\ndef clean_call_type(value):\\n    \\\"\\\"\\\"Apply all normalization rules to a call type.\\\"\\\"\\\"\\n    if pd.isna(value):\\n        return \\\"\\\"\\n\\n    s = str(value)\\n\\n    # Trim and collapse internal whitespace\\n    s = s.strip()\\n    s = re.sub(r\\\"\\\\s+\\\", \\\" \\\", s)\\n\\n    # Replace em dash, en dash and non-breaking spaces\\n    s = s.replace(\\\"\\\\u2014\\\", \\\"-\\\").replace(\\\"\\\\u2013\\\", \\\"-\\\").replace(\\\"\\\\u00A0\\\", \\\" \\\")\\n    \\n    # Remove other hidden Unicode characters\\n    s = ''.join(c for c in s if ord(c) < 128 or c.isalpha())\\n\\n    # Try to find a statute suffix\\n    m = STATUTE_SUFFIX_RE.search(s)\\n    if not m:\\n        # No \\\" - 2C\\\" or \\\" - 39\\\" style suffix\\n        return smart_title_case(s)\\n\\n    # Text before the suffix\\n    base_text = s[: m.start()]\\n    base_text = base_text.rstrip(\\\" -\\\")\\n    base_text = smart_title_case(base_text)\\n\\n    # Statute part with spaces removed\\n    statute = m.group(1).replace(\\\" \\\", \\\"\\\")\\n\\n    # Final normalized form\\n    return f\\\"{base_text} - {statute}\\\"\\n\\ndef normalize_for_dup_check(text):\\n    \\\"\\\"\\\"Normalize text for duplicate detection.\\\"\\\"\\\"\\n    if pd.isna(text):\\n        return ''\\n    \\n    # Remove all non-alphanumeric chars and convert to lowercase\\n    s = re.sub(r'[^a-zA-Z0-9]', '', str(text).lower())\\n    return s\\n\\ndef load_historical_incidents():\\n    \\\"\\\"\\\"Load and process historical incident data.\\\"\\\"\\\"\\n    try:\\n        # Read as tab-delimited\\n        hist_df = pd.read_csv(HISTORICAL_DATA_PATH, sep='\\\\t', encoding='latin1')\\n        \\n        # Collect all incidents across years\\n        all_incidents = []\\n        incident_counts = defaultdict(int)\\n        \\n        incident_cols = [col for col in hist_df.columns if col.startswith('Incident_')]\\n        for col in incident_cols:\\n            for incident in hist_df[col].dropna():\\n                all_incidents.append(incident)\\n                incident_counts[incident] += 1\\n        \\n        return all_incidents, incident_counts\\n    \\n    except Exception as e:\\n        print(f\\\"Error loading historical data: {e}\\\")\\n        return [], defaultdict(int)\\n\\ndef create_master_mapping():\\n    \\\"\\\"\\\"Create the master mapping from all sources.\\\"\\\"\\\"\\n    # Load clean file (baseline)\\n    clean_df = pd.read_excel(CLEAN_FILE_PATH)\\n    \\n    # Load historical incidents\\n    hist_incidents, hist_counts = load_historical_incidents()\\n    \\n    # Create mapping dataframe\\n    mapping_rows = []\\n    \\n    # Track which raw values are already mapped\\n    mapped_raw_values = set()\\n    \\n    # Process current clean types first\\n    for _, row in clean_df.iterrows():\\n        raw_value = row['Call Type Original'] if 'Call Type Original' in clean_df.columns else row['Call Type']\\n        normalized = clean_call_type(row['Call Type'])\\n        \\n        # Extract statute info\\n        statute_type, statute = extract_statute(normalized)\\n        \\n        mapping_rows.append({\\n            'Raw_Value': raw_value,\\n            'Incident_Norm': normalized,\\n            'Category_Type': '',  # Not available in current data\\n            'Response_Type': row.get('Response', ''),\\n            'Statute': statute_type if statute_type else '',\\n            'Source': 'Current CAD',\\n            'Status': 'Approved',\\n            'Note': ''\\n        })\\n        \\n        mapped_raw_values.add(raw_value)\\n    \\n    # Process historical incidents that aren't already mapped\\n    for raw_incident in hist_incidents:\\n        if raw_incident not in mapped_raw_values:\\n            normalized = clean_call_type(raw_incident)\\n            \\n            # Extract statute info\\n            statute_type, statute = extract_statute(normalized)\\n            \\n            mapping_rows.append({\\n                'Raw_Value': raw_incident,\\n                'Incident_Norm': normalized,\\n                'Category_Type': '',\\n                'Response_Type': '',\\n                'Statute': statute_type if statute_type else '',\\n                'Source': 'Historical CAD',\\n                'Status': 'Needs Review',\\n                'Note': f\\\"Count in historical data: {hist_counts[raw_incident]}\\\"\\n            })\\n            \\n            mapped_raw_values.add(raw_incident)\\n    \\n    # Create dataframe\\n    mapping_df = pd.DataFrame(mapping_rows)\\n    \\n    # Sort by source (Current CAD first) then by normalized value\\n    mapping_df = mapping_df.sort_values(['Source', 'Incident_Norm'], ascending=[False, True])\\n    \\n    return mapping_df, hist_counts\\n\\ndef identify_duplicates(mapping_df):\\n    \\\"\\\"\\\"Identify potential duplicates in the mapping.\\\"\\\"\\\"\\n    # Add normalized column for duplicate detection\\n    mapping_df['Normalized'] = mapping_df['Incident_Norm'].apply(normalize_for_dup_check)\\n    \\n    # Find duplicates\\n    dup_norm_values = mapping_df['Normalized'].value_counts()\\n    potential_dups = dup_norm_values[dup_norm_values > 1].index.tolist()\\n    \\n    # Create duplicates review dataframe\\n    dup_rows = []\\n    \\n    for norm_val in potential_dups:\\n        dupes = mapping_df[mapping_df['Normalized'] == norm_val]\\n        \\n        # Group by current normalized values\\n        for norm_incident, group in dupes.groupby('Incident_Norm'):\\n            raw_values = group['Raw_Value'].tolist()\\n            sources = group['Source'].tolist()\\n            \\n            dup_rows.append({\\n                'Normalized_Group': norm_val,\\n                'Current_Incident_Norm': norm_incident,\\n                'Raw_Values': '; '.join(raw_values),\\n                'Sources': '; '.join(sources),\\n                'Suggested_Keeper': norm_incident,\\n                'Action': 'Review'\\n            })\\n    \\n    return pd.DataFrame(dup_rows)\\n\\ndef identify_anomalies(mapping_df):\\n    \\\"\\\"\\\"Identify anomalies in the normalized values.\\\"\\\"\\\"\\n    anomaly_rows = []\\n    \\n    for _, row in mapping_df.iterrows():\\n        issues = []\\n        \\n        # Check title case issues\\n        incident_norm = row['Incident_Norm']\\n        \\n        # Skip statute part for these checks\\n        base_text = re.sub(r'\\\\s*-\\\\s*((? :2C|39):[0-9A-Za-z.\\\\-]+)\\\\s*$', '', incident_norm)\\n        \\n        # Check for small words that should be lowercase\\n        words = base_text.split()\\n        for i, word in enumerate(words):\\n            if i > 0 and i < len(words) - 1:  # Not first or last\\n                if word.lower() in SMALL_WORDS and word != word.lower():\\n                    issues.append(f\\\"'{word}' should be lowercase\\\")\\n        \\n        # Check for acronyms\\n        for acronym in ACRONYMS:\\n            if acronym.lower() in base_text.lower() and acronym not in base_text:\\n                if acronym.upper() in base_text.upper():  # It exists but in wrong case\\n                    issues.append(f\\\"Acronym '{acronym}' not in correct case\\\")\\n        \\n        # Special case for 9-1-1 Emergency\\n        if '9-1-1emergency' in base_text.lower() or '9-1-1Emergency' in base_text:\\n            issues.append(\\\"Should be '9-1-1 Emergency'\\\")\\n        \\n        # Check statute formatting\\n        statute_match = re.search(r'\\\\s*-\\\\s*((? :2C|39):[0-9A-Za-z.\\\\-]+)\\\\s*$', incident_norm)\\n        if statute_match:\\n            statute = statute_match.group(1)\\n            \\n            # Check for proper spacing around hyphen\\n            prefix_match = re.search(r'(\\\\S)-\\\\s*(?