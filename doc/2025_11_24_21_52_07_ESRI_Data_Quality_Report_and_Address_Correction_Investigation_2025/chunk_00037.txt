if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file)
    corrections = corrections[corrections['Corrected_Value'].notna() & (corrections['Corrected_Value'] != '')]
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Clean case numbers - remove newlines, whitespace, and normalize
    corrections['ReportNumberNew_clean'] = corrections['ReportNumberNew'].astype(str).str.strip().str.replace('\n', '').str.replace('\r', '')
    corrections['Corrected_Value_clean'] = corrections['Corrected_Value'].astype(str).str.strip()
    
    # Create mapping from old to new case numbers
    case_map = dict(zip(corrections['ReportNumberNew_clean'], corrections['Corrected_Value_clean']))
    
    # Clean ReportNumberNew in the dataframe for matching
    df['ReportNumberNew_clean'] = df['ReportNumberNew'].astype(str).str.strip().str.replace('\n', '').str.replace('\r', '')
    
    # Apply corrections
    mask = df['ReportNumberNew_clean'].isin(case_map.keys())
    count = mask.sum()
    df.loc[mask, 'ReportNumberNew'] = df.loc[mask, 'ReportNumberNew_clean'].map(case_map)
    
    # Drop temporary column
    df = df.drop(columns=['ReportNumberNew_clean'])
    
    return df, count

def apply_address_corrections(df, corrections_file):
    """Apply FullAddress2 corrections.""" if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file, dtype=str).fillna('')
    corrections = corrections[corrections['Corrected_Value'].astype(str).str.strip() != '']
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Remove duplicates from corrections - keep first occurrence per ReportNumberNew
    corrections = corrections.drop_duplicates(subset=['ReportNumberNew'], keep='first')
    print(f"  Unique corrections to apply: {len(corrections):,}")
    
    # Create a mapping dictionary for faster lookup
    correction_map = dict(zip(
        corrections['ReportNumberNew'].astype(str).str.strip(),
        corrections['Corrected_Value'].astype(str).str.strip()
    ))
    
    # Apply corrections using map (faster and avoids merge duplicates)
    df['ReportNumberNew_stripped'] = df['ReportNumberNew'].astype(str).str.strip()
    df['FullAddress2_new'] = df['ReportNumberNew_stripped'].map(correction_map)
    
    # Apply corrections where new value exists
    mask = df['FullAddress2_new'].notna() & (df['FullAddress2_new'] != '')
    count = mask.sum()
    df.loc[mask, 'FullAddress2'] = df.loc[mask, 'FullAddress2_new']
    df = df.drop(columns=['FullAddress2_new', 'ReportNumberNew_stripped'])
    
    return df, count

def fix_hour_field(df):
    """Extract HH:mm from TimeOfCall and populate Hour field without rounding.""" print("\n  Fixing Hour field format...")
    
    if 'TimeOfCall' not in df.columns:
        print("  WARNING: TimeOfCall column not found. Skipping Hour field fix.") return df, 0
    
    if 'Hour' not in df.columns:
        print("  WARNING: Hour column not found. Creating Hour column.") df['Hour'] = ''
    
    def extract_time_from_timeofcall(timeofcall_str):
        """Extract HH:mm from TimeOfCall (format: MM/dd/YYYY HH:mm).""" if pd.isna(timeofcall_str):
            return ''
        
        timeofcall_str = str(timeofcall_str).strip()
        
        # Try to parse MM/dd/YYYY HH:mm format
        import re
        # Pattern: MM/dd/YYYY HH:mm or similar datetime formats
        time_match = re.search(r'(\d{1,2}):(\d{2})', timeofcall_str)
        if time_match:
            hour = time_match.group(1).zfill(2)  # Pad to 2 digits
            minute = time_match.group(2)
            return f"{hour}:{minute}"
        
        # Try parsing as datetime if it's in a different format
        try:
            from datetime import datetime
            # Try common formats
            for fmt in ['%m/%d/%Y %H:%M', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%m/%d/%Y %H:%M:%S']:
                try:
                    dt = datetime.strptime(timeofcall_str, fmt)
                    return f"{dt.hour:02d}:{dt.minute:02d}"
                except:
                    continue
        except:
            pass
        
        return ''
    
    # Extract time from TimeOfCall for all records
    df['Hour'] = df['TimeOfCall'].apply(extract_time_from_timeofcall)
    
    # Count how many were populated
    count = (df['Hour'] != '').sum()
    
    return df, count

def main():
    print("="*60)
    print("APPLYING MANUAL CORRECTIONS")
    print("="*60)
    print(f"\nInput file: {ESRI_FILE}")
    print(f"Corrections directory: {CORRECTIONS_DIR}\n")
    
    # Load main file
    print("Loading ESRI export file...")
    df = pd.read_excel(ESRI_FILE)
    print(f"  Loaded {len(df):,} records\n")
    
    total_corrections = 0
    
    # Apply corrections
    print("Applying corrections...")
    
    # 1. How Reported
    print("\n1. How Reported corrections:")
    df, count = apply_how_reported_corrections(df, CORRECTIONS_DIR / "how_reported_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 2. Disposition
    print("\n2. Disposition corrections:")
    df, count = apply_disposition_corrections(df, CORRECTIONS_DIR / "disposition_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 3. Case Numbers
    print("\n3. Case Number corrections:")
    df, count = apply_case_number_corrections(df, CORRECTIONS_DIR / "case_number_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 4. Addresses
    print("\n4. Address corrections:")
    df, count = apply_address_corrections(df, CORRECTIONS_DIR / "address_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 5. Hour Field (automated fix)
    print("\n5. Hour field format fix:")
    df, count = fix_hour_field(df)
    print(f"  Fixed {count:,} records")
    total_corrections += count
    
    # Save corrected file - update the production file directly
    print(f"\nSaving corrected file...")
    print(f"  Updating production file: {ESRI_FILE.name}")
    df.to_excel(ESRI_FILE, index=False)
    print(f"  Saved to: {ESRI_FILE}")
    
    # Also save a timestamped backup
    timestamp = datetime.now().strftime('%Y%m%d')
    backup_file = OUTPUT_DIR / f"CAD_ESRI_Final_{timestamp}_corrected.xlsx"
    df.to_excel(backup_file, index=False)
    print(f"  Backup saved to: {backup_file}")
    
    print("\n" + "="*60)
    print("CORRECTIONS COMPLETE")
    print("="*60)
    print(f"Total corrections applied: {total_corrections:,}")
    print(f"Output file: {output_file}")
    print("\nNext step: Re-run validation to verify corrections")

if __name__ == "__main__":
    main()


```


Fixing the error and regenerating the report with the correct numbers:


```python
#!/usr/bin/env python3
"""
Apply manual corrections from CSV files to the ESRI final export.
""" import pandas as pd
from pathlib import Path
from datetime import datetime

BASE_DIR = Path(r"C:\Users\carucci_r\OneDrive - City of Hackensack\02_ETL_Scripts\CAD_Data_Cleaning_Engine")
ESRI_FILE = BASE_DIR / "data" / "ESRI_CADExport" / "CAD_ESRI_Final_20251117_v2.xlsx"
CORRECTIONS_DIR = BASE_DIR / "manual_corrections"
OUTPUT_DIR = BASE_DIR / "data" / "ESRI_CADExport"

def apply_how_reported_corrections(df, corrections_file):
    """Apply How Reported corrections.""" if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file)
    corrections = corrections[corrections['Corrected_Value'].notna() & (corrections['Corrected_Value'] != '')]
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Merge corrections
    df = df.merge(
        corrections[['ReportNumberNew', 'Corrected_Value']].rename(columns={'Corrected_Value': 'How Reported'}),
        on='ReportNumberNew',
        how='left',
        suffixes=('', '_new')
    )
    
    # Apply corrections where new value exists
    mask = df['How Reported_new'].notna()
    count = mask.sum()
    df.loc[mask, 'How Reported'] = df.loc[mask, 'How Reported_new']
    df = df.drop(columns=['How Reported_new'])
    
    return df, count

def apply_disposition_corrections(df, corrections_file):
    """Apply Disposition corrections.""" if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file)
    corrections = corrections[corrections['Corrected_Value'].notna() & (corrections['Corrected_Value'] != '')]
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Merge corrections
    df = df.merge(
        corrections[['ReportNumberNew', 'Corrected_Value']].rename(columns={'Corrected_Value': 'Disposition'}),
        on='ReportNumberNew',
        how='left',
        suffixes=('', '_new')
    )
    
    # Apply corrections where new value exists
    mask = df['Disposition_new'].notna()
    count = mask.sum()
    df.loc[mask, 'Disposition'] = df.loc[mask, 'Disposition_new']
    df = df.drop(columns=['Disposition_new'])
    
    return df, count

def apply_case_number_corrections(df, corrections_file):
    """Apply ReportNumberNew corrections.""" if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file)
    corrections = corrections[corrections['Corrected_Value'].notna() & (corrections['Corrected_Value'] != '')]
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Clean case numbers - remove newlines, whitespace, and normalize
    corrections['ReportNumberNew_clean'] = corrections['ReportNumberNew'].astype(str).str.strip().str.replace('\n', '').str.replace('\r', '')
    corrections['Corrected_Value_clean'] = corrections['Corrected_Value'].astype(str).str.strip()
    
    # Create mapping from old to new case numbers
    case_map = dict(zip(corrections['ReportNumberNew_clean'], corrections['Corrected_Value_clean']))
    
    # Clean ReportNumberNew in the dataframe for matching
    df['ReportNumberNew_clean'] = df['ReportNumberNew'].astype(str).str.strip().str.replace('\n', '').str.replace('\r', '')
    
    # Apply corrections
    mask = df['ReportNumberNew_clean'].isin(case_map.keys())
    count = mask.sum()
    df.loc[mask, 'ReportNumberNew'] = df.loc[mask, 'ReportNumberNew_clean'].map(case_map)
    
    # Drop temporary column
    df = df.drop(columns=['ReportNumberNew_clean'])
    
    return df, count

def apply_address_corrections(df, corrections_file):
    """Apply FullAddress2 corrections.""" if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file, dtype=str).fillna('')
    corrections = corrections[corrections['Corrected_Value'].astype(str).str.strip() != '']
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Remove duplicates from corrections - keep first occurrence per ReportNumberNew
    corrections = corrections.drop_duplicates(subset=['ReportNumberNew'], keep='first')
    print(f"  Unique corrections to apply: {len(corrections):,}")
    
    # Create a mapping dictionary for faster lookup
    correction_map = dict(zip(
        corrections['ReportNumberNew'].astype(str).str.strip(),
        corrections['Corrected_Value'].astype(str).str.strip()
    ))
    
    # Apply corrections using map (faster and avoids merge duplicates)
    df['ReportNumberNew_stripped'] = df['ReportNumberNew'].astype(str).str.strip()
    df['FullAddress2_new'] = df['ReportNumberNew_stripped'].map(correction_map)
    
    # Apply corrections where new value exists
    mask = df['FullAddress2_new'].notna() & (df['FullAddress2_new'] != '')
    count = mask.sum()
    df.loc[mask, 'FullAddress2'] = df.loc[mask, 'FullAddress2_new']
    df = df.drop(columns=['FullAddress2_new', 'ReportNumberNew_stripped'])
    
    return df, count

def fix_hour_field(df):
    """Extract HH:mm from TimeOfCall and populate Hour field without rounding.""" print("\n  Fixing Hour field format...")
    
    if 'TimeOfCall' not in df.columns:
        print("  WARNING: TimeOfCall column not found. Skipping Hour field fix.") return df, 0
    
    if 'Hour' not in df.columns:
        print("  WARNING: Hour column not found. Creating Hour column.") df['Hour'] = ''
    
    def extract_time_from_timeofcall(timeofcall_str):
        """Extract HH:mm from TimeOfCall (format: MM/dd/YYYY HH:mm).""" if pd.isna(timeofcall_str):
            return ''
        
        timeofcall_str = str(timeofcall_str).strip()
        
        # Try to parse MM/dd/YYYY HH:mm format
        import re
        # Pattern: MM/dd/YYYY HH:mm or similar datetime formats
        time_match = re.search(r'(\d{1,2}):(\d{2})', timeofcall_str)
        if time_match:
            hour = time_match.group(1).zfill(2)  # Pad to 2 digits
            minute = time_match.group(2)
            return f"{hour}:{minute}"
        
        # Try parsing as datetime if it's in a different format
        try:
            from datetime import datetime
            # Try common formats
            for fmt in ['%m/%d/%Y %H:%M', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%m/%d/%Y %H:%M:%S']:
                try:
                    dt = datetime.strptime(timeofcall_str, fmt)
                    return f"{dt.hour:02d}:{dt.minute:02d}"
                except:
                    continue
        except:
            pass
        
        return ''
    
    # Extract time from TimeOfCall for all records
    df['Hour'] = df['TimeOfCall'].apply(extract_time_from_timeofcall)
    
    # Count how many were populated
    count = (df['Hour'] != '').sum()
    
    return df, count

def main():
    print("="*60)
    print("APPLYING MANUAL CORRECTIONS")
    print("="*60)
    print(f"\nInput file: {ESRI_FILE}")
    print(f"Corrections directory: {CORRECTIONS_DIR}\n")
    
    # Load main file
    print("Loading ESRI export file...")
    df = pd.read_excel(ESRI_FILE)
    print(f"  Loaded {len(df):,} records\n")
    
    total_corrections = 0
    
    # Apply corrections
    print("Applying corrections...")
    
    # 1. How Reported
    print("\n1. How Reported corrections:")
    df, count = apply_how_reported_corrections(df, CORRECTIONS_DIR / "how_reported_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 2. Disposition
    print("\n2. Disposition corrections:")
    df, count = apply_disposition_corrections(df, CORRECTIONS_DIR / "disposition_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 3. Case Numbers
    print("\n3. Case Number corrections:")
    df, count = apply_case_number_corrections(df, CORRECTIONS_DIR / "case_number_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 4. Addresses
    print("\n4. Address corrections:")
    df, count = apply_address_corrections(df, CORRECTIONS_DIR / "address_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 5. Hour Field (automated fix)
    print("\n5. Hour field format fix:")
    df, count = fix_hour_field(df)
    print(f"  Fixed {count:,} records")
    total_corrections += count
    
    # Save corrected file - update the production file directly
    print(f"\nSaving corrected file...")
    print(f"  Updating production file: {ESRI_FILE.name}")
    df.to_excel(ESRI_FILE, index=False)
    print(f"  Saved to: {ESRI_FILE}")
    
    # Also save a timestamped backup
    timestamp = datetime.now().strftime('%Y%m%d')
    backup_file = OUTPUT_DIR / f"CAD_ESRI_Final_{timestamp}_corrected.xlsx"
    df.to_excel(backup_file, index=False)
    print(f"  Backup saved to: {backup_file}")
    
    print("\n" + "="*60)
    print("CORRECTIONS COMPLETE")
    print("="*60)
    print(f"Total corrections applied: {total_corrections:,}")
    print(f"Production file updated: {ESRI_FILE.name}")
    print(f"Backup file: {backup_file.name}")
    print("\nNext step: Re-run validation to verify corrections")

if __name__ == "__main__":
    main()


```


The file now has 955,759 records (was 728,593), indicating duplicates. Checking the Hour field fix function:



The v2 file now has 955,759 records (was 728,593), indicating duplicates from a previous merge. The 0.9% improvement is because address corrections weren't applied to v2; they were only in a separate corrected file. Summary:
- Address corrections weren't applied to the production v2 file
- The corrected file had duplicates, so it wasn't used for the report
- The report analyzed v2, which didn't have the corrections

I've updated the script to:
1. Use a map instead of merge to avoid duplicates
2. Apply corrections directly to v2
3. Save a backup

The file now has 955,759 records, suggesting duplicates were introduced. We should restore v2 from a clean backup or regenerate it. Checking for a clean backup:



The file now has duplicates (955,759 vs 728,593). The corrections were applied (10/10 verified), but the duplicates skew the analysis. Restoring the v2 file from a clean backup, then reapplying corrections correctly:


---

**User**

@scripts/backfill_address_from_rms_with_geocode_validation.py  would script be useful? ---

**Cursor**


Analyzing the script and its relevance to the current issue:



Yes, the geocoding validation script would be useful, with some considerations:

## Benefits of the geocoding validation script

1. More accurate validation
   - Current script: pattern-based (regex) validation
   - Geocoding script: actual geocoding via ArcGIS Pro
   - Only backfills addresses that geocode successfully (score ≥ 90)

2. Quality assurance
   - Catches addresses that look valid but don't geocode
   - Provides geocoding metrics (match rate, scores, match types)
   - Reduces false positives

3. Production readiness
   - Ensures addresses are geocodable for ESRI
   - Validates against your Hackensack composite locator
   - Better fit for final production data

## Current situation

The immediate issue is applying existing corrections, not finding new ones:
- 1,688 corrections in CSV
- Only 896 applied to v2
- File has duplicates (955,759 vs 728,593)

## Recommendation

Use the geocoding script after:
1. Fixing the duplicate issue in v2
2. Applying existing corrections from the CSV
3. Running it as a final validation/backfill pass

It would be most useful for:
- Final quality check before ESRI delivery
- Finding additional RMS addresses that can be backfilled
- Validating that all addresses actually geocode

Should I:
1. First fix the duplicate issue and apply existing corrections? 2. Then integrate the geocoding validation script? 3. Or run the geocoding script now to validate what we have? Note: The script requires ArcGIS Pro with arcpy. Do you have ArcGIS Pro available? ---

**User**

provide a detailed summary for Claude to investigate add paths, file names and examples 

---

**Cursor**


Creating a detailed investigation summary with paths, file names, and examples:


```markdown
# Address Correction Investigation Summary
## For AI Assistant Investigation

**Date**: November 24, 2025  
**Issue**: Address corrections showing only 0.9% improvement instead of expected ~9%  
**Status**: Critical - Corrections not fully applied to production file

---

## Problem Statement

The address correction system was designed to improve address quality by applying 1,688 corrections from `address_corrections.csv`, but the final ESRI production file shows only a 0.9% improvement in invalid addresses (102,717 → 102,700), suggesting corrections were not fully applied. **Expected**: ~9,607 address corrections applied (1,688 unique corrections × multiple records per case)  
**Actual**: Only 896 corrections verified as applied  
**Discrepancy**: ~8,711 corrections missing

---

## File Structure and Paths

### Production Files
```
C:\Users\carucci_r\OneDrive - City of Hackensack\02_ETL_Scripts\CAD_Data_Cleaning_Engine\
├── data\
│   ├── ESRI_CADExport\
│   │   ├── CAD_ESRI_Final_20251117_v2.xlsx          [PRODUCTION FILE - 728,593 records expected]
│   │   ├── CAD_ESRI_Final_20251117_v3.xlsx          [Backup - 728,593 records]
│   │   ├── CAD_ESRI_Final_20251124_corrected.xlsx   [Has duplicates - 955,759 records]
│   │   └── archive\
│   │       ├── CAD_ESRI_Final_20251117_v2_pre_full_rebuild.xlsx
│   │       └── [other archived versions]
│   └── rms\
│       └── 2019_2025_11_16_16_57_00_ALL_RMS_Export.xlsx
```

### Correction Files
```
├── manual_corrections\
│   ├── address_corrections.csv                      [1,696 records, 1,688 with corrections]
│   ├── disposition_corrections.csv
│   ├── case_number_corrections.csv
│   └── how_reported_corrections.csv
```

### Scripts
```
├── scripts\
│   ├── apply_manual_corrections.py                  [Applies all corrections to ESRI file]
│   ├── check_address_corrections_applied.py         [Verifies corrections were applied]
│   ├── generate_esri_quality_report.py             [Generates quality report]
│   ├── analyze_address_improvement.py              [Analyzes before/after]
│   ├── backfill_address_from_rms.py                  [RMS backfill (pattern-based)]
│   └── backfill_address_from_rms_with_geocode_validation.py  [RMS backfill with ArcGIS geocoding]
```

### Reports
```
├── data\02_reports\
│   ├── ESRI_Data_Quality_Report.md                  [Current quality report]
│   ├── address_backfill_from_rms_report.md          [RMS backfill report]
│   └── ADDRESS_CORRECTION_INVESTIGATION_SUMMARY.md  [This file]
```

---

## Current State Analysis

### Production File Status
- **File**: `CAD_ESRI_Final_20251117_v2.xlsx`
- **Location**: `data/ESRI_CADExport/CAD_ESRI_Final_20251117_v2.xlsx`
- **Expected Records**: 728,593
- **Current Records**: 955,759 (⚠️ **DUPLICATES DETECTED**)
- **Unique ReportNumberNew**: 541,458
- **Duplicate Count**: 414,301 records

### Address Quality Metrics (Current v2 File)
```
Total Records: 955,759
├── Valid Addresses: 853,059 (89.3%)
└── Invalid Addresses: 102,700 (10.7%)
    ├── Incomplete Intersections: 87,221
    └── Generic Locations: 15,479
```

### Address Corrections CSV Status
- **File**: `manual_corrections/address_corrections.csv`
- **Total Records**: 1,696
- **Records with Corrected_Value**: 1,688
- **Unique ReportNumberNew**: 1,468 (some cases have multiple corrections)

### Verification Results
**Script**: `scripts/check_address_corrections_applied.py`

```
Total corrections in CSV: 1,688
Records in ESRI that have corrections: 4,895
Corrections actually applied (FullAddress2 matches Corrected_Value): 896
Corrections NOT applied: 3,999

[WARNING] Only 896 of 1,688 corrections appear to be applied! ```

---

## Example Corrections (Not Applied)

### Example 1: Park Location Correction
```csv
ReportNumberNew: 19-053037
Original FullAddress2: "Second Ward Park & , Hackensack, NJ, 07601"
Corrected_Value: "59 Polifly Road, Hackensack, NJ 07601"
Status: ❌ NOT APPLIED (ESRI still shows "Second Ward Park & ,")
```

### Example 2: Generic Location Correction
```csv
ReportNumberNew: 19-002541
Original FullAddress2: "Home & , Hackensack, NJ, 07601"
Corrected_Value: "225 State Street, Hackensack, NJ, 07601"
Status: ❌ NOT APPLIED (ESRI still shows "Home & ,")
```

### Example 3: Park to Street Address
```csv
ReportNumberNew: 19-020045
Original FullAddress2: "Carver Park & , Hackensack, NJ, 07601"
Corrected_Value: "302 Second Street, Hackensack, NJ 07601"
Status: ❌ NOT APPLIED (ESRI still shows "Carver Park & ,")
```

### Example 4: Multiple Records Same Case
```csv
ReportNumberNew: 19-088646 (appears 3 times in ESRI file)
Original FullAddress2: "Home & , Hackensack, NJ, 07601" (all 3 records)
Corrected_Value: "225 State Street, Hackensack, NJ, 07601"
Status: ❌ NOT APPLIED (all 3 records still show "Home & ,")
```

---

## Root Cause Analysis

### Issue 1: Corrections Not Applied to Production File
**Problem**: The `apply_manual_corrections.py` script was creating a new timestamped file instead of updating the production v2 file directly. **Evidence**:
- Script output shows: "Saved to: CAD_ESRI_Final_20251124_corrected.xlsx"
- Production file `CAD_ESRI_Final_20251117_v2.xlsx` was not updated
- Quality report analyzed the wrong file

**Fix Applied**: Modified script to update production file directly (see `scripts/apply_manual_corrections.py` lines 237-245)

### Issue 2: Duplicate Records Created
**Problem**: The merge operation in `apply_address_corrections()` was creating duplicates when multiple records share the same `ReportNumberNew`. **Evidence**:
- Original file: 728,593 records
- After corrections: 955,759 records
- Unique ReportNumberNew: 541,458 (unchanged)
- Duplicate increase: 187,135 → 414,301

**Root Cause**: The merge operation with `how='left'` was creating a cartesian product when:
- Multiple records in ESRI file have same `ReportNumberNew`
- Multiple corrections in CSV have same `ReportNumberNew`

**Fix Applied**: Changed from merge to dictionary mapping (see `scripts/apply_manual_corrections.py` lines 106-139)

### Issue 3: Address Correction Function Logic
**Previous Implementation** (causing duplicates):
```python
# OLD CODE - Creates duplicates
df = df.merge(
    corrections[['ReportNumberNew', 'Corrected_Value']].rename(columns={'Corrected_Value': 'FullAddress2'}),
    on='ReportNumberNew',
    how='left',
    suffixes=('', '_new')
)
```

**New Implementation** (prevents duplicates):
```python
# NEW CODE - Uses dictionary mapping
corrections = corrections.drop_duplicates(subset=['ReportNumberNew'], keep='first')
correction_map = dict(zip(
    corrections['ReportNumberNew'].astype(str).str.strip(),
    corrections['Corrected_Value'].astype(str).str.strip()
))
df['ReportNumberNew_stripped'] = df['ReportNumberNew'].astype(str).str.strip()
df['FullAddress2_new'] = df['ReportNumberNew_stripped'].map(correction_map)
```

---

## Correction Application Process

### Step-by-Step Process
1. **Load Corrections CSV**
   - File: `manual_corrections/address_corrections.csv`
   - Filter: `Corrected_Value` not null/empty
   - Deduplicate: Keep first occurrence per `ReportNumberNew`

2. **Load Production ESRI File**
   - File: `data/ESRI_CADExport/CAD_ESRI_Final_20251117_v2.xlsx`
   - Expected: 728,593 records
   - Verify: No duplicates before applying

3. **Apply Corrections**
   - Create mapping dictionary: `ReportNumberNew` → `Corrected_Value`
   - Strip whitespace from keys for matching
   - Map corrections to all records with matching `ReportNumberNew`
   - Update `FullAddress2` where correction exists

4. **Save Results**
   - Update production file: `CAD_ESRI_Final_20251117_v2.xlsx`
   - Create backup: `CAD_ESRI_Final_YYYYMMDD_corrected.xlsx`
   - Verify: Record count unchanged (728,593)

---

## Verification Checklist

### Pre-Application Verification
- [ ] Production file has exactly 728,593 records
- [ ] No duplicate `ReportNumberNew` values (or expected duplicates documented)
- [ ] Address corrections CSV has 1,688 valid corrections
- [ ] Backup of production file created

### Post-Application Verification
- [ ] Record count unchanged (728,593)
- [ ] No new duplicates created
- [ ] Sample of corrections verified (use `check_address_corrections_applied.py`)
- [ ] Address quality improved (invalid addresses reduced)
- [ ] All corrections from CSV applied

### Expected Results After Fix
```
Before:
- Invalid Addresses: 102,717 (14.1%)
- Valid Addresses: 625,876 (85.9%)

After (Expected):
- Invalid Addresses: ~93,000-95,000 (12.8-13.0%)
- Valid Addresses: ~633,000-635,000 (87.0-87.2%)
- Improvement: ~7,000-9,000 addresses corrected
```

---

## Scripts to Run (In Order)

### 1. Restore Clean Production File
```bash
# Option A: Use v3 backup (if clean)
copy "data\ESRI_CADExport\CAD_ESRI_Final_20251117_v3.xlsx" "data\ESRI_CADExport\CAD_ESRI_Final_20251117_v2.xlsx"

# Option B: Use archived backup
copy "data\ESRI_CADExport\archive\CAD_ESRI_Final_20251117_v2_pre_full_rebuild.xlsx" "data\ESRI_CADExport\CAD_ESRI_Final_20251117_v2.xlsx"
```

### 2. Verify File is Clean
```bash
python scripts/analyze_address_improvement.py
```
**Expected Output**:
- v2 file: 728,593 records
- Unique ReportNumberNew: 541,458
- Duplicates: 187,135 (expected - multiple records per case)

### 3. Apply Corrections
```bash
python scripts/apply_manual_corrections.py
```
**Expected Output**:
```
4. Address corrections:
  Unique corrections to apply: 1,468
  Applied 3,649 corrections
```

### 4. Verify Corrections Applied
```bash
python scripts/check_address_corrections_applied.py
```
**Expected Output**:
```
Corrections actually applied: 3,649+ (should match applied count)
Corrections NOT applied: 0
```

### 5. Generate Updated Report
```bash
python scripts/generate_esri_quality_report.py
```

### 6.