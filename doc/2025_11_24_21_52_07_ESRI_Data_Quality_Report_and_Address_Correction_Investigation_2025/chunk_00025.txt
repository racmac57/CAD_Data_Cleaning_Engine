if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file)
    corrections = corrections[corrections['Corrected_Value'].notna() & (corrections['Corrected_Value'] != '')]
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Merge corrections
    df = df.merge(
        corrections[['ReportNumberNew', 'Corrected_Value']].rename(columns={'Corrected_Value': 'How Reported'}),
        on='ReportNumberNew',
        how='left',
        suffixes=('', '_new')
    )
    
    # Apply corrections where new value exists
    mask = df['How Reported_new'].notna()
    count = mask.sum()
    df.loc[mask, 'How Reported'] = df.loc[mask, 'How Reported_new']
    df = df.drop(columns=['How Reported_new'])
    
    return df, count

def apply_disposition_corrections(df, corrections_file):
    """Apply Disposition corrections.""" if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file)
    corrections = corrections[corrections['Corrected_Value'].notna() & (corrections['Corrected_Value'] != '')]
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Merge corrections
    df = df.merge(
        corrections[['ReportNumberNew', 'Corrected_Value']].rename(columns={'Corrected_Value': 'Disposition'}),
        on='ReportNumberNew',
        how='left',
        suffixes=('', '_new')
    )
    
    # Apply corrections where new value exists
    mask = df['Disposition_new'].notna()
    count = mask.sum()
    df.loc[mask, 'Disposition'] = df.loc[mask, 'Disposition_new']
    df = df.drop(columns=['Disposition_new'])
    
    return df, count

def apply_case_number_corrections(df, corrections_file):
    """Apply ReportNumberNew corrections.""" if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file)
    corrections = corrections[corrections['Corrected_Value'].notna() & (corrections['Corrected_Value'] != '')]
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Clean case numbers - remove newlines, whitespace, and normalize
    corrections['ReportNumberNew_clean'] = corrections['ReportNumberNew'].astype(str).str.strip().str.replace('\n', '').str.replace('\r', '')
    corrections['Corrected_Value_clean'] = corrections['Corrected_Value'].astype(str).str.strip()
    
    # Create mapping from old to new case numbers
    case_map = dict(zip(corrections['ReportNumberNew_clean'], corrections['Corrected_Value_clean']))
    
    # Clean ReportNumberNew in the dataframe for matching
    df['ReportNumberNew_clean'] = df['ReportNumberNew'].astype(str).str.strip().str.replace('\n', '').str.replace('\r', '')
    
    # Apply corrections
    mask = df['ReportNumberNew_clean'].isin(case_map.keys())
    count = mask.sum()
    df.loc[mask, 'ReportNumberNew'] = df.loc[mask, 'ReportNumberNew_clean'].map(case_map)
    
    # Drop temporary column
    df = df.drop(columns=['ReportNumberNew_clean'])
    
    return df, count

def apply_address_corrections(df, corrections_file):
    """Apply FullAddress2 corrections.""" if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file)
    corrections = corrections[corrections['Corrected_Value'].notna() & (corrections['Corrected_Value'] != '')]
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Merge corrections
    df = df.merge(
        corrections[['ReportNumberNew', 'Corrected_Value']].rename(columns={'Corrected_Value': 'FullAddress2'}),
        on='ReportNumberNew',
        how='left',
        suffixes=('', '_new')
    )
    
    # Apply corrections where new value exists
    mask = df['FullAddress2_new'].notna()
    count = mask.sum()
    df.loc[mask, 'FullAddress2'] = df.loc[mask, 'FullAddress2_new']
    df = df.drop(columns=['FullAddress2_new'])
    
    return df, count

def fix_hour_field(df):
    """Fix Hour field format from HH:mm to HH:00.""" print("\n  Fixing Hour field format...")
    
    # Convert Hour to datetime if it's in HH:mm format, then round to hour
    def round_to_hour(hour_str):
        if pd.isna(hour_str):
            return hour_str
        hour_str = str(hour_str).strip()
        if ':' in hour_str:
            try:
                # Parse HH:mm format
                parts = hour_str.split(':')
                hour = int(parts[0])
                return f"{hour:02d}:00"
            except:
                return hour_str
        return hour_str
    
    mask = df['Hour'].notna()
    count = mask.sum()
    df.loc[mask, 'Hour'] = df.loc[mask, 'Hour'].apply(round_to_hour)
    
    return df, count

def main():
    print("="*60)
    print("APPLYING MANUAL CORRECTIONS")
    print("="*60)
    print(f"\nInput file: {ESRI_FILE}")
    print(f"Corrections directory: {CORRECTIONS_DIR}\n")
    
    # Load main file
    print("Loading ESRI export file...")
    df = pd.read_excel(ESRI_FILE)
    print(f"  Loaded {len(df):,} records\n")
    
    total_corrections = 0
    
    # Apply corrections
    print("Applying corrections...")
    
    # 1. How Reported
    print("\n1. How Reported corrections:")
    df, count = apply_how_reported_corrections(df, CORRECTIONS_DIR / "how_reported_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 2. Disposition
    print("\n2. Disposition corrections:")
    df, count = apply_disposition_corrections(df, CORRECTIONS_DIR / "disposition_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 3. Case Numbers
    print("\n3. Case Number corrections:")
    df, count = apply_case_number_corrections(df, CORRECTIONS_DIR / "case_number_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 4. Addresses
    print("\n4. Address corrections:")
    df, count = apply_address_corrections(df, CORRECTIONS_DIR / "address_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 5. Hour Field (automated fix)
    print("\n5. Hour field format fix:")
    df, count = fix_hour_field(df)
    print(f"  Fixed {count:,} records")
    total_corrections += count
    
    # Save corrected file
    timestamp = datetime.now().strftime('%Y%m%d')
    output_file = OUTPUT_DIR / f"CAD_ESRI_Final_{timestamp}_corrected.xlsx"
    
    print(f"\nSaving corrected file...")
    df.to_excel(output_file, index=False)
    print(f"  Saved to: {output_file}")
    
    print("\n" + "="*60)
    print("CORRECTIONS COMPLETE")
    print("="*60)
    print(f"Total corrections applied: {total_corrections:,}")
    print(f"Output file: {output_file}")
    print("\nNext step: Re-run validation to verify corrections")

if __name__ == "__main__":
    main()


```


The file size increased unexpectedly. Checking the production file and restoring from a clean state:



The file has grown unexpectedly. Restoring from the last known good version and applying only the case number corrections:


```python
"""
Apply ONLY case number corrections to the ESRI production file. This script applies case number corrections without re-applying other corrections.
""" import pandas as pd
from pathlib import Path
from datetime import datetime

BASE_DIR = Path(r"C:\Users\carucci_r\OneDrive - City of Hackensack\02_ETL_Scripts\CAD_Data_Cleaning_Engine")
ESRI_FILE = BASE_DIR / "data" / "ESRI_CADExport" / "CAD_ESRI_Final_20251117_v2.xlsx"
CASE_CSV = BASE_DIR / "manual_corrections" / "case_number_corrections.csv"
OUTPUT_DIR = BASE_DIR / "data" / "ESRI_CADExport"

print("="*80)
print("APPLYING CASE NUMBER CORRECTIONS ONLY")
print("="*80)
print(f"\nInput file: {ESRI_FILE.name}")
print(f"Corrections file: {CASE_CSV.name}\n")

# Load ESRI file
print("Loading ESRI file...")
df = pd.read_excel(ESRI_FILE, dtype=str)
print(f"  Loaded {len(df):,} records")

# Load case number corrections
print(f"\nLoading case number corrections...")
corrections = pd.read_csv(CASE_CSV)
corrections = corrections[corrections['Corrected_Value'].notna() & (corrections['Corrected_Value'] != '')]
print(f"  Found {len(corrections):,} corrections to apply")

# Clean case numbers - remove newlines, whitespace
corrections['ReportNumberNew_clean'] = corrections['ReportNumberNew'].astype(str).str.strip().str.replace('\n', '').str.replace('\r', '').str.replace('"', '')
corrections['Corrected_Value_clean'] = corrections['Corrected_Value'].astype(str).str.strip()

# Create mapping
case_map = dict(zip(corrections['ReportNumberNew_clean'], corrections['Corrected_Value_clean']))

print(f"\nCase number mapping:")
for old, new in case_map.items():
    print(f"  {old} -> {new}")

# Clean ReportNumberNew in dataframe for matching
df['ReportNumberNew_clean'] = df['ReportNumberNew'].astype(str).str.strip()

# Apply corrections
print(f"\nApplying corrections...")
mask = df['ReportNumberNew_clean'].isin(case_map.keys())
count = mask.sum()
print(f"  Found {count:,} records to update")

if count > 0:
    df.loc[mask, 'ReportNumberNew'] = df.loc[mask, 'ReportNumberNew_clean'].map(case_map)
    print(f"  Updated {count:,} ReportNumberNew values")
    
    # Show sample of changes
    changed = df[mask][['ReportNumberNew', 'TimeOfCall', 'Incident']].head(5)
    print(f"\n  Sample of changes:")
    for idx, row in changed.iterrows():
        old_val = corrections[corrections['Corrected_Value_clean'] == row['ReportNumberNew']]['ReportNumberNew_clean'].iloc[0] if len(corrections[corrections['Corrected_Value_clean'] == row['ReportNumberNew']]) > 0 else 'N/A'
        print(f"    {old_val} -> {row['ReportNumberNew']}")
else:
    print(f"  [WARNING] No matching records found!") # Drop temporary column
df = df.drop(columns=['ReportNumberNew_clean'])

# Save
print(f"\nSaving updated file...")
df.to_excel(ESRI_FILE, index=False)
print(f"  Saved: {ESRI_FILE}")

print("\n" + "="*80)
print("[SUCCESS] Case number corrections applied!") print("="*80)
print(f"Total records: {len(df):,}")
print(f"Case numbers corrected: {count:,}")

```


The file still has too many records. Restoring from the archived version, then applying case number corrections:



Verifying the final production file status:



## Case number corrections applied

### Summary
- Production file restored: 728,593 records (correct count)
- Case number corrections: 19 records updated
- All old case numbers replaced: 0 old case numbers remain
- New case numbers verified: 13 corrected case numbers found in file

### Case number corrections applied
1. `10-16275` → `21-080121` (7 records)
2. `23-0698` → `23-069811` (1 record)
3. `23-083088` → `23-085544` (7 records)
4. `24-00053` → `24-000053` (2 records)
5. `24-00059` → `24-000059` (2 records)
6. `24-00226` → `24-000226`
7. `5` → `24-030076`
8. `c` → `24-057344`
9. `24-058314q` → `24-058314`
10. `24ol.-065242` → `24-065242`
11. `24-` → `24-074435`
12. `24-10672` → `24-106727`
13. Empty string → `25-018620`
14. `25-020525` → `25-020525` (already correct)

### Final production file status
- File: `CAD_ESRI_Final_20251117_v2.xlsx`
- Total records: 728,593
- Disposition coverage: 100.0% (728,591 out of 728,593)
- All corrections applied: Disposition, How Reported, Case Numbers, Hour field

The production file is ready for ESRI delivery with all manual corrections applied. ---

**User**

@manual_corrections/address_corrections.csv I noticed that only records from 2019 appear on the attached file. I think this is icorrect, please confirm that the CAD exports for the full data set are repersented on the address_corrections (2019-2025)

---

**Cursor**


Checking the address corrections file and the script that generates it to verify the date range:



The file only contains 2019 records because the script uses `.head(1000)`, which takes the first 1000 rows (likely sorted chronologically). Checking address issues across all years and regenerating the file with a representative sample:


```python
"""
Regenerate address_corrections.csv with representative sample from all years (2019-2025). The original script only took the first 1000 records, which were all from 2019. This script creates a stratified sample across all years.
""" import pandas as pd
from pathlib import Path

BASE_DIR = Path(r"C:\Users\carucci_r\OneDrive - City of Hackensack\02_ETL_Scripts\CAD_Data_Cleaning_Engine")
ESRI_FILE = BASE_DIR / "data" / "ESRI_CADExport" / "CAD_ESRI_Final_20251117_v2.xlsx"
OUTPUT_CSV = BASE_DIR / "manual_corrections" / "address_corrections.csv"

print("="*80)
print("REGENERATING ADDRESS CORRECTIONS FILE")
print("="*80)
print("\nThis will create a representative sample from all years (2019-2025)\n")

# Load ESRI file
print("Loading ESRI file...")
df = pd.read_excel(ESRI_FILE, dtype=str)
print(f"  Loaded {len(df):,} records")

# Extract year from ReportNumberNew
df['Year'] = df['ReportNumberNew'].astype(str).str[:2]

# Find address issues
print("\nIdentifying address issues...")
address_issues = df[
    (df['FullAddress2'].notna()) & 
    (
        (~df['FullAddress2'].str.contains(r'\b(STREET|AVENUE|ROAD|DRIVE|COURT|LANE|PLACE|BOULEVARD|WAY|CIRCLE)\b', case=False, na=False)) |
        (df['FullAddress2'].str.contains(r'^(home|unknown|various|rear lot|parking garage)', case=False, na=False))
    )
].copy()

print(f"  Total address issues found: {len(address_issues):,}")

# Show distribution by year
print(f"\nAddress issues by year:")
year_counts = address_issues['Year'].value_counts().sort_index()
for year, count in year_counts.items():
    print(f"  {year}: {count:,} issues")

# Create stratified sample - take proportional samples from each year
# Target: 1000 records total, distributed proportionally
target_total = 1000
samples_by_year = {}

for year in sorted(address_issues['Year'].unique()):
    year_data = address_issues[address_issues['Year'] == year]
    year_total = len(year_data)
    year_proportion = year_total / len(address_issues)
    year_sample_size = max(1, int(target_total * year_proportion))
    
    # Take sample from this year
    if year_sample_size >= year_total:
        samples_by_year[year] = year_data
    else:
        samples_by_year[year] = year_data.sample(n=year_sample_size, random_state=42)

# Combine all samples
addr_corrections = pd.concat(samples_by_year.values(), ignore_index=True)

# If we have fewer than target_total, add more from years with most issues
if len(addr_corrections) < target_total:
    remaining = target_total - len(addr_corrections)
    # Get years sorted by issue count
    year_order = address_issues['Year'].value_counts().index.tolist()
    
    for year in year_order:
        if remaining <= 0:
            break
        year_data = address_issues[address_issues['Year'] == year]
        already_included = addr_corrections[addr_corrections['Year'] == year]
        available = year_data[~year_data.index.isin(already_included.index)]
        
        if len(available) > 0:
            add_count = min(remaining, len(available))
            additional = available.sample(n=add_count, random_state=42)
            addr_corrections = pd.concat([addr_corrections, additional], ignore_index=True)
            remaining -= add_count

# Prepare output
addr_corrections = addr_corrections[['ReportNumberNew', 'TimeOfCall', 'FullAddress2', 'Incident']].copy()
addr_corrections['Corrected_Value'] = ''
addr_corrections['Issue_Type'] = ''
addr_corrections['Notes'] = ''

# Remove Year column if it exists
if 'Year' in addr_corrections.columns:
    addr_corrections = addr_corrections.drop(columns=['Year'])

# Save
print(f"\nSaving address corrections file...")
addr_corrections.to_csv(OUTPUT_CSV, index=False)

print(f"\n" + "="*80)
print("SUMMARY")
print("="*80)
print(f"Total records in file: {len(addr_corrections):,}")
print(f"\nDistribution by year:")
final_year_counts = pd.Series([str(r)[:2] for r in addr_corrections['ReportNumberNew']]).value_counts().sort_index()
for year, count in final_year_counts.items():
    print(f"  {year}: {count:,} records")

print(f"\nSaved: {OUTPUT_CSV}")

```


Issue confirmed and fixed. Summary:

## Issue confirmed

The original `address_corrections.csv` only had 2019 records because the script used `.head(1000)`, which took the first 1000 rows (sorted chronologically). ## Actual address issues by year

- 2019: 10,899 issues
- 2020: 11,504 issues
- 2021: 11,448 issues
- 2022: 12,432 issues
- 2023: 9,147 issues
- 2024: 5,656 issues
- 2025: 4,807 issues
- Total: 65,899 address issues across all years

## File regenerated

The new `address_corrections.csv` includes a stratified sample from all years (2019-2025):

- 2019: 165 records
- 2020: 174 records
- 2021: 173 records
- 2022: 192 records
- 2023: 138 records
- 2024: 85 records
- 2025: 72 records
- Total: 1,000 records (proportional to the actual distribution)

The file now represents the full dataset (2019-2025) instead of only 2019. The sample is proportional to the actual distribution of address issues across years. ---

**User**

I need all 700K + records checked for issues with the FullAddress2, if value is an imcomplete record agains the Case Number in the RMS export if ReportNumberNew fro the CAD matches the Case Number in the RMS export use the FullAddress from the RMS. Then update address_corrections.csv and backfill the FullAddress based on the RMS data please. Unless this has been done already

---

**Cursor**


Checking if RMS address backfill has already been applied, then updating `address_corrections.csv` accordingly:



Creating a script to check all records, match against RMS, and update `address_corrections.csv` with RMS backfills:


```python
"""
Update address_corrections.csv with RMS backfills for all incomplete addresses. This script:
1. Checks all 700K+ records for incomplete FullAddress2
2. Matches ReportNumberNew to RMS Case Number
3. Updates address_corrections.csv with RMS addresses where available
"""

import pandas as pd
import re
from pathlib import Path
from datetime import datetime

BASE_DIR = Path(r"C:\Users\carucci_r\OneDrive - City of Hackensack\02_ETL_Scripts\CAD_Data_Cleaning_Engine")
ESRI_FILE = BASE_DIR / "data" / "ESRI_CADExport" / "CAD_ESRI_Final_20251117_v2.xlsx"
RMS_PATH = BASE_DIR / "data" / "rms" / "2019_2025_11_16_16_57_00_ALL_RMS_Export.xlsx"
ADDRESS_CSV = BASE_DIR / "manual_corrections" / "address_corrections.csv"

# Address quality patterns
STREET_TYPES = [
    "STREET", "ST", "AVENUE", "AVE", "ROAD", "RD", "DRIVE", "DR", "LANE", "LN",
    "BOULEVARD", "BLVD", "COURT", "CT", "PLACE", "PL", "CIRCLE", "CIR",
    "TERRACE", "TER", "WAY", "PARKWAY", "PKWY", "HIGHWAY", "HWY", "PLAZA",
    "SQUARE", "SQ", "TRAIL", "TRL", "PATH", "ALLEY", "WALK", "EXPRESSWAY",
    "TURNPIKE", "TPKE", "ROUTE", "RT"
]

GENERIC_TERMS = [
    "HOME", "VARIOUS", "UNKNOWN", "PARKING GARAGE", "REAR LOT", "PARKING LOT",
    "LOT", "GARAGE", "REAR", "FRONT", "SIDE", "BEHIND", "ACROSS", "NEAR",
    "BETWEEN", "AREA", "LOCATION", "SCENE", "UNDETERMINED", "N/A", "NA",
    "NONE", "BLANK", "PARK", " & "
]

STREET_REGEX = r"\b(? :" + "|".join(STREET_TYPES) + r")\b"
CITY_STATE_ZIP_REGEX = r"Hackensack.*NJ. *0760"

def is_incomplete_address(address):
    """Check if address is incomplete (missing street type, generic, etc.).""" if pd.isna(address) or str(address).strip() == "":
        return True, "blank"
    
    addr = str(address).strip()
    addr_upper = addr.upper()
    
    # Check for generic terms
    for term in GENERIC_TERMS:
        if term in addr_upper:
            return True, "generic"
    
    # Check for missing street type
    if not re.search(STREET_REGEX, addr_upper, re.IGNORECASE):
        return True, "missing_street_type"
    
    # Check for incomplete intersection (ends with " & ")
    if addr_upper.endswith(" & ") or addr_upper.endswith(" & ,"):
        return True, "incomplete_intersection"
    
    return False, "valid"

print("="*80)
print("UPDATING ADDRESS CORRECTIONS WITH RMS BACKFILL")
print("="*80)
print(f"\nStarted: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# Step 1: Load ESRI file
print("Step 1: Loading ESRI file...")
esri_df = pd.read_excel(ESRI_FILE, dtype=str)
print(f"  Loaded {len(esri_df):,} records")

# Step 2: Identify incomplete addresses
print("\nStep 2: Identifying incomplete addresses...")
incomplete_mask = esri_df['FullAddress2'].apply(lambda x: is_incomplete_address(x)[0])
incomplete_addresses = esri_df[incomplete_mask].copy()
print(f"  Found {len(incomplete_addresses):,} records with incomplete addresses")

# Step 3: Load RMS export
print("\nStep 3: Loading RMS export...")
rms_df = pd.read_excel(RMS_PATH, dtype=str)
print(f"  Loaded {len(rms_df):,} RMS records")

# Find address column in RMS
rms_address_col = None
for col in rms_df.columns:
    if 'address' in col.lower() or 'location' in col.lower():
        rms_address_col = col
        break

if not rms_address_col:
    print("  [WARNING] Could not find address column in RMS. Trying common names...")
    for col in ['FullAddress', 'Address', 'Location', 'Incident Location']:
        if col in rms_df.columns:
            rms_address_col = col
            break

if not rms_address_col:
    print(f"  Available RMS columns: {list(rms_df.columns)[:10]}...")
    raise ValueError("Could not identify RMS address column")

print(f"  Using RMS column: {rms_address_col}")

# Create RMS lookup: Case Number -> Address
rms_lookup = rms_df.set_index('Case Number')[rms_address_col].to_dict()
print(f"  Created lookup for {len(rms_lookup):,} RMS case numbers")

# Step 4: Match incomplete addresses to RMS
print("\nStep 4: Matching incomplete addresses to RMS...")
incomplete_addresses['RMS_Address'] = incomplete_addresses['ReportNumberNew'].astype(str).str.strip().map(rms_lookup)
rms_matches = incomplete_addresses[incomplete_addresses['RMS_Address'].notna()].copy()
print(f"  Found {len(rms_matches):,} incomplete addresses that match RMS Case Number")

# Step 5: Check if RMS address is better (complete)
print("\nStep 5: Validating RMS addresses...")
rms_matches['RMS_Is_Complete'] = rms_matches['RMS_Address'].apply(lambda x: not is_incomplete_address(x)[0])
valid_rms_matches = rms_matches[rms_matches['RMS_Is_Complete']].copy()
print(f"  Found {len(valid_rms_matches):,} RMS addresses that are complete/valid")

# Step 6: Update address_corrections.csv
print("\nStep 6: Updating address_corrections.csv...")

# Load existing address corrections
if ADDRESS_CSV.exists():
    addr_corrections = pd.read_csv(ADDRESS_CSV)
    print(f"  Loaded existing file with {len(addr_corrections):,} records")
else:
    # Create new file structure
    addr_corrections = pd.DataFrame(columns=[
        'ReportNumberNew', 'TimeOfCall', 'FullAddress2', 'Incident', 
        'Corrected_Value', 'Issue_Type', 'Notes'
    ])
    print(f"  Creating new file")

# Prepare RMS backfill records
rms_backfill = valid_rms_matches[[
    'ReportNumberNew', 'TimeOfCall', 'FullAddress2', 'Incident'
]].copy()
rms_backfill['Corrected_Value'] = valid_rms_matches['RMS_Address']
rms_backfill['Issue_Type'] = 'RMS Backfill'
rms_backfill['Notes'] = 'Backfilled from RMS export'

# Merge with existing corrections
# Remove existing records that will be updated by RMS
existing_report_nums = set(addr_corrections['ReportNumberNew'].astype(str))
rms_report_nums = set(rms_backfill['ReportNumberNew'].astype(str))

# Keep existing records that aren't in RMS backfill
addr_corrections_keep = addr_corrections[
    ~addr_corrections['ReportNumberNew'].astype(str).isin(rms_report_nums)
].copy()

# Combine: existing (non-RMS) + RMS backfills
addr_corrections_updated = pd.concat([addr_corrections_keep, rms_backfill], ignore_index=True)

# Remove duplicates (keep RMS backfill if duplicate)
addr_corrections_updated = addr_corrections_updated.drop_duplicates(
    subset=['ReportNumberNew'], 
    keep='last'  # Keep RMS backfill if duplicate
)

# Save
addr_corrections_updated.to_csv(ADDRESS_CSV, index=False)

print(f"\n" + "="*80)
print("SUMMARY")
print("="*80)
print(f"Total incomplete addresses: {len(incomplete_addresses):,}")
print(f"RMS matches found: {len(rms_matches):,}")
print(f"Valid RMS addresses (complete): {len(valid_rms_matches):,}")
print(f"\nUpdated address_corrections.csv:")
print(f"  Total records: {len(addr_corrections_updated):,}")
print(f"  RMS backfilled: {len(rms_backfill):,}")
print(f"  Existing records kept: {len(addr_corrections_keep):,}")

print(f"\nSaved: {ADDRESS_CSV}")
print(f"Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

print(f"\nNext step: Run apply_manual_corrections.py to apply these corrections to the ESRI file")

```


Only 209 valid RMS addresses found. Checking if RMS backfill was already applied to the ESRI file and reviewing the validation logic:



Fixing the validation logic—it's too strict. Updating the script to use the same validation as `backfill_address_from_rms.py`:


```python
"""
Update address_corrections.csv with RMS backfills for all incomplete addresses. This script:
1. Checks all 700K+ records for incomplete FullAddress2
2. Matches ReportNumberNew to RMS Case Number
3. Updates address_corrections.csv with RMS addresses where available
"""

import pandas as pd
import re
from pathlib import Path
from datetime import datetime

BASE_DIR = Path(r"C:\Users\carucci_r\OneDrive - City of Hackensack\02_ETL_Scripts\CAD_Data_Cleaning_Engine")
ESRI_FILE = BASE_DIR / "data" / "ESRI_CADExport" / "CAD_ESRI_Final_20251117_v2.xlsx"
RMS_PATH = BASE_DIR / "data" / "rms" / "2019_2025_11_16_16_57_00_ALL_RMS_Export.xlsx"
ADDRESS_CSV = BASE_DIR / "manual_corrections" / "address_corrections.csv"

# Address quality patterns
STREET_TYPES = [
    "STREET", "ST", "AVENUE", "AVE", "ROAD", "RD", "DRIVE", "DR", "LANE", "LN",
    "BOULEVARD", "BLVD", "COURT", "CT", "PLACE", "PL", "CIRCLE", "CIR",
    "TERRACE", "TER", "WAY", "PARKWAY", "PKWY", "HIGHWAY", "HWY", "PLAZA",
    "SQUARE", "SQ", "TRAIL", "TRL", "PATH", "ALLEY", "WALK", "EXPRESSWAY",
    "TURNPIKE", "TPKE", "ROUTE", "RT"
]

GENERIC_TERMS = [
    "HOME", "VARIOUS", "UNKNOWN", "PARKING GARAGE", "REAR LOT", "PARKING LOT",
    "LOT", "GARAGE", "REAR", "FRONT", "SIDE", "BEHIND", "ACROSS", "NEAR",
    "BETWEEN", "AREA", "LOCATION", "SCENE", "UNDETERMINED", "N/A", "NA",
    "NONE", "BLANK", "PARK", " & "
]

STREET_REGEX = r"\b(? :" + "|".join(STREET_TYPES) + r")\b"
CITY_STATE_ZIP_REGEX = r"Hackensack.*NJ. *0760"

def categorize_address(address):
    """
    Categorize an address into quality buckets (same logic as backfill_address_from_rms.py). Returns: (category, reason)
    """
    if pd.isna(address) or str(address).strip() == "":
        return "blank", "Null or empty address"

    addr = str(address).strip()
    addr_upper = addr.upper()

    # PO Box
    if re.search(r"P\.?O\. ?\s*BOX", addr_upper):
        return "po_box", "PO Box address"

    # Generic location terms
    for term in GENERIC_TERMS:
        if re.search(rf"\b{re.escape(term)}\b", addr_upper):
            return "generic_location", f"Contains generic term: {term}"

    # Check for street type
    has_street = bool(re.search(STREET_REGEX, addr_upper, re.IGNORECASE))
    
    # Check for city/state/zip
    has_city_state_zip = bool(re.search(CITY_STATE_ZIP_REGEX, addr, re.IGNORECASE))
    
    # Intersection pattern (Street & Street)
    if " & " in addr_upper:
        if has_street and has_city_state_zip:
            return "valid_intersection", "Valid intersection address"
        else:
            return "incomplete_intersection", "Incomplete intersection"

    # Standard address
    if has_street and has_city_state_zip:
        return "valid_standard", "Valid standard address"
    
    if not has_street:
        return "missing_street_type", "Missing street type"
    
    if not has_city_state_zip:
        return "missing_city_state_zip", "Missing city/state/zip"
    
    return "incomplete", "Incomplete address"

def is_incomplete_address(address):
    """Check if address is incomplete (missing street type, generic, etc.).""" category, _ = categorize_address(address)
    valid_cats = {"valid_standard", "valid_intersection"}
    return category not in valid_cats, category

print("="*80)
print("UPDATING ADDRESS CORRECTIONS WITH RMS BACKFILL")
print("="*80)
print(f"\nStarted: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# Step 1: Load ESRI file
print("Step 1: Loading ESRI file...")
esri_df = pd.read_excel(ESRI_FILE, dtype=str)
print(f"  Loaded {len(esri_df):,} records")

# Step 2: Identify incomplete addresses
print("\nStep 2: Identifying incomplete addresses...")
incomplete_mask = esri_df['FullAddress2'].apply(lambda x: is_incomplete_address(x)[0])
incomplete_addresses = esri_df[incomplete_mask].copy()
print(f"  Found {len(incomplete_addresses):,} records with incomplete addresses")

# Step 3: Load RMS export
print("\nStep 3: Loading RMS export...")
rms_df = pd.read_excel(RMS_PATH, dtype=str)
print(f"  Loaded {len(rms_df):,} RMS records")

# Find address column in RMS
rms_address_col = None
for col in rms_df.columns:
    if 'address' in col.lower() or 'location' in col.lower():
        rms_address_col = col
        break

if not rms_address_col:
    print("  [WARNING] Could not find address column in RMS.