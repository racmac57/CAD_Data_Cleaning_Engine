Returns:
            dict: Extrapolated results with estimated pass/fail counts.
        """ extrapolated = {'critical_rules': {}, 'important_rules': {}, 'optional_rules': {}}
        full_size = len(full_df)
        sample_size = sample_results['sample_size']
        factor = full_size / sample_size if sample_size > 0 else 0
        
        for category in extrapolated.keys():
            for rule_id, res in sample_results[category].items():
                extrapolated[category][rule_id] = {
                    'rule_id': rule_id, 'description': res['description'], 'severity': res['severity'],
                    'sample_passed': res['passed'], 'sample_failed': res['failed'],
                    'sample_pass_rate': res['pass_rate'],
                    'estimated_full_passed': int(res['passed'] * factor),
                    'estimated_full_failed': int(res['failed'] * factor),
                    'estimated_full_pass_rate': res['pass_rate'],
                    'failed_records': res.get('failed_records', {}),
                    'fix_suggestion': res.get('fix_suggestion', '')
                }
        extrapolated['overall_quality_score'] = self._calculate_overall_quality_score(extrapolated)
        return extrapolated

    def _calculate_overall_quality_score(self, results: dict) -> float:
        """Calculate overall data quality score based on rule pass rates. Args:
            results (dict): Validation results with pass rates. Returns:
            float: Weighted quality score (0-100).
        """ weights = {'critical': 0.5, 'important': 0.3, 'optional': 0.2}
        score, total_weight = 0.0, 0.0
        for category, weight in weights.items():
            rates = [r['estimated_full_pass_rate'] for r in results[f"{category}_rules"].values()]
            if rates:
                avg_rate = sum(rates) / len(rates)
                score += avg_rate * weight
                total_weight += weight
        return (score / total_weight * 100) if total_weight > 0 else 0.0

    def _generate_validation_recommendations(self, results: dict) -> list:
        """Generate recommendations based on validation results. Args:
            results (dict): Validation results with pass rates. Returns:
            list: List of recommendation strings for failed rules.
        """ recommendations = []
        for category in ['critical', 'important', 'optional']:
            threshold = self.sampling_config['quality_thresholds'][category]
            for rule_id, res in results[f"{category}_rules"].items():
                if res['estimated_full_pass_rate'] < threshold:
                    recommendations.append(
                        f"{category.upper()}: {res['description']} - "
                        f"Pass rate: {res['estimated_full_pass_rate']:.1%} "
                        f"(threshold: {threshold:.1%})"
                    )
                    recommendations.append(f"  Fix suggestion: {res['fix_suggestion']}")
        unmapped = self.validation_results.get('unmapped_incidents', [])
        if unmapped:
            sample_examples = ', '.join(list(unmapped)[:5])
            recommendations.append(
                f"INCIDENT MAPPING: {len(unmapped)} incident types not found in CallType_Categories.csv. " f"Examples: {sample_examples}"
            )
        address_issues = self.validation_results.get('address_issue_summary') or {}
        if address_issues:
            top_issue, count = max(address_issues.items(), key=lambda item: item[1])
            recommendations.append(
                f"ADDRESS QUALITY: {count} address records flagged (top issue: {top_issue}). " f"Review CAD vs RMS/Geocode backfill." )
        return recommendations

    def create_validation_report(self, output_path: str = None) -> str:
        """Create a comprehensive validation report in JSON format. Args:
            output_path (str, optional): Path to save the report. Defaults to timestamped filename. Returns:
            str: Path to the saved report file.
        """ dataset_label = self.validation_results.get('source_dataset') or getattr(self, 'current_dataset_label', None)
        if output_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            prefix = ''
            if dataset_label:
                label_stem = os.path.splitext(os.path.basename(dataset_label))[0]
                year_match = re.match(r'^(\d{4})', label_stem)
                if year_match:
                    prefix = f"{year_match.group(1)}_"
                else:
                    sanitized = re.sub(r'[^A-Za-z0-9]+', '_', label_stem).strip('_')
                    if sanitized:
                        prefix = f"{sanitized}_"
            output_path = f"{prefix}cad_validation_report_{timestamp}.json"

        report = {
            'validation_metadata': {
                'timestamp': datetime.now().isoformat(),
                'validator_version': 'CAD_Validator_2025.10.17',
                'total_records_validated': self.validation_results['total_records'],
                'stratification_method': self.validation_results.get('stratification_method', 'unknown'),
                'source_dataset': dataset_label,
                'report_filename': output_path
            },
            'validation_summary': {
                'overall_quality_score': self.validation_results.get('overall_quality_score', 0),
                'recommendations_count': len(self.validation_results.get('recommended_actions', []))
            },
            'sampling_metadata': {
                'stratum_distribution': self.validation_results.get('stratum_distribution', {}),
                'stratification_method': self.validation_results.get('stratification_method', 'unknown'),
                'sample_size': self.sampling_config.get('stratified_sample_size', 1000)
            },
            'recommended_actions': self.validation_results.get('recommended_actions', []),
            'validation_details': self.validation_results,
            'sampling_configuration': self.sampling_config
        }
        def clean_for_json(obj):
            """Recursively clean data structures for JSON serialization.""" if isinstance(obj, dict):
                return {str(k): clean_for_json(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [clean_for_json(item) for item in obj]
            elif hasattr(obj, 'isoformat'):  # datetime objects
                return obj.isoformat()
            elif isinstance(obj, (pd.Timestamp, pd.Timedelta)):
                return str(obj)
            elif hasattr(obj, 'tolist'):  # numpy arrays
                return obj.tolist()
            elif hasattr(obj, '__dict__'):  # custom objects
                return str(obj)
            else:
                return obj
        
        # Clean the report data before JSON serialization
        cleaned_report = clean_for_json(report)
        
        with open(output_path, 'w') as f:
            json.dump(cleaned_report, f, indent=2)
        logger.info(f"Validation report saved: {output_path}")
        return output_path

    def _initialize_validation_rules(self) -> dict:
        """Initialize comprehensive validation rules for CAD data. Returns:
            dict: Dictionary of validation rules organized by severity.
        """ return {
            'critical_rules': {
                'case_number_format': {
                    'rule_id': 'CRIT_001',
                    'description': 'Case number must follow format: YY-XXXXXX or YY-XXXXXX[A-Z] for supplements',
                    'field': 'ReportNumberNew',
                    'pattern': r'^\d{2,4}-\d{6}[A-Z]?$',
                    'severity': 'critical',
                    'fix_suggestion': 'Standardize case number format, pad with zeros if needed'
                },
                'case_number_uniqueness': {
                    'rule_id': 'CRIT_002',
                    'description': 'Case numbers must be unique within dataset',
                    'field': 'ReportNumberNew',
                    'severity': 'critical',
                    'fix_suggestion': 'Remove duplicates, investigate duplicate sources'
                },
                'call_datetime_validity': {
                    'rule_id': 'CRIT_003',
                    'description': 'Time of Call must be valid datetime within reasonable range',
                    'field': 'Time of Call',
                    'severity': 'critical',
                    'fix_suggestion': 'Validate datetime parsing, check for timezone issues'
                },
                'incident_type_presence': {
                    'rule_id': 'CRIT_004',
                    'description': 'Incident type must not be null or empty',
                    'field': 'Incident',
                    'severity': 'critical',
                    'fix_suggestion': 'Map null values to "Unknown" or investigate missing data'
                }
            },
            'important_rules': {
                'address_completeness': {
                    'rule_id': 'IMP_001',
                    'description': 'Address should be present and not generic',
                    'field': 'FullAddress2',
                    'severity': 'important',
                    'fix_suggestion': 'Validate address completeness, flag generic addresses'
                },
                'officer_assignment': {
                    'rule_id': 'IMP_002',
                    'description': 'Officer field should not be null for dispatched calls',
                    'field': 'Officer',
                    'severity': 'important',
                    'fix_suggestion': 'Check officer assignment logic, validate against roster'
                },
                'disposition_consistency': {
                    'rule_id': 'IMP_003',
                    'description': 'Disposition should be from approved list',
                    'field': 'Disposition',
                    'severity': 'important',
                    'fix_suggestion': 'Standardize disposition values, create lookup table'
                },
                'time_sequence_validity': {
                    'rule_id': 'IMP_004',
                    'description': 'Time sequence should be logical (Call -> Dispatch -> Out -> In)',
                    'fields': ['Time of Call', 'Time Dispatched', 'Time Out', 'Time In'],
                    'severity': 'important',
                    'fix_suggestion': 'Validate time sequence logic, flag outliers'
                },
                'datetime_duration_validity': {
                    'rule_id': 'IMP_005',
                    'description': 'Durations between datetime fields must be non-negative',
                    'fields': ['Time of Call', 'Time Dispatched', 'Time Out', 'Time In'],
                    'severity': 'important',
                    'fix_suggestion': 'Correct negative durations or investigate data entry errors'
                },
                'time_spent_validity': {
                    'rule_id': 'IMP_006',
                    'description': 'Time Spent must be a positive duration',
                    'field': 'Time Spent',
                    'severity': 'important',
                    'fix_suggestion': 'Validate duration format and ensure positive values'
                }
            },
            'optional_rules': {
                'how_reported_standardization': {
                    'rule_id': 'OPT_001',
                    'description': 'How Reported should be from standardized list',
                    'field': 'How Reported',
                    'severity': 'optional',
                    'fix_suggestion': 'Map variations to standard values (9-1-1, Phone, Walk-in, etc.)' },
                'zone_validity': {
                    'rule_id': 'OPT_002',
                    'description': 'PD Zone should be valid zone identifier',
                    'field': 'PDZone',
                    'severity': 'optional',
                    'fix_suggestion': 'Validate against zone master list'
                },
                'response_type_consistency': {
                    'rule_id': 'OPT_003',
                    'description': 'Response Type should align with incident severity',
                    'fields': ['Response Type', 'Incident'],
                    'severity': 'optional',
                    'fix_suggestion': 'Create response type validation matrix'
                }
            }
        }

def validate_cad_dataset_with_sampling(
    config: Dict[str, Any],
    sampling_method: str = 'stratified',
    validator: CADDataValidator | None = None
):
    """Validate CAD dataset with specified sampling method. Args:
        config (Dict[str, Any]): Configuration dictionary with 'file_path' and optional 'config_path'. sampling_method (str): Sampling method ('stratified', 'systematic', 'random'). Defaults to 'stratified'. validator (CADDataValidator | None): Existing validator instance to reuse. Returns:
        tuple: Validation results and path to the saved report. Raises:
        ValueError: If 'file_path' is missing from config.
    """ cad_file = config.get('file_path', '')
    if not cad_file:
        raise ValueError("Config must include 'file_path' key.") logger.info(f"Loading data from: {cad_file}")
    df = pd.read_excel(cad_file) if cad_file.endswith('.xlsx') else pd.read_csv(cad_file)
    if validator is None:
        validator = CADDataValidator(config_path=config.get('config_path', 'config_enhanced.json'))
    else:
        config_path = config.get('config_path')
        if config_path and config_path != validator.config_path:
            validator.reload_config(config_path)
    dataset_label = os.path.basename(cad_file)
    validator.current_dataset_label = dataset_label
    validator.validation_results['source_dataset'] = dataset_label
    results = validator.validate_cad_dataset(df, sampling_method)
    report_path = validator.create_validation_report()
    return results, report_path

# --- Unit Tests ---

@pytest.fixture
def sample_data():
    """Provide sample data for testing, including edge cases.""" return pd.DataFrame({
        'ReportNumberNew': ['25-123456', '25-987654A', '25-123457', '25-123456B', None, 'invalid', '25-123456AB'],
        'Incident': ['Noise Complaint', 'ROBBERY', 'Task Assignment', 'ASSAULT', '', None, 'UNKNOWN'],
        'How Reported': ['PHONE', '9-1-1', 'INVALID', 'RADIO', None, '2024-01-01', 'EMAIL'],
        'FullAddress2': ['123 Main Street, Hackensack, NJ, 07601', 'INVALID', '456 Elm St, Hackensack, NJ, 07601', 'Main & Atlantic, Hackensack, NJ, 07601', None, '123 Main St', '123 Main Street, Other City, NJ, 00000'],
        'PDZone': ['7', '5', '10', '6', None, '4', '9'],
        'Time of Call': ['2025-10-17 17:00:00', '2025-10-17 17:05:00', '2025-10-17 17:10:00', '2025-10-17 17:15:00', '2035-01-01 00:00:00', None, '2019-01-01 00:00:00'],
        'Time Dispatched': ['2025-10-17 17:02:00', '2025-10-17 17:06:00', '2025-10-17 17:12:00', '2025-10-17 17:14:00', None, '2025-10-17 16:59:00', '2025-10-17 17:16:00'],
        'Time Out': ['2025-10-17 17:03:00', '2025-10-17 17:07:00', '2025-10-17 17:13:00', '2025-10-17 17:16:00', None, '2025-10-17 17:00:00', '2025-10-17 17:15:00'],
        'Time In': ['2025-10-17 17:04:00', '2025-10-17 17:08:00', '2025-10-17 17:14:00', '2025-10-17 17:17:00', None, '2025-10-17 16:58:00', '2025-10-17 17:18:00'],
        'Time Spent': ['0 days 00:04:00', '0 days 00:03:00', '-0 days 00:01:00', '0 days 25:00:00', None, 'invalid', '0 days 00:00:00'],
        'Officer': ['P.O. John Doe', None, 'P.O. Jane Doe', 'P.O. John Doe', '', 'P.O. Invalid', None],
        'Disposition': ['COMPLETE', 'ADVISED', 'INVALID', 'COMPLETE', None, 'COMPLETE', 'ADVISED'],
        'Response Type': ['ROUTINE', 'EMERGENCY', 'ROUTINE', 'URGENT', None, 'EMERGENCY', 'ROUTINE']
    })

@pytest.fixture
def validator():
    """Provide a test instance of CADDataValidator.""" return CADDataValidator()

def test_validate_case_number_format(validator, sample_data):
    result = validator._validate_case_number_format(sample_data, {'rule_id': 'CRIT_001', 'field': 'ReportNumberNew', 'severity': 'critical'})
    assert result['passed'] == 4
    assert result['failed'] == 3

def test_validate_case_number_uniqueness(validator, sample_data):
    result = validator._validate_case_number_uniqueness(sample_data, {'rule_id': 'CRIT_002', 'field': 'ReportNumberNew', 'severity': 'critical'})
    assert result['passed'] == 6
    assert result['failed'] == 1

def test_validate_call_datetime(validator, sample_data):
    result = validator._validate_call_datetime(sample_data, {'rule_id': 'CRIT_003', 'field': 'Time of Call', 'severity': 'critical'})
    assert result['passed'] == 4
    assert result['failed'] == 3

def test_validate_incident_type_presence(validator, sample_data):
    result = validator._validate_incident_type_presence(sample_data, {'rule_id': 'CRIT_004', 'field': 'Incident', 'severity': 'critical'})
    assert result['passed'] == 5
    assert result['failed'] == 2

def test_validate_address_completeness(validator, sample_data):
    result = validator._validate_address_completeness(sample_data, {'rule_id': 'IMP_001', 'field': 'FullAddress2', 'severity': 'important'})
    assert result['passed'] == 2
    assert result['failed'] == 5

def test_validate_officer_assignment(validator, sample_data):
    result = validator._validate_officer_assignment(sample_data, {'rule_id': 'IMP_002', 'field': 'Officer', 'severity': 'important'})
    assert result['passed'] == 5
    assert result['failed'] == 2

def test_validate_disposition_consistency(validator, sample_data):
    result = validator._validate_disposition_consistency(sample_data, {'rule_id': 'IMP_003', 'field': 'Disposition', 'severity': 'important'})
    assert result['passed'] == 5
    assert result['failed'] == 2

def test_validate_time_sequence(validator, sample_data):
    result = validator._validate_time_sequence(sample_data, {'rule_id': 'IMP_004', 'fields': ['Time of Call', 'Time Dispatched', 'Time Out', 'Time In'], 'severity': 'important'})
    assert result['passed'] == 4
    assert result['failed'] == 3

def test_validate_datetime_duration(validator, sample_data):
    result = validator._validate_datetime_duration(sample_data, {'rule_id': 'IMP_005', 'fields': ['Time of Call', 'Time Dispatched', 'Time Out', 'Time In'], 'severity': 'important'})
    assert result['passed'] == 4
    assert result['failed'] == 3

def test_validate_time_spent(validator, sample_data):
    result = validator._validate_time_spent(sample_data, {'rule_id': 'IMP_006', 'field': 'Time Spent', 'severity': 'important'})
    assert result['passed'] == 4
    assert result['failed'] == 3

def test_validate_how_reported(validator, sample_data):
    result = validator._validate_how_reported(sample_data, {'rule_id': 'OPT_001', 'field': 'How Reported', 'severity': 'optional'})
    assert result['passed'] == 4
    assert result['failed'] == 3

def test_validate_zone_validity(validator, sample_data):
    result = validator._validate_zone_validity(sample_data, {'rule_id': 'OPT_002', 'field': 'PDZone', 'severity': 'optional'})
    assert result['passed'] == 4
    assert result['failed'] == 3

def test_validate_response_type_consistency(validator, sample_data):
    result = validator._validate_response_type_consistency(sample_data, {'rule_id': 'OPT_003', 'fields': ['Response Type', 'Incident'], 'severity': 'optional'})
    assert result['passed'] == 6
    assert result['failed'] == 1

def test_clean_data_incident_mapping(validator, sample_data):
    cleaned = validator.clean_data(sample_data)
    assert cleaned['Incident'].iloc[0] == 'Noise Complaint'
    assert cleaned['Response Type'].iloc[0] == 'Routine'

def test_fix_mojibake_replaces_encoded_dash():
    text = "MOTOR VEHICLE CRASH â€“ PEDESTRIAN STRUCK"
    fixed = fix_mojibake(text)
    assert 'â€“' not in fixed
    assert '–' in fixed

def test_how_reported_normalization_variants():
    variants = ['911', '9/1/1', '9 1 1', 'EMERGENCY 911', '2001-09-01']
    normalized = {normalize_how_reported_value(v) for v in variants}
    assert normalized == {'9-1-1'}

def test_apply_incident_mapping_backfills(validator):
    mapping_df = pd.DataFrame({
        'Incident': ['MOTOR VEHICLE CRASH – PEDESTRIAN STRUCK'],
        'Incident_Norm': ['TRAFFIC CRASH'],
        'Response_Type': ['EMERGENCY RESPONSE']
    })
    mapping_df['Incident'] = mapping_df['Incident'].apply(fix_mojibake)
    mapping_df['Incident_Norm'] = mapping_df['Incident_Norm'].apply(fix_mojibake)
    mapping_df['Response_Type'] = mapping_df['Response_Type'].apply(fix_mojibake)
    mapping_df['Incident_key'] = mapping_df['Incident'].apply(normalize_incident_key)
    validator.incident_mapping = mapping_df

    df = pd.DataFrame({
        'Incident': ['MOTOR VEHICLE CRASH â€“ PEDESTRIAN STRUCK', 'MOTOR VEHICLE CRASH â€“ PEDESTRIAN STRUCK'],
        'Response Type': ['', 'ROUTINE'],
        'How Reported': ['911', 'PHONE']
    })

    result = validator._apply_incident_mapping(df)
    assert result.loc[0, 'Incident'] == 'TRAFFIC CRASH'
    assert result.loc[0, 'Response Type'] == 'EMERGENCY RESPONSE'
    assert result.loc[1, 'Response Type'] == 'ROUTINE'

def test_validate_empty_dataframe(validator):
    empty_df = pd.DataFrame()
    cleaned = validator.clean_data(empty_df)
    assert cleaned.empty
    results = validator.validate_cad_dataset(empty_df)
    assert results['total_records'] == 0
    assert results['data_quality_score'] == 0.0

def test_validate_missing_columns(validator):
    df_missing = pd.DataFrame({'InvalidColumn': [1, 2]})
    results = validator.validate_cad_dataset(df_missing)
    for category, rules in results.items():
        if isinstance(rules, dict):
            for rule in rules.values():
                if isinstance(rule, dict):
                    assert 'error' in rule or rule.get('passed', 0) == 0

def test_config_reload(tmp_path):
    cfg = {
        "address_abbreviations": {" ST ": " STREET "},
        "validation_lists": {
            "valid_dispositions": ["COMPLETE", "ADVISED"],
            "valid_zones": ["5", "6", "7", "8", "9"],
            "emergency_incidents": ["ROBBERY"],
            "non_emergency_incidents": ["NOISE COMPLAINT"],
            "how_reported": ["9-1-1", "PHONE"]
        }
    }
    config_path = tmp_path / "config_enhanced.json"
    config_path.write_text(json.dumps(cfg))
    validator = CADDataValidator(config_path=str(config_path))
    cfg["validation_lists"]["valid_dispositions"].append("REFERRED")
    config_path.write_text(json.dumps(cfg))
    validator.reload_config(str(config_path))
    assert "REFERRED" in validator.config["validation_lists"]["valid_dispositions"]


def main():
    pd.options.mode.copy_on_write = True
    parser = argparse.ArgumentParser(description="Clean and validate CAD datasets.") parser.add_argument(
        "--config",
        default=os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'config_enhanced.json'),
        help="Path to configuration JSON file." )
    parser.add_argument(
        "--raw-dir",
        default=None,
        help="Directory containing CAD source files (overrides config path)." )
    parser.add_argument(
        "--sampling-method",
        choices=['stratified', 'systematic', 'random'],
        default='stratified',
        help="Sampling method to apply." )
    parser.add_argument(
        "--reload-config",
        action="store_true",
        help="Reload configuration before processing." )
    args = parser.parse_args()

    validator = CADDataValidator(config_path=args.config)
    if args.reload_config:
        validator.reload_config(args.config)

    # Use --raw-dir argument if provided, otherwise use config path
    raw_dir = args.raw_dir
    if raw_dir is None and 'paths' in validator.config and 'raw_data_dir' in validator.config['paths']:
        raw_dir = validator.config['paths']['raw_data_dir']

    if raw_dir is None:
        raise ValueError("No raw data directory specified. Use --raw-dir or set paths.raw_data_dir in config.") all_cad_files = sorted(glob.glob(os.path.join(raw_dir, "*.xlsx")))
    print(f"Found {len(all_cad_files)} files to process.") for cad_file_path in all_cad_files:
        print(f"\n{'='*60}\nPROCESSING: {os.path.basename(cad_file_path)}\n{'='*60}")
        validator.current_dataset_label = os.path.basename(cad_file_path)
        validator.validation_results['source_dataset'] = validator.current_dataset_label
        results, report_file = validate_cad_dataset_with_sampling(
            {
                'file_path': cad_file_path,
                'config_path': args.config
            },
            args.sampling_method,
            validator=validator
        )
        print(f"  - Overall Quality Score: {results.get('overall_quality_score', 0):.1f}/100")
        print(f"  - Full report saved to: {report_file}")

    print("\nAll files processed successfully!") if __name__ == "__main__":
    main()
```

```python
import argparse
import pandas as pd
import numpy as np
import logging
import re
import json
import glob
import os
from datetime import datetime
from collections import Counter
from pathlib import Path
from sklearn.model_selection import train_test_split
from typing import Dict, Any
import dask.dataframe as dd
from dask import delayed
import dask.diagnostics as diagnostics
import psutil
from pydantic import BaseModel, ValidationError
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pytest

# --- Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# --- Path expansion helpers ---
def expand_config_paths(config: dict) -> dict:
    """Expand path templates in config using {key} substitution. Args:
        config (dict): Configuration dictionary with paths section. Returns:
        dict: Config with expanded paths.
    """