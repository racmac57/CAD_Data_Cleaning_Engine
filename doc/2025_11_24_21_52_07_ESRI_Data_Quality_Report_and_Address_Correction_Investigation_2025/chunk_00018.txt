if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file)
    corrections = corrections[corrections['Corrected_Value'].notna() & (corrections['Corrected_Value'] != '')]
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Merge corrections
    df = df.merge(
        corrections[['ReportNumberNew', 'Corrected_Value']].rename(columns={'Corrected_Value': 'How Reported'}),
        on='ReportNumberNew',
        how='left',
        suffixes=('', '_new')
    )
    
    # Apply corrections where new value exists
    mask = df['How Reported_new'].notna()
    count = mask.sum()
    df.loc[mask, 'How Reported'] = df.loc[mask, 'How Reported_new']
    df = df.drop(columns=['How Reported_new'])
    
    return df, count

def apply_disposition_corrections(df, corrections_file):
    """Apply Disposition corrections.""" if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file)
    corrections = corrections[corrections['Corrected_Value'].notna() & (corrections['Corrected_Value'] != '')]
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Merge corrections
    df = df.merge(
        corrections[['ReportNumberNew', 'Corrected_Value']].rename(columns={'Corrected_Value': 'Disposition'}),
        on='ReportNumberNew',
        how='left',
        suffixes=('', '_new')
    )
    
    # Apply corrections where new value exists
    mask = df['Disposition_new'].notna()
    count = mask.sum()
    df.loc[mask, 'Disposition'] = df.loc[mask, 'Disposition_new']
    df = df.drop(columns=['Disposition_new'])
    
    return df, count

def apply_case_number_corrections(df, corrections_file):
    """Apply ReportNumberNew corrections.""" if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file)
    corrections = corrections[corrections['Corrected_Value'].notna() & (corrections['Corrected_Value'] != '')]
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Create mapping from old to new case numbers
    case_map = dict(zip(corrections['ReportNumberNew'], corrections['Corrected_Value']))
    
    # Apply corrections
    mask = df['ReportNumberNew'].isin(case_map.keys())
    count = mask.sum()
    df.loc[mask, 'ReportNumberNew'] = df.loc[mask, 'ReportNumberNew'].map(case_map)
    
    return df, count

def apply_address_corrections(df, corrections_file):
    """Apply FullAddress2 corrections.""" if not corrections_file.exists():
        print(f"  Skipping: {corrections_file.name} (file not found)")
        return df, 0
    
    corrections = pd.read_csv(corrections_file)
    corrections = corrections[corrections['Corrected_Value'].notna() & (corrections['Corrected_Value'] != '')]
    
    if len(corrections) == 0:
        print(f"  No corrections to apply in {corrections_file.name}")
        return df, 0
    
    # Merge corrections
    df = df.merge(
        corrections[['ReportNumberNew', 'Corrected_Value']].rename(columns={'Corrected_Value': 'FullAddress2'}),
        on='ReportNumberNew',
        how='left',
        suffixes=('', '_new')
    )
    
    # Apply corrections where new value exists
    mask = df['FullAddress2_new'].notna()
    count = mask.sum()
    df.loc[mask, 'FullAddress2'] = df.loc[mask, 'FullAddress2_new']
    df = df.drop(columns=['FullAddress2_new'])
    
    return df, count

def fix_hour_field(df):
    """Fix Hour field format from HH:mm to HH:00.""" print("\n  Fixing Hour field format...")
    
    # Convert Hour to datetime if it's in HH:mm format, then round to hour
    def round_to_hour(hour_str):
        if pd.isna(hour_str):
            return hour_str
        hour_str = str(hour_str).strip()
        if ':' in hour_str:
            try:
                # Parse HH:mm format
                parts = hour_str.split(':')
                hour = int(parts[0])
                return f"{hour:02d}:00"
            except:
                return hour_str
        return hour_str
    
    mask = df['Hour'].notna()
    count = mask.sum()
    df.loc[mask, 'Hour'] = df.loc[mask, 'Hour'].apply(round_to_hour)
    
    return df, count

def main():
    print("="*60)
    print("APPLYING MANUAL CORRECTIONS")
    print("="*60)
    print(f"\nInput file: {ESRI_FILE}")
    print(f"Corrections directory: {CORRECTIONS_DIR}\n")
    
    # Load main file
    print("Loading ESRI export file...")
    df = pd.read_excel(ESRI_FILE)
    print(f"  Loaded {len(df):,} records\n")
    
    total_corrections = 0
    
    # Apply corrections
    print("Applying corrections...")
    
    # 1. How Reported
    print("\n1. How Reported corrections:")
    df, count = apply_how_reported_corrections(df, CORRECTIONS_DIR / "how_reported_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 2. Disposition
    print("\n2. Disposition corrections:")
    df, count = apply_disposition_corrections(df, CORRECTIONS_DIR / "disposition_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 3. Case Numbers
    print("\n3. Case Number corrections:")
    df, count = apply_case_number_corrections(df, CORRECTIONS_DIR / "case_number_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 4. Addresses
    print("\n4. Address corrections:")
    df, count = apply_address_corrections(df, CORRECTIONS_DIR / "address_corrections.csv")
    print(f"  Applied {count:,} corrections")
    total_corrections += count
    
    # 5. Hour Field (automated fix)
    print("\n5. Hour field format fix:")
    df, count = fix_hour_field(df)
    print(f"  Fixed {count:,} records")
    total_corrections += count
    
    # Save corrected file
    timestamp = datetime.now().strftime('%Y%m%d')
    output_file = OUTPUT_DIR / f"CAD_ESRI_Final_{timestamp}_corrected.xlsx"
    
    print(f"\nSaving corrected file...")
    df.to_excel(output_file, index=False)
    print(f"  Saved to: {output_file}")
    
    print("\n" + "="*60)
    print("CORRECTIONS COMPLETE")
    print("="*60)
    print(f"Total corrections applied: {total_corrections:,}")
    print(f"Output file: {output_file}")
    print("\nNext step: Re-run validation to verify corrections")

if __name__ == "__main__":
    main()


```

```python
#!/usr/bin/env python3
"""
Create manual correction CSV files for data quality issues.
""" import pandas as pd
from pathlib import Path

BASE_DIR = Path(r"C:\Users\carucci_r\OneDrive - City of Hackensack\02_ETL_Scripts\CAD_Data_Cleaning_Engine")
ESRI_FILE = BASE_DIR / "data" / "ESRI_CADExport" / "CAD_ESRI_Final_20251117_v2.xlsx"
OUTPUT_DIR = BASE_DIR / "manual_corrections"

# Valid How Reported values
VALID_HOW_REPORTED = [
    '9-1-1', 'Walk-In', 'Phone', 'Self-Initiated', 'Radio', 
    'Teletype', 'Fax', 'Other - See Notes', 'eMail', 'Mail', 
    'Virtual Patrol', 'Canceled Call'
]

OUTPUT_DIR.mkdir(exist_ok=True)

print("Loading ESRI export file...")
df = pd.read_excel(ESRI_FILE)
print(f"Loaded {len(df):,} records")

# 1. How Reported Corrections
print("\nCreating How Reported corrections file...")
invalid_how = df[
    ~df['How Reported'].isin(VALID_HOW_REPORTED) & 
    df['How Reported'].notna()
].copy()
null_how = df[df['How Reported'].isna()].copy()

how_corrections = pd.DataFrame()
if len(invalid_how) > 0:
    how_corrections = invalid_how[['ReportNumberNew', 'TimeOfCall', 'How Reported', 'Incident']].copy()
    how_corrections['Corrected_Value'] = ''
    how_corrections['Notes'] = ''
    how_corrections['Issue_Type'] = 'Invalid Value'

if len(null_how) > 0:
    null_corrections = null_how[['ReportNumberNew', 'TimeOfCall', 'How Reported', 'Incident']].copy()
    null_corrections['Corrected_Value'] = ''
    null_corrections['Notes'] = ''
    null_corrections['Issue_Type'] = 'Null Value'
    
    if len(how_corrections) > 0:
        how_corrections = pd.concat([how_corrections, null_corrections], ignore_index=True)
    else:
        how_corrections = null_corrections

# Limit to first 2000 records for manual editing
how_corrections = how_corrections.head(2000)
how_corrections.to_csv(OUTPUT_DIR / "how_reported_corrections.csv", index=False)
print(f"  Created: {len(how_corrections):,} records (total issues: {len(invalid_how) + len(null_how):,})")

# 2. Disposition Corrections
print("\nCreating Disposition corrections file...")
null_disp = df[df['Disposition'].isna()].copy()
disp_corrections = null_disp[['ReportNumberNew', 'TimeOfCall', 'Incident', 'How Reported', 'Disposition']].copy()
disp_corrections['Corrected_Value'] = ''
disp_corrections['Notes'] = ''
# Limit to first 2000 records
disp_corrections = disp_corrections.head(2000)
disp_corrections.to_csv(OUTPUT_DIR / "disposition_corrections.csv", index=False)
print(f"  Created: {len(disp_corrections):,} sample records (total nulls: {len(null_disp):,})")

# 3. Case Number Corrections (from validation report)
print("\nCreating Case Number corrections file...")
case_corrections = pd.read_csv(BASE_DIR / "data" / "02_reports" / "invalid_case_numbers_20251124_000244.csv")
case_corrections['Corrected_Value'] = ''
case_corrections['Notes'] = ''
case_corrections.to_csv(OUTPUT_DIR / "case_number_corrections.csv", index=False)
print(f"  Created: {len(case_corrections):,} records")

# 4. Address Corrections (sample)
print("\nCreating Address corrections file (sample)...")
# Get addresses with issues (no street type, no number, generic text)
address_issues = df[
    (df['FullAddress2'].notna()) & 
    (
        (~df['FullAddress2'].str.contains(r'\b(STREET|AVENUE|ROAD|DRIVE|COURT|LANE|PLACE|BOULEVARD|WAY|CIRCLE)\b', case=False, na=False)) |
        (df['FullAddress2'].str.contains(r'^(home|unknown|various|rear lot|parking garage)', case=False, na=False))
    )
].head(1000).copy()

addr_corrections = address_issues[['ReportNumberNew', 'TimeOfCall', 'FullAddress2', 'Incident']].copy()
addr_corrections['Corrected_Value'] = ''
addr_corrections['Issue_Type'] = ''
addr_corrections['Notes'] = ''
addr_corrections.to_csv(OUTPUT_DIR / "address_corrections.csv", index=False)
print(f"  Created: {len(addr_corrections):,} sample records")

# 5. Hour Field Corrections (sample showing the issue)
print("\nCreating Hour field corrections reference file...")
hour_issues = df[df['Hour'].notna()].copy()
hour_issues = hour_issues[hour_issues['Hour'].astype(str).str.contains(':', na=False)].head(100).copy()
hour_corrections = hour_issues[['ReportNumberNew', 'TimeOfCall', 'Hour', 'cYear', 'cMonth']].copy()
hour_corrections['Corrected_Value'] = ''
hour_corrections['Notes'] = 'Round to HH:00 format (e.g., 00:04 -> 00:00, 14:35 -> 14:00)'
hour_corrections.to_csv(OUTPUT_DIR / "hour_field_corrections.csv", index=False)
print(f"  Created: {len(hour_corrections):,} sample records showing format issue")

# 6. Master Corrections Template
print("\nCreating Master Corrections Template...")
master_template = pd.DataFrame(columns=[
    'ReportNumberNew', 'TimeOfCall', 'Field_Name', 'Current_Value', 
    'Corrected_Value', 'Issue_Type', 'Notes', 'Status'
])
master_template.to_csv(OUTPUT_DIR / "master_corrections_template.csv", index=False)
print("  Created: Master template for any additional corrections")

print("\n" + "="*60)
print("All correction files created in: manual_corrections/")
print("="*60)
print("\nFiles created:")
print("  1. how_reported_corrections.csv")
print("  2. disposition_corrections.csv")
print("  3. case_number_corrections.csv")
print("  4. address_corrections.csv")
print("  5. hour_field_corrections.csv")
print("  6. master_corrections_template.csv")
print("\nNext steps:")
print("  1. Open each CSV file in Excel")
print("  2. Fill in the 'Corrected_Value' column")
print("  3. Add notes if needed")
print("  4. Save the files")
print("  5. Run apply_corrections.py to apply changes")


```

```python
"""
ESRI Final Export Quality Validation Script

Validates the CAD ESRI final export file against the standards defined in
CAD_Data_Cleaning_Engine.md. This script performs comprehensive validation across 12 categories:
1. Schema Validation
2. TimeOfCall Validation
3. Response_Type Validation
4. How Reported Validation
5. FullAddress2 Validation
6. ReportNumberNew Validation
7. Officer Validation
8. Disposition Validation
9. Grid and PDZone Validation
10. Incident Validation
11. Latitude/Longitude Validation
12. Data Quality Flags

Output: Markdown validation report and CSV files for invalid records
"""

import pandas as pd
import numpy as np
import re
from datetime import datetime
from pathlib import Path
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ESRIFinalValidator:
    """Comprehensive ESRI final export validator.""" def __init__(self, input_file: str, output_dir: str = "data/02_reports"):
        """Initialize validator with file paths.""" self.input_file = input_file
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Define expected schema
        self.required_columns = [
            'TimeOfCall', 'cYear', 'cMonth', 'Hour', 'DayofWeek',
            'Incident', 'Response_Type', 'How Reported', 'FullAddress2',
            'Grid', 'PDZone', 'Officer', 'Disposition', 'ReportNumberNew',
            'Latitude', 'Longitude', 'data_quality_flag'
        ]

        # Define valid How Reported values
        self.valid_how_reported = [
            '9-1-1', 'Walk-In', 'Phone', 'Self-Initiated', 'Radio',
            'Teletype', 'Fax', 'Other - See Notes', 'eMail', 'Mail',
            'Virtual Patrol', 'Canceled Call'
        ]

        # Define valid Response_Type values
        self.valid_response_types = ['Emergency', 'Urgent', 'Routine', 'Administrative']

        # Define Beat to PDZone mapping
        self.beat_to_pdzone = {
            'E1': '5', 'E2': '5', 'E3': '5',
            'F1': '6', 'F2': '6', 'F3': '6',
            'G1': '7', 'G2': '7', 'G3': '7', 'G4': '7', 'G5': '7',
            'H1': '8', 'H2': '8', 'H3': '8', 'H4': '8', 'H5': '8',
            'I1': '9', 'I2': '9', 'I3': '9', 'I4': '9A', 'I5': '9', 'I6': '9'
        }

        # Validation results
        self.validation_results = {}
        self.invalid_records = {}

    def load_data(self) -> pd.DataFrame:
        """Load ESRI export file.""" logger.info(f"Loading data from {self.input_file}...")
        df = pd.read_excel(self.input_file)
        logger.info(f"Loaded {len(df):,} records")
        return df

    def validate_schema(self, df: pd.DataFrame) -> dict:
        """Validate schema and column presence.""" logger.info("Validating schema...")

        actual_columns = list(df.columns)
        missing_columns = [col for col in self.required_columns if col not in actual_columns]
        unexpected_columns = [col for col in actual_columns if col not in self.required_columns]

        result = {
            'status': 'PASS' if not missing_columns else 'FAIL',
            'total_columns': len(actual_columns),
            'required_columns': len(self.required_columns),
            'missing_columns': missing_columns,
            'unexpected_columns': unexpected_columns,
            'column_list': actual_columns
        }

        return result

    def validate_timeofcall(self, df: pd.DataFrame) -> dict:
        """Validate TimeOfCall field and derived fields.""" logger.info("Validating TimeOfCall and derived fields...")

        # Check for null values
        null_count = df['TimeOfCall'].isna().sum()

        # Validate format
        valid_format_count = 0
        if 'TimeOfCall' in df.columns:
            valid_format_count = df['TimeOfCall'].notna().sum()

        # Validate derived fields
        cyear_issues = 0
        cmonth_issues = 0
        hour_issues = 0
        dayofweek_issues = 0

        # Check cYear
        if 'cYear' in df.columns:
            df_check = df[df['TimeOfCall'].notna()].copy()
            df_check['expected_year'] = df_check['TimeOfCall'].dt.year.astype(str)
            cyear_issues = (df_check['cYear'] != df_check['expected_year']).sum()

        # Check cMonth
        if 'cMonth' in df.columns:
            df_check = df[df['TimeOfCall'].notna()].copy()
            df_check['expected_month'] = df_check['TimeOfCall'].dt.strftime('%B')
            cmonth_issues = (df_check['cMonth'] != df_check['expected_month']).sum()

        # Check Hour
        if 'Hour' in df.columns:
            df_check = df[df['TimeOfCall'].notna()].copy()
            df_check['expected_hour'] = df_check['TimeOfCall'].dt.strftime('%H:00')
            hour_issues = (df_check['Hour'] != df_check['expected_hour']).sum()

        # Check DayofWeek
        if 'DayofWeek' in df.columns:
            df_check = df[df['TimeOfCall'].notna()].copy()
            df_check['expected_dayofweek'] = df_check['TimeOfCall'].dt.strftime('%A')
            dayofweek_issues = (df_check['DayofWeek'] != df_check['expected_dayofweek']).sum()

        result = {
            'status': 'PASS' if null_count == 0 and cyear_issues == 0 and cmonth_issues == 0 and hour_issues == 0 and dayofweek_issues == 0 else 'FAIL',
            'null_count': int(null_count),
            'valid_format_count': int(valid_format_count),
            'total_records': len(df),
            'cyear_issues': int(cyear_issues),
            'cmonth_issues': int(cmonth_issues),
            'hour_issues': int(hour_issues),
            'dayofweek_issues': int(dayofweek_issues)
        }

        return result

    def validate_response_type(self, df: pd.DataFrame) -> dict:
        """Validate Response_Type field.""" logger.info("Validating Response_Type...")

        total = len(df)
        null_count = df['Response_Type'].isna().sum()

        # Check for valid values
        valid_mask = df['Response_Type'].isin(self.valid_response_types)
        valid_count = valid_mask.sum()
        invalid_count = (~valid_mask & df['Response_Type'].notna()).sum()

        coverage = ((total - null_count) / total * 100) if total > 0 else 0

        # Get distribution
        distribution = df['Response_Type'].value_counts(dropna=False).to_dict()

        result = {
            'status': 'PASS' if coverage >= 99 and invalid_count == 0 else 'FAIL',
            'total_records': total,
            'null_count': int(null_count),
            'valid_count': int(valid_count),
            'invalid_count': int(invalid_count),
            'coverage_percent': round(coverage, 2),
            'distribution': {str(k): int(v) for k, v in distribution.items()}
        }

        return result

    def validate_how_reported(self, df: pd.DataFrame) -> dict:
        """Validate How Reported field.""" logger.info("Validating How Reported...")

        total = len(df)
        null_count = df['How Reported'].isna().sum()

        # Check for valid values
        valid_mask = df['How Reported'].isin(self.valid_how_reported)
        valid_count = valid_mask.sum()
        invalid_count = (~valid_mask & df['How Reported'].notna()).sum()

        # Get invalid values
        invalid_values = df.loc[~valid_mask & df['How Reported'].notna(), 'How Reported'].value_counts().to_dict()

        result = {
            'status': 'PASS' if null_count == 0 and invalid_count == 0 else 'FAIL',
            'total_records': total,
            'null_count': int(null_count),
            'valid_count': int(valid_count),
            'invalid_count': int(invalid_count),
            'invalid_values': {str(k): int(v) for k, v in list(invalid_values.items())[:10]}
        }

        return result

    def validate_fulladdress2(self, df: pd.DataFrame) -> dict:
        """Validate FullAddress2 field.""" logger.info("Validating FullAddress2...")

        total = len(df)
        null_count = df['FullAddress2'].isna().sum()

        # Define patterns for valid addresses
        # Standard address: starts with number, has street type, city, state, zip
        standard_pattern = r'^\d+\s+[A-Z\s]+\s+(STREET|AVENUE|ROAD|DRIVE|COURT|LANE|BOULEVARD|PLACE|PARKWAY|WAY|TERRACE|CIRCLE)\s*,?\s*[A-Z\s]+,\s*[A-Z]{2}\s+\d{5}'

        # Intersection: two streets with & separator
        intersection_pattern = r'^[A-Z\s]+\s+(STREET|AVENUE|ROAD|DRIVE|COURT|LANE|BOULEVARD|PLACE|PARKWAY|WAY|TERRACE|CIRCLE)\s*&\s*[A-Z\s]+\s+(STREET|AVENUE|ROAD|DRIVE|COURT|LANE|BOULEVARD|PLACE|PARKWAY|WAY|TERRACE|CIRCLE)\s*,?\s*[A-Z\s]+,\s*[A-Z]{2}\s+\d{5}'

        # Check valid addresses
        df_check = df[df['FullAddress2'].notna()].copy()
        df_check['addr_upper'] = df_check['FullAddress2'].astype(str).str.upper().str.strip()

        standard_valid = df_check['addr_upper'].str.contains(standard_pattern, regex=True, na=False)
        intersection_valid = df_check['addr_upper'].str.contains(intersection_pattern, regex=True, na=False)

        valid_mask = standard_valid | intersection_valid
        valid_count = valid_mask.sum()
        invalid_count = (~valid_mask).sum()

        # Identify invalid patterns
        invalid_patterns = {
            'no_street_number': (~df_check['addr_upper'].str.match(r'^\d+', na=False) & ~df_check['addr_upper'].str.contains('&', na=False)).sum(),
            'no_street_type': (~df_check['addr_upper'].str.contains(r'(STREET|AVENUE|ROAD|DRIVE|COURT|LANE|BOULEVARD|PLACE|PARKWAY|WAY|TERRACE|CIRCLE)', na=False)).sum(),
            'po_box': df_check['addr_upper'].str.contains(r'P\.?O\. ?\s*BOX', regex=True, na=False).sum(),
            'generic_text': df_check['addr_upper'].str.contains(r'(VARIOUS|UNKNOWN|REAR LOT|PARKING GARAGE|^HOME$)', regex=True, na=False).sum()
        }

        # Store invalid addresses for CSV output
        if invalid_count > 0:
            invalid_df = df_check[~valid_mask].copy()
            invalid_df = invalid_df[['ReportNumberNew', 'TimeOfCall', 'FullAddress2', 'Incident']].head(1000)
            self.invalid_records['invalid_addresses'] = invalid_df

        result = {
            'status': 'PASS' if (valid_count / (total - null_count) * 100) >= 90 else 'WARNING',
            'total_records': total,
            'null_count': int(null_count),
            'valid_count': int(valid_count),
            'invalid_count': int(invalid_count),
            'valid_percent': round((valid_count / (total - null_count) * 100) if (total - null_count) > 0 else 0, 2),
            'invalid_patterns': {k: int(v) for k, v in invalid_patterns.items()}
        }

        return result

    def validate_reportnumbernew(self, df: pd.DataFrame) -> dict:
        """Validate ReportNumberNew field.""" logger.info("Validating ReportNumberNew...")

        total = len(df)
        null_count = df['ReportNumberNew'].isna().sum()

        # Define valid pattern: YY-XXXXXX or YY-XXXXXXA
        valid_pattern = r'^\d{2}-\d{6}[A-Z]?$'

        df_check = df[df['ReportNumberNew'].notna()].copy()
        valid_mask = df_check['ReportNumberNew'].astype(str).str.match(valid_pattern, na=False)
        valid_count = valid_mask.sum()
        invalid_count = (~valid_mask).sum()

        # Check for duplicates
        duplicates = df_check[df_check['ReportNumberNew'].duplicated(keep=False)]
        duplicate_count = len(duplicates)

        # Store invalid case numbers for CSV output
        if invalid_count > 0:
            invalid_df = df_check[~valid_mask].copy()
            invalid_df = invalid_df[['ReportNumberNew', 'TimeOfCall', 'Incident']].head(1000)
            self.invalid_records['invalid_case_numbers'] = invalid_df

        result = {
            'status': 'PASS' if invalid_count == 0 and duplicate_count == 0 else 'FAIL',
            'total_records': total,
            'null_count': int(null_count),
            'valid_count': int(valid_count),
            'invalid_count': int(invalid_count),
            'duplicate_count': duplicate_count,
            'valid_percent': round((valid_count / (total - null_count) * 100) if (total - null_count) > 0 else 0, 2)
        }

        return result

    def validate_officer(self, df: pd.DataFrame) -> dict:
        """Validate Officer field.""" logger.info("Validating Officer...")

        total = len(df)
        null_count = df['Officer'].isna().sum()

        # Define valid pattern: RANK FirstName LastName BadgeNumber
        valid_pattern = r'^([A-Z]{2,4}\.)\s+([A-Z][a-z]+(?:-[A-Z][a-z]+)?)\s+([A-Za-z]+(?:-[A-Za-z]+)*(?:\s[A-Za-z]+(? :-[A-Za-z]+)*)*)\s+(\d{2,4})$'

        df_check = df[df['Officer'].notna()].copy()
        valid_mask = df_check['Officer'].astype(str).str.match(valid_pattern, na=False)
        valid_count = valid_mask.sum()
        invalid_count = (~valid_mask).sum()

        # Store invalid officers for CSV output
        if invalid_count > 0:
            invalid_df = df_check[~valid_mask].copy()
            invalid_df = invalid_df[['ReportNumberNew', 'TimeOfCall', 'Officer', 'Incident']].head(1000)
            self.invalid_records['invalid_officers'] = invalid_df

        result = {
            'status': 'WARNING' if invalid_count > 0 else 'PASS',
            'total_records': total,
            'null_count': int(null_count),
            'valid_count': int(valid_count),
            'invalid_count': int(invalid_count),
            'valid_percent': round((valid_count / (total - null_count) * 100) if (total - null_count) > 0 else 0, 2)
        }

        return result

    def validate_disposition(self, df: pd.DataFrame) -> dict:
        """Validate Disposition field.""" logger.info("Validating Disposition...")

        total = len(df)
        null_count = df['Disposition'].isna().sum()

        # Get distribution
        distribution = df['Disposition'].value_counts(dropna=False).head(20).to_dict()

        result = {
            'status': 'PASS' if null_count == 0 else 'FAIL',
            'total_records': total,
            'null_count': int(null_count),
            'unique_values': int(df['Disposition'].nunique(dropna=False)),
            'distribution': {str(k): int(v) for k, v in distribution.items()}
        }

        return result

    def validate_grid_pdzone(self, df: pd.DataFrame) -> dict:
        """Validate Grid and PDZone fields.""" logger.info("Validating Grid and PDZone...")

        total = len(df)
        grid_null = df['Grid'].isna().sum()
        pdzone_null = df['PDZone'].isna().sum()

        # Check valid Grid values (Beat values)
        valid_grids = list(self.beat_to_pdzone.keys())
        grid_valid = df['Grid'].isin(valid_grids).sum()
        grid_invalid = (~df['Grid'].isin(valid_grids) & df['Grid'].notna()).sum()

        # Check valid PDZone values
        valid_pdzones = ['5', '6', '7', '8', '9', '9A']
        pdzone_valid = df['PDZone'].isin(valid_pdzones).sum()
        pdzone_invalid = (~df['PDZone'].isin(valid_pdzones) & df['PDZone'].notna()).sum()

        # Check Grid to PDZone mapping
        mapping_issues = 0
        df_check = df[(df['Grid'].notna()) & (df['PDZone'].notna())].copy()
        for idx, row in df_check.iterrows():
            expected_pdzone = self.beat_to_pdzone.get(str(row['Grid']), None)
            if expected_pdzone and str(row['PDZone']) != expected_pdzone:
                mapping_issues += 1

        result = {
            'status': 'WARNING' if grid_invalid > 0 or pdzone_invalid > 0 or mapping_issues > 0 else 'PASS',
            'total_records': total,
            'grid_null_count': int(grid_null),
            'pdzone_null_count': int(pdzone_null),
            'grid_valid_count': int(grid_valid),
            'grid_invalid_count': int(grid_invalid),
            'pdzone_valid_count': int(pdzone_valid),
            'pdzone_invalid_count': int(pdzone_invalid),
            'mapping_issues': mapping_issues
        }

        return result

    def validate_incident(self, df: pd.DataFrame) -> dict:
        """Validate Incident field.""" logger.info("Validating Incident...")

        total = len(df)
        null_count = df['Incident'].isna().sum()

        # Check for mojibake characters
        df_check = df[df['Incident'].notna()].copy()
        mojibake_pattern = r'[^\x00-\x7F]+'
        mojibake_count = df_check['Incident'].astype(str).str.contains(mojibake_pattern, regex=True, na=False).sum()

        # Get top incidents
        top_incidents = df['Incident'].value_counts(dropna=False).head(20).to_dict()

        # Check for case sensitivity duplicates
        incident_lower = df_check['Incident'].astype(str).str.lower()
        case_duplicates = len(df_check['Incident'].unique()) - len(incident_lower.unique())

        result = {
            'status': 'WARNING' if null_count > (total * 0.01) else 'PASS',
            'total_records': total,
            'null_count': int(null_count),
            'null_percent': round((null_count / total * 100) if total > 0 else 0, 2),
            'unique_values': int(df['Incident'].nunique(dropna=True)),
            'mojibake_count': int(mojibake_count),
            'case_duplicates': case_duplicates,
            'top_incidents': {str(k): int(v) for k, v in list(top_incidents.items())[:10]}
        }

        return result

    def validate_coordinates(self, df: pd.DataFrame) -> dict:
        """Validate Latitude and Longitude fields."""