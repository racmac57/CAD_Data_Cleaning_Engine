635 -      ap.add_argument("--engine", default="auto", choices=["auto", "calamine", "openpyxl"],
       635 +      ap.add_argument("--engine", default="auto", choices=["auto", "calamine", "openpyxl"],
       636                        help="Engine for reading Excel files (auto selects based on file size)")
       637 -      ap.add_argument("--chunksize", type=int, default=None,
       637 +      ap.add_argument("--chunksize", type=int, default=None,
       638                        help="Read file in chunks of this size for large files (auto-enabled for 50MB+
             files)")
       639 -      ap.add_argument("--benchmark", action="store_true",
       639 +      ap.add_argument("--benchmark", action="store_true",
       640                        help="Enable performance benchmarking and include results in manifest")
       641 -      ap.add_argument("--parallel", action="store_true",
       641 +      ap.add_argument("--parallel", action="store_true",
       642                        help="Enable parallel sheet processing for better performance")
       643 -      ap.add_argument("--max-parallel-sheets", type=int, default=4,
       643 +      ap.add_argument("--max-parallel-sheets", type=int, default=4,
       644                        help="Maximum number of sheets to process in parallel (default: 4)")
       645 +      ap.add_argument("--max-parallel-files", type=int, default=2,
       646 +                      help="Maximum number of files to process in parallel (default: 2)")
       647 +      ap.add_argument("--buffer-size", type=int, default=65536,
       648 +                      help="Buffer size for CSV writing in bytes (default: 64KB)")
       649        args = ap.parse_args()
       650
       651        # Disable AF gate if flagged

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 20 additions and 200 removals
       860            args.min_af_nonempty = 0
       861            logger.info("A-F gate disabled - exporting all rows")
       862
       863 -      # --- NEW LOOP: Iterate over all provided file paths ---
       864 -      for xlsx_path_arg in args.xlsx_files:
       865 -          try:
       866 -              xlsx_path = os.path.abspath(xlsx_path_arg)  # Absolute path
       867 -
       868 -              # --- All original processing logic is now inside this loop ---
       869 -
       870 -              # Auto engine and chunksize
       871 -              file_size = os.path.getsize(xlsx_path)
       872 -              available_memory = psutil.virtual_memory().available / (1024 * 1024)  # MB
       873 -
       874 -              if args.engine == "auto":
       875 -                  args.engine = "calamine"  # Default to calamine
       876 -
       877 -              if args.chunksize is None:
       878 -                  args.chunksize = get_adaptive_chunksize(file_size, available_memory)
       879 -                  if file_size > 50 * 1024 * 1024:
       880 -                      logger.info(f"Large file detected ({file_size / (1024*1024):.1f}MB), enabling
           - adaptive chunking with size {args.chunksize}")
       881 -
       882 -              logger.info(f"Processing {os.path.basename(xlsx_path)} - Using engine: {args.engine},
           - chunksize: {args.chunksize}, available memory: {available_memory:.1f}MB")
       883 -              if not os.path.isfile(xlsx_path):
       884 -                  logger.error(f"File not found: {xlsx_path}")
       885 -                  raise FileNotFoundError(f"File not found: {xlsx_path}")
       886 -
       887 -              # Set output dir: subfolder in source dir with timestamp
       888 -              source_dir = os.path.dirname(xlsx_path)
       889 -              wb_stem = os.path.splitext(os.path.basename(xlsx_path))[0]
       890 -              stamp = datetime.now().strftime("%Y-%m-%d_%H%M")
       891 -              out_dir = os.path.join(source_dir, f"{wb_stem}_{stamp}")
       892 -              os.makedirs(out_dir, exist_ok=True)
       893 -
       894 -              # Load sheet names based on engine
       895 -              wb = None
       896 -              if args.engine == "openpyxl":
       897 -                  wb = load_workbook(xlsx_path, read_only=True, data_only=True)
       898 -                  sheet_names = wb.sheetnames
       899 -              else:
       900 -                  excel_file = pd.ExcelFile(xlsx_path, engine='calamine')
       901 -                  sheet_names = excel_file.sheet_names
       902 -
       903 -              used_names = set()  # For unique filenames
       904 -              exported = 0
       905 -              skipped = 0
       906 -              stats = {"rows_skipped_af_gate": 0}
       907 -              benchmark_results = []  # List for benchmarks
       908 -
       909 -              # Manifest dict for output JSON
       910 -              manifest = {
       911 -                  "workbook": xlsx_path,
       912 -                  "timestamp": datetime.now().isoformat(),
       913 -                  "output_dir": out_dir,
       914 -                  "workbook_sha256": get_file_hash(xlsx_path),
       915 -                  "engine": args.engine,
       916 -                  "chunksize": args.chunksize,
       917 -                  "benchmark_enabled": bool(args.benchmark),
       918 -                  "parallel_enabled": bool(args.parallel),
       919 -                  "max_parallel_sheets": args.max_parallel_sheets,
       920 -                  "file_size_mb": round(file_size / (1024 * 1024), 2),
       921 -                  "available_memory_mb": round(available_memory, 2),
       922 -                  "rows_skipped_af_gate": stats.get("rows_skipped_af_gate", 0),
       923 -                  "af_gate": {
       924 -                      "columns": args.af_columns,
       925 -                      "min_nonempty": args.min_af_nonempty,
       926 -                      "visible_only": bool(args.af_gate_visible_only),
       927 -                      "disabled": bool(args.disable_af_gate)
       928 -                  },
       929 -                  "sheets": [],
       930 -                  "benchmark_results": []
       863 +      # --- OPTIMIZED: Process files in parallel if enabled ---
       864 +      if len(args.xlsx_files) > 1 and args.max_parallel_files > 1:
       865 +          logger.info(f"Processing {len(args.xlsx_files)} files in parallel (max
           + {args.max_parallel_files} workers)")
       866 +          with ThreadPoolExecutor(max_workers=args.max_parallel_files) as executor:
       867 +              future_to_file = {
       868 +                  executor.submit(process_single_file, xlsx_file, args): xlsx_file
       869 +                  for xlsx_file in args.xlsx_files
       870                }
       871
       872 -              # Filter sheets
       873 -              filtered_sheets = []
       874 -              for sheet_name in sheet_names:
       875 -                  if not args.all_sheets:
       876 -                      if args.exclude and any(pat.lower() in sheet_name.lower() for pat in
           - args.exclude):
       877 -                          continue
       878 -                      if args.include and not any(pat.lower() in sheet_name.lower() for pat in
           - args.include):
       879 -                          continue
       880 -                  filtered_sheets.append(sheet_name)
       881 -
       882 -              if args.parallel and len(filtered_sheets) > 1:
       883 -                  # Parallel
       884 -                  logger.info(f"Processing {len(filtered_sheets)} sheets in parallel (max
           - {args.max_parallel_sheets} workers)")
       885 -                  with ThreadPoolExecutor(max_workers=args.max_parallel_sheets) as executor:
       886 -                      future_to_sheet = {
       887 -                          executor.submit(process_sheet_parallel, sheet_name, xlsx_path, out_dir,
           - args, args.engine, used_names, benchmark_results, None if len(filtered_sheets) != 1 else
           - os.path.splitext(os.path.basename(xlsx_path))[0]): sheet_name
       888 -                          for sheet_name in filtered_sheets
       889 -                      }
       890 -
       891 -                      for future in tqdm(as_completed(future_to_sheet), total=len(filtered_sheets),
           - desc="Processing sheets"):
       892 -                          sheet_name = future_to_sheet[future]
       893 -                          try:
       894 -                              result = future.result()
       895 -                              if result["status"] == "success":
       896 -                                  exported += 1
       897 -                                  manifest["sheets"].append(result)
       898 -                              elif result["status"] == "skipped":
       899 -                                  skipped += 1
       900 -                              else:
       901 -                                  logger.error(f"Error processing {sheet_name}: {result.get('error',
           - 'Unknown error')}")
       902 -                          except Exception as e:
       903 -                              logger.error(f"Exception processing {sheet_name}: {str(e)}")
       904 -              else:
       905 -                  # Sequential
       906 -                  for sheet_name in tqdm(filtered_sheets, desc="Exporting sheets"):
       907 -                      if args.benchmark:
       908 -                          benchmark_results = {}  # Should be list? Inconsistent with parallel
       909 -                          for eng in ['calamine', 'openpyxl']:
       910 -                              duration = benchmark_engine_performance(xlsx_path, sheet_name, eng,
           - is_detailed=True)
       911 -                              benchmark_results[eng] = duration
       912 -                              logger.info(f"Benchmark: {sheet_name} with {eng} -
           - {json.dumps(duration)}")
       913 -                          manifest["benchmark_results"].append({"sheet_name": sheet_name, "results":
           - benchmark_results})
       914 -
       915 -                      # Load data
       916 -                      if args.engine == "openpyxl":
       917 -                          ws = wb[sheet_name]
       918 -                          max_r, max_c = _get_used_range(ws)
       919 -                          is_df = False
       920 -                      else:
       921 -                          df = pd.read_excel(xlsx_path, sheet_name=sheet_name, engine='calamine',
           - keep_default_na=False)
       922 -                          setattr(df, "_include_headers", True)
       923 -                          max_r, max_c = _get_used_range(df)
       924 -                          is_df = True
       925 -
       926 -                      if not args.include_empty and (max_r == 0 or max_c == 0):
       927 -                          skipped += 1
       928 -                          continue
       872 +              for future in as_completed(future_to_file):
       873 +                  xlsx_file = future_to_file[future]
       874 +                  try:
       875 +                      success = future.result()
       876 +                      if not success:
       877 +                          logger.error(f"Failed to process {xlsx_file}")
       878 +                  except Exception as e:
       879 +                      logger.error(f"Exception processing {xlsx_file}: {str(e)}")
       880 +      else:
       881 +          # Sequential file processing
       882 +          for xlsx_path_arg in args.xlsx_files:
       883 +              process_single_file(xlsx_path_arg, args)
       884 +      # --- END of optimized file processing ---
       885
       886 -                      safe = sanitize_filename(sheet_name, used_names)
       887 -                      if len(filtered_sheets) == 1:
       888 -                          csv_name = os.path.splitext(os.path.basename(xlsx_path))[0] + ".csv"
       889 -                      else:
       890 -                          csv_name = f"{safe}.csv"
       891 -                      csv_path = os.path.join(out_dir, csv_name)
       892 -                      try:
       893 -                          @timeout_handler(300)
       894 -                          def export_with_timeout():
       895 -                              if args.engine == "openpyxl":
       896 -                                  return _stream_write_csv(csv_path, _iter_rows(ws,
           - args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           - args.min_af_nonempty, args.af_gate_visible_only, show_progress=False), args.encoding, args.bom,
           - buffer_size=getattr(args, 'buffer_size', 65536))
       897 -                              else:
       898 -                                  return _stream_write_csv(csv_path, _iter_rows_pandas(df,
           - args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           - args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom, buffer_size=getattr(args,
           -  'buffer_size', 65536))
       899 -
       900 -                          row_count, col_count = export_with_timeout()
       901 -                          if row_count == 0:
       902 -                              skipped += 1
       903 -                              continue
       904 -                          exported += 1
       905 -                          last7_path = None
       906 -                          if is_df and args.report_last7:
       907 -                              last7_path = _write_last7_by_reportdate_csv(df, csv_path,
           - args.encoding, args.bom)
       908 -                          manifest["sheets"].append({
       909 -                              "sheet_name": sheet_name,
       910 -                              "sanitized_name": safe,
       911 -                              "file": csv_path,
       912 -                              "file_sha256": get_file_hash(csv_path),
       913 -                              "rows": row_count,
       914 -                              "cols": col_count,
       915 -                              "last7_file": last7_path
       916 -                          })
       917 -                      except ExportTimeoutError:
       918 -                          logger.error(f"Timeout during export of {safe}")
       919 -                          raise ExportError(f"Timeout exporting {safe}")
       920 -                      except Exception as e:
       921 -                          logger.error(f"Error exporting {safe}: {str(e)}")
       922 -                          raise ExportError(f"Error exporting {safe}: {str(e)}")
       923 -
       924 -              # Benchmark summary
       925 -              if args.benchmark and manifest["benchmark_results"]:
       926 -                  total_duration = 0
       927 -                  total_rows = 0
       928 -                  for benchmark in manifest["benchmark_results"]:
       929 -                      for engine, results in benchmark["results"].items():
       930 -                          total_duration += results.get("total_duration", 0)
       931 -                          if results.get("throughput_rows_sec", 0) > 0:
       932 -                              total_rows += results.get("throughput_rows_sec", 0) *
           - results.get("total_duration", 0)
       933 -
       934 -                  manifest["benchmark_summary"] = {
       935 -                      "total_duration_seconds": round(total_duration, 3),
       936 -                      "total_rows_processed": round(total_rows, 0),
       937 -                      "average_rows_per_second": round(total_rows / total_duration, 1) if
           - total_duration > 0 else 0,
       938 -                      "sheets_benchmarked": len(manifest["benchmark_results"])
       939 -                  }
       940 -
       941 -              with open(os.path.join(out_dir, "manifest.json"), "w", encoding="utf-8") as f:
       942 -                  json.dump(manifest, f, indent=2)  # Write manifest
       943 -
       944 -              # --- Print statements (inside loop) ---
       945 -              print(f"--- Processed File: {os.path.basename(xlsx_path)} ---")
       946 -              print(f"Source workbook : {xlsx_path}")
       947 -              print(f"Output folder   : {out_dir}")
       948 -              print(f"Sheets exported : {exported}")
       949 -              print(f"Sheets skipped  : {skipped} (empty or no columns)")
       950 -              print(f"Manifest        : {os.path.join(out_dir, 'manifest.json')}")
       951 -              print("\n") # Add a newline for separation
       952 -
       953 -          except Exception as e:
       954 -              # If one file fails, log it and continue to the next
       955 -              logger.error(f"Failed to process file {xlsx_path_arg}: {e}")
       956 -              print(f"--- FAILED to process file: {os.path.basename(xlsx_path_arg)} ---")
       957 -              print(f"Error: {e}")
       958 -              print("\n")
       959 -      # --- END of new loop ---
       960 -
       886    if __name__ == "__main__":
       887        main()
       888   \ No newline at end of file

● Update(Convert_to_CSV.bat)
  ⎿  Updated Convert_to_CSV.bat with 9 additions and 2 removals
       30    echo. 31    echo Running Python script...
       32
       33 -  REM --- Run Python with the file list ---
       34 -  py "C:\Dev\scripts\export_sheets.py" !FILE_LIST! --engine calamine
       33 +  REM --- Run Python with optimized performance settings ---
       34 +  REM Performance flags:
       35 +  REM   --engine calamine: Fast engine for reading Excel files
       36 +  REM   --parallel: Enable parallel sheet processing within each file
       37 +  REM   --max-parallel-sheets 6: Process up to 6 sheets simultaneously (optimal for most CPUs)
       38 +  REM   --max-parallel-files 2: Process up to 2 files simultaneously (when multiple files provided)
       39 +  REM   --buffer-size 131072: 128KB buffer for faster CSV writing
       40
       41 +  py "C:\Dev\scripts\export_sheets.py" !FILE_LIST! --engine calamine --parallel --max-parallel-sheets
          + 6 --max-parallel-files 2 --buffer-size 131072
       42 +
       43    if %ERRORLEVEL% NEQ 0 (
       44        echo. 45        echo ----------------------------------------

∴ Thought for 1s (ctrl+o to show thinking)

· Optimizing Convert_to_CSV.bat… (esc to interrupt · ctrl+t to show todos)
  ⎿  Next: Test and verify the optimizations work correctly

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
> 
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
  ⏵⏵ accept edits on (shift+tab to cycle)

╭─── Claude Code v2.0.35 ─────────────────────────────────────────────────────────────────────────╮
│                             │ Tips for getting started                                          │
│      Welcome back RAC! │ Run /init to create a CLAUDE.md file with instructions for Claude │
│                             │ ───────────────────────────────────────────────────────────────── │
│           ▐▛███▜▌           │ Recent activity                                                   │
│          ▝▜█████▛▘          │ No recent activity                                                │
│            ▘▘ ▝▝            │                                                                   │
│                             │                                                                   │
│   Sonnet 4.5 · Claude Pro   │                                                                   │
│       C:\Dev\scripts        │                                                                   │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯

> I need to optimize my Excel-to-CSV conversion workflow for better performance. I have two files:

1. C:\Dev\scripts\Convert_to_CSV.bat - A batch file that processes Excel files and passes them to Python
2. C:\Dev\scripts\export_sheets.py - A Python script that converts Excel files to CSV

Currently, the process works but is slower than I'd like, especially with large files. Here's my current batch file:
```batch
@echo off
title Convert to CSV

REM --- Enable delayed expansion for variables ---
setlocal enabledelayedexpansion

REM --- Debug information ---
echo Current directory: %CD%
echo Raw arguments: %*
echo. REM --- Check if any files were provided ---
if "%~1"=="" (
    echo No files provided. pause
    exit /b 1
)

REM --- Build file list properly ---
set "FILE_LIST="
:process_args
if "%~1"=="" goto done_args
    set "FILE_LIST=!FILE_LIST! "%~1""
    shift
    goto process_args
:done_args

REM --- Show what we're processing ---
echo Files to process: !FILE_LIST! echo. echo Running Python script...

REM --- Run Python with the file list ---
py "C:\Dev\scripts\export_sheets.py" !FILE_LIST! --engine calamine

if %ERRORLEVEL% NEQ 0 (
    echo. echo ----------------------------------------
    echo ERROR: Python script failed with code %ERRORLEVEL%
    echo ----------------------------------------
) else (
    echo. echo ----------------------------------------
    echo All files processed successfully. echo ----------------------------------------
)

pause
exit /b
```

Here's a portion of my export_sheets.py that shows the main functions:
```python
import argparse
import os
import sys
import pandas as pd
import json
import time
import logging
from datetime import datetime
from pathlib import Path
import concurrent.futures
from tqdm import tqdm

# Global variables and setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def export_sheet(xlsx_path, sheet_name, output_dir, engine='auto', chunksize=5000):
    """Export a single sheet to CSV.""" start_time = time.time()

    try:
        # Create output filename
        clean_sheet_name = "".join(c if c.isalnum() or c in "._- " else "_" for c in sheet_name)
        clean_sheet_name = clean_sheet_name.strip()
        output_file = os.path.join(output_dir, f"{clean_sheet_name}.csv")

        # Read the Excel sheet with specified engine
        if engine == 'calamine':
            # Using calamine approach
            import polars as pl
            df = pl.read_excel(xlsx_path, sheet_name=sheet_name)

            # Skip empty sheets or sheets with no columns
            if df.shape[0] == 0 or df.shape[1] == 0:
                return False, 0, 0, f"Skipped: Empty sheet or no columns"

            # Write to CSV
            df.write_csv(output_file)
            rows, cols = df.shape
        else:
            # Using pandas/openpyxl approach
            df = pd.read_excel(
                xlsx_path,
                sheet_name=sheet_name,
                engine='openpyxl' if engine == 'openpyxl' else None
            )

            # Skip empty sheets or sheets with no columns
            if df.shape[0] == 0 or df.shape[1] == 0:
                return False, 0, 0, f"Skipped: Empty sheet or no columns"

            # Write to CSV
            df.to_csv(output_file, index=False)
            rows, cols = df.shape

        duration = time.time() - start_time
        return True, rows, cols, f"Exported: {rows} rows, {cols} columns in {duration:.2f}s"

    except Exception as e:
        return False, 0, 0, f"Error: {str(e)}"

def get_sheet_names(xlsx_path, engine='auto'):
    """Get all sheet names from an Excel file.""" try:
        if engine == 'calamine':
            import polars as pl
            return pl.read_excel_schema(xlsx_path)
        else:
            return pd.ExcelFile(xlsx_path, engine='openpyxl' if engine == 'openpyxl' else None).sheet_names
    except Exception as e:
        logger.error(f"Failed to get sheet names: {e}")
        return []

def process_workbook(xlsx_path, engine='auto', chunksize=5000):
    """Process a whole Excel workbook and export all sheets.""" # Create output directory
    base_name = os.path.splitext(os.path.basename(xlsx_path))[0]
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M")
    output_dir = f"{os.path.dirname(xlsx_path)}/{base_name}_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)

    # Get all sheet names
    sheet_names = get_sheet_names(xlsx_path, engine)
    if not sheet_names:
        logger.error(f"No sheets found or error reading workbook: {xlsx_path}")
        return False

    # Process each sheet
    results = {}
    skipped = 0

    for sheet_name in tqdm(sheet_names, desc="Exporting sheets"):
        success, rows, cols, message = export_sheet(xlsx_path, sheet_name, output_dir, engine, chunksize)
        results[sheet_name] = {
            "success": success,
            "rows": rows,
            "columns": cols,
            "message": message
        }
        if not success:
            skipped += 1

    # Create manifest
    manifest = {
        "source": xlsx_path,
        "exported_at": datetime.now().isoformat(),
        "output_directory": output_dir,
        "sheets": results
    }

    with open(os.path.join(output_dir, "manifest.json"), "w") as f:
        json.dump(manifest, f, indent=2)

    # Report results
    print(f"--- Processed File: {os.path.basename(xlsx_path)} ---")
    print(f"Source workbook : {xlsx_path}")
    print(f"Output folder   : {output_dir}")
    print(f"Sheets exported : {len(sheet_names) - skipped}")
    print(f"Sheets skipped  : {skipped} (empty or no columns)")
    print(f"Manifest        : {os.path.join(output_dir, 'manifest.json')}")

    return True

def main():
    ap = argparse.ArgumentParser(description="Export all Excel sheets to individual CSV files.") ap.add_argument("xlsx_files", nargs='+', help="One or more paths to the source .xlsx files")
    ap.add_argument("--engine", default="auto", choices=["auto", "calamine", "openpyxl"],
                    help="Engine for reading Excel files")
    ap.add_argument("--chunksize", type=int, default=5000,
                    help="Chunk size for processing large files")

    args = ap.parse_args()

    # Check available memory and adjust chunksize if needed
    import psutil
    available_memory_mb = psutil.virtual_memory().available / (1024 * 1024)
    logger.info(f"Available memory: {available_memory_mb:.1f}MB")

    # Process each file
    for xlsx_path_arg in args.xlsx_files:
        try:
            xlsx_path = os.path.abspath(xlsx_path_arg)
            logger.info(f"Processing {os.path.basename(xlsx_path)} - Using engine: {args.engine}, chunksize:
{args.chunksize}, available memory: {available_memory_mb:.1f}MB")
            process_workbook(xlsx_path, args.engine, args.chunksize)
        except Exception as e:
            logger.error(f"Failed to process file: {os.path.basename(xlsx_path_arg)}")
            logger.error(f"Error: {e}")

if __name__ == "__main__":
    main()
```

Please enhance both files for maximum performance:

1. For the batch file (Convert_to_CSV.bat):
   - Optimize the command-line arguments to use the best performance settings
   - Keep the same basic structure and error handling
   - Don't change how it processes file arguments

2. For the Python script (export_sheets.py):
   - Add parallel processing capability for multi-file processing
   - Optimize memory usage and chunksize determination based on file size
   - Add multi-threaded sheet processing within a single workbook
   - Optimize the CSV writing process
   - Add command-line options for performance tuning
   - Keep the same output format and manifest generation

The main bottlenecks appear to be in:
- Sequential processing of multiple files
- Sequential processing of sheets within a workbook
- Sub-optimal memory usage and chunking strategy
- Inefficient data conversion for very large files

Please provide optimized versions of both files that maintain compatibility with my existing workflow but
significantly improve performance. ● I'll help you optimize both files for maximum performance. Let me first read the current files to understand the
  complete implementation, then create optimized versions. ● Read(export_sheets.py)
  ⎿  Read 853 lines

● Read(Convert_to_CSV.bat)
  ⎿  Read 49 lines

● Now I'll create optimized versions of both files. Let me update the todo list and proceed with the optimizations. ● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 6 additions and 4 removals
       475            logger.error(f"Error processing sheet {sheet_name}: {str(e)}")
       476            return {"sheet_name": sheet_name, "status": "error", "error": str(e)}
       477
       478 -  def _iter_rows(ws, visible_only: bool, date_fmt="%Y-%m-%d", dt_fmt="%Y-%m-%d %H:%M:%S",
           -  t_fmt="%H:%M:%S", af_columns="A,B,C,D,E,F", min_af_nonempty=4, af_gate_visible_only=False):
       478 +  def _iter_rows(ws, visible_only: bool, date_fmt="%Y-%m-%d", dt_fmt="%Y-%m-%d %H:%M:%S",
           +  t_fmt="%H:%M:%S", af_columns="A,B,C,D,E,F", min_af_nonempty=4, af_gate_visible_only=False
           +  , show_progress=False):
       479        """
       480        Iterator for openpyxl worksheet rows, applying visibility, AF gate, and formatting.