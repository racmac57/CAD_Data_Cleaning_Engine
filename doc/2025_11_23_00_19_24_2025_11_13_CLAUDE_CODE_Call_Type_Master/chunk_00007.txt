For the Python script (export_sheets.py):
   - Add parallel processing capability for multi-file processing
   - Optimize memory usage and chunksize determination based on file size
   - Add multi-threaded sheet processing within a single workbook
   - Optimize the CSV writing process
   - Add command-line options for performance tuning
   - Keep the same output format and manifest generation

The main bottlenecks appear to be in:
- Sequential processing of multiple files
- Sequential processing of sheets within a workbook
- Sub-optimal memory usage and chunking strategy
- Inefficient data conversion for very large files

Please provide optimized versions of both files that maintain compatibility with my existing workflow but
significantly improve performance. ● I'll help you optimize both files for maximum performance. Let me first read the current files to understand the
  complete implementation, then create optimized versions. ● Read(export_sheets.py)
  ⎿  Read 853 lines

● Read(Convert_to_CSV.bat)
  ⎿  Read 49 lines

● Now I'll create optimized versions of both files. Let me update the todo list and proceed with the optimizations. ● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 6 additions and 4 removals
       475            logger.error(f"Error processing sheet {sheet_name}: {str(e)}")
       476            return {"sheet_name": sheet_name, "status": "error", "error": str(e)}
       477
       478 -  def _iter_rows(ws, visible_only: bool, date_fmt="%Y-%m-%d", dt_fmt="%Y-%m-%d %H:%M:%S",
           -  t_fmt="%H:%M:%S", af_columns="A,B,C,D,E,F", min_af_nonempty=4, af_gate_visible_only=False):
       478 +  def _iter_rows(ws, visible_only: bool, date_fmt="%Y-%m-%d", dt_fmt="%Y-%m-%d %H:%M:%S",
           +  t_fmt="%H:%M:%S", af_columns="A,B,C,D,E,F", min_af_nonempty=4, af_gate_visible_only=False
           +  , show_progress=False):
       479        """
       480        Iterator for openpyxl worksheet rows, applying visibility, AF gate, and formatting. 481        :param ws: Worksheet
     ...
       486        :param af_columns: AF gate columns
       487        :param min_af_nonempty: Min non-empty
       488        :param af_gate_visible_only: Visible for gate
       489 +      :param show_progress: Show progress bar (disabled by default for performance)
       490        :yield: Formatted row lists
       491        """
       492        max_r, max_c = get_used(ws)  # Get range
     ...
       496                for rr in range(rng.min_row, rng.max_row + 1):
       497                    for cc in range(rng.min_col, rng.max_col + 1):
       498                        merged_top_left[(rr, cc)] = (tl_r, tl_c)
       499 -
       499 +
       500        af_cols = _parse_af_columns(af_columns)
       501 -
       502 -      for r in tqdm(range(1, max_r + 1), desc="Processing rows"):  # Progress bar
       501 +
       502 +      row_range = tqdm(range(1, max_r + 1), desc="Processing rows") if
           +  show_progress else range(1, max_r + 1)
       503 +      for r in row_range:  # Progress bar optional
       504            if visible_only and ws.row_dimensions[r].hidden:
       505                continue  # Skip hidden row
       506

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 4 additions and 3 removals
       537            if row_out and any(v != "" for v in row_out):
       538                yield row_out
       539
       540 -  def _stream_write_csv(path, rows_iterable, encoding: str, bom: bool):
       540 +  def _stream_write_csv(path, rows_iterable, encoding: str, bom: bool, buffer_size=65536):
       541        """
       542 -      Streams rows to CSV file. 542 +      Streams rows to CSV file with optimized buffering. 543        :param path: Output path
       544        :param rows_iterable: Iterable of rows
       545        :param encoding: Encoding
       546        :param bom: Add BOM for UTF-8
       547 +      :param buffer_size: Buffer size for file writing (default 64KB)
       548        :return: (row_count, max_cols)
       549        """
       550        os.makedirs(os.path.dirname(path), exist_ok=True)  # Ensure dir
       551        final_encoding = ("utf-8-sig" if bom and encoding.lower().replace("-", "") == "utf8" else
             encoding)
       552        row_count = 0
       553        max_cols_seen = 0
       554 -      with open(path, "w", encoding=final_encoding, newline="") as f:
       554 +      with open(path, "w", encoding=final_encoding, newline="", buffering=buffer_size) as f:
       555            writer = csv.writer(f)
       556            for row in rows_iterable:
       557                if row is None:

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 2 additions and 2 removals
       438            @timeout_handler(300)
       439            def export_with_timeout():
       440                if args.engine == "openpyxl":
       441 -                  return _stream_write_csv(csv_path, _iter_rows(ws, args.visible_only,
           -  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           -  args.af_gate_visible_only), args.encoding, args.bom)
       441 +                  return _stream_write_csv(csv_path, _iter_rows(ws, args.visible_only,
           +  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           +  args.af_gate_visible_only, show_progress=False), args.encoding, args.bom
           +  , buffer_size=getattr(args, 'buffer_size', 65536))
       442                else:
       443 -                  return _stream_write_csv(csv_path, _iter_rows_pandas(df, args.visible_only,
           -  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           -  args.af_gate_visible_only), args.encoding, args.bom)
       443 +                  return _stream_write_csv(csv_path, _iter_rows_pandas(df, args.visible_only,
           +  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           +  args.af_gate_visible_only), args.encoding, args.bom, buffer_size=getattr(args, 'buffer_size', 65536
           +  ))
       444
       445            row_count, col_count = export_with_timeout()  # Export with timeout
       446            last7_path = None

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 2 additions and 2 removals
       787                            @timeout_handler(300)
       788                            def export_with_timeout():
       789                                if args.engine == "openpyxl":
       790 -                                  return _stream_write_csv(csv_path, _iter_rows(ws,
           -  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           -  args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom)
       790 +                                  return _stream_write_csv(csv_path, _iter_rows(ws,
           +  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           +  args.min_af_nonempty, args.af_gate_visible_only, show_progress=False), args.encoding, args.bom
           +  , buffer_size=getattr(args, 'buffer_size', 65536))
       791                                else:
       792 -                                  return _stream_write_csv(csv_path, _iter_rows_pandas(df,
           -  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           -  args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom)
       792 +                                  return _stream_write_csv(csv_path, _iter_rows_pandas(df,
           +  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           +  args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom
           +  , buffer_size=getattr(args, 'buffer_size', 65536))
       793
       794                            row_count, col_count = export_with_timeout()
       795                            if row_count == 0:

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 217 additions and 6 removals
       598        _stream_write_csv(out_path, _iter_filtered_rows(), encoding, bom)
       599        return out_path
       600
       601 +  def process_single_file(xlsx_path_arg, args):
       602 +      """
       603 +      Process a single Excel file with all the export logic. 604 +      :param xlsx_path_arg: Path to Excel file
       605 +      :param args: Parsed arguments
       606 +      :return: Success status
       607 +      """
       608 +      try:
       609 +          xlsx_path = os.path.abspath(xlsx_path_arg)  # Absolute path
       610 +
       611 +          # Auto engine and chunksize
       612 +          file_size = os.path.getsize(xlsx_path)
       613 +          available_memory = psutil.virtual_memory().available / (1024 * 1024)  # MB
       614 +
       615 +          if args.engine == "auto":
       616 +              args.engine = "calamine"  # Default to calamine
       617 +
       618 +          if args.chunksize is None:
       619 +              args.chunksize = get_adaptive_chunksize(file_size, available_memory)
       620 +              if file_size > 50 * 1024 * 1024:
       621 +                  logger.info(f"Large file detected ({file_size / (1024*1024):.1f}MB), enabling
           + adaptive chunking with size {args.chunksize}")
       622 +
       623 +          logger.info(f"Processing {os.path.basename(xlsx_path)} - Using engine: {args.engine},
           + chunksize: {args.chunksize}, available memory: {available_memory:.1f}MB")
       624 +          if not os.path.isfile(xlsx_path):
       625 +              logger.error(f"File not found: {xlsx_path}")
       626 +              raise FileNotFoundError(f"File not found: {xlsx_path}")
       627 +
       628 +          # Set output dir: subfolder in source dir with timestamp
       629 +          source_dir = os.path.dirname(xlsx_path)
       630 +          wb_stem = os.path.splitext(os.path.basename(xlsx_path))[0]
       631 +          stamp = datetime.now().strftime("%Y-%m-%d_%H%M")
       632 +          out_dir = os.path.join(source_dir, f"{wb_stem}_{stamp}")
       633 +          os.makedirs(out_dir, exist_ok=True)
       634 +
       635 +          # Load sheet names based on engine
       636 +          wb = None
       637 +          if args.engine == "openpyxl":
       638 +              wb = load_workbook(xlsx_path, read_only=True, data_only=True)
       639 +              sheet_names = wb.sheetnames
       640 +          else:
       641 +              excel_file = pd.ExcelFile(xlsx_path, engine='calamine')
       642 +              sheet_names = excel_file.sheet_names
       643 +
       644 +          used_names = set()  # For unique filenames
       645 +          exported = 0
       646 +          skipped = 0
       647 +          stats = {"rows_skipped_af_gate": 0}
       648 +          benchmark_results = []  # List for benchmarks
       649 +
       650 +          # Manifest dict for output JSON
       651 +          manifest = {
       652 +              "workbook": xlsx_path,
       653 +              "timestamp": datetime.now().isoformat(),
       654 +              "output_dir": out_dir,
       655 +              "workbook_sha256": get_file_hash(xlsx_path),
       656 +              "engine": args.engine,
       657 +              "chunksize": args.chunksize,
       658 +              "benchmark_enabled": bool(args.benchmark),
       659 +              "parallel_enabled": bool(args.parallel),
       660 +              "max_parallel_sheets": args.max_parallel_sheets,
       661 +              "file_size_mb": round(file_size / (1024 * 1024), 2),
       662 +              "available_memory_mb": round(available_memory, 2),
       663 +              "rows_skipped_af_gate": stats.get("rows_skipped_af_gate", 0),
       664 +              "af_gate": {
       665 +                  "columns": args.af_columns,
       666 +                  "min_nonempty": args.min_af_nonempty,
       667 +                  "visible_only": bool(args.af_gate_visible_only),
       668 +                  "disabled": bool(args.disable_af_gate)
       669 +              },
       670 +              "sheets": [],
       671 +              "benchmark_results": []
       672 +          }
       673 +
       674 +          # Filter sheets
       675 +          filtered_sheets = []
       676 +          for sheet_name in sheet_names:
       677 +              if not args.all_sheets:
       678 +                  if args.exclude and any(pat.lower() in sheet_name.lower() for pat in args.exclude):
       679 +                      continue
       680 +                  if args.include and not any(pat.lower() in sheet_name.lower() for pat in
           + args.include):
       681 +                      continue
       682 +              filtered_sheets.append(sheet_name)
       683 +
       684 +          if args.parallel and len(filtered_sheets) > 1:
       685 +              # Parallel
       686 +              logger.info(f"Processing {len(filtered_sheets)} sheets in parallel (max
           + {args.max_parallel_sheets} workers)")
       687 +              with ThreadPoolExecutor(max_workers=args.max_parallel_sheets) as executor:
       688 +                  future_to_sheet = {
       689 +                      executor.submit(process_sheet_parallel, sheet_name, xlsx_path, out_dir, args,
           + args.engine, used_names, benchmark_results, None if len(filtered_sheets) != 1 else
           + os.path.splitext(os.path.basename(xlsx_path))[0]): sheet_name
       690 +                      for sheet_name in filtered_sheets
       691 +                  }
       692 +
       693 +                  for future in tqdm(as_completed(future_to_sheet), total=len(filtered_sheets),
           + desc="Processing sheets"):
       694 +                      sheet_name = future_to_sheet[future]
       695 +                      try:
       696 +                          result = future.result()
       697 +                          if result["status"] == "success":
       698 +                              exported += 1
       699 +                              manifest["sheets"].append(result)
       700 +                          elif result["status"] == "skipped":
       701 +                              skipped += 1
       702 +                          else:
       703 +                              logger.error(f"Error processing {sheet_name}: {result.get('error',
           + 'Unknown error')}")
       704 +                      except Exception as e:
       705 +                          logger.error(f"Exception processing {sheet_name}: {str(e)}")
       706 +          else:
       707 +              # Sequential
       708 +              for sheet_name in tqdm(filtered_sheets, desc="Exporting sheets"):
       709 +                  if args.benchmark:
       710 +                      benchmark_results = {}  # Should be list? Inconsistent with parallel
       711 +                      for eng in ['calamine', 'openpyxl']:
       712 +                          duration = benchmark_engine_performance(xlsx_path, sheet_name, eng,
           + is_detailed=True)
       713 +                          benchmark_results[eng] = duration
       714 +                          logger.info(f"Benchmark: {sheet_name} with {eng} - {json.dumps(duration)}")
       715 +                      manifest["benchmark_results"].append({"sheet_name": sheet_name, "results":
           + benchmark_results})
       716 +
       717 +                  # Load data
       718 +                  if args.engine == "openpyxl":
       719 +                      ws = wb[sheet_name]
       720 +                      max_r, max_c = _get_used_range(ws)
       721 +                      is_df = False
       722 +                  else:
       723 +                      df = pd.read_excel(xlsx_path, sheet_name=sheet_name, engine='calamine',
           + keep_default_na=False)
       724 +                      setattr(df, "_include_headers", True)
       725 +                      max_r, max_c = _get_used_range(df)
       726 +                      is_df = True
       727 +
       728 +                  if not args.include_empty and (max_r == 0 or max_c == 0):
       729 +                      skipped += 1
       730 +                      continue
       731 +
       732 +                  safe = sanitize_filename(sheet_name, used_names)
       733 +                  if len(filtered_sheets) == 1:
       734 +                      csv_name = os.path.splitext(os.path.basename(xlsx_path))[0] + ".csv"
       735 +                  else:
       736 +                      csv_name = f"{safe}.csv"
       737 +                  csv_path = os.path.join(out_dir, csv_name)
       738 +                  try:
       739 +                      @timeout_handler(300)
       740 +                      def export_with_timeout():
       741 +                          if args.engine == "openpyxl":
       742 +                              return _stream_write_csv(csv_path, _iter_rows(ws, args.visible_only,
           + args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           + args.af_gate_visible_only, show_progress=False), args.encoding, args.bom, buffer_size=getattr(args,
           + 'buffer_size', 65536))
       743 +                          else:
       744 +                              return _stream_write_csv(csv_path, _iter_rows_pandas(df,
           + args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           + args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom, buffer_size=getattr(args,
           +  'buffer_size', 65536))
       745 +
       746 +                      row_count, col_count = export_with_timeout()
       747 +                      if row_count == 0:
       748 +                          skipped += 1
       749 +                          continue
       750 +                      exported += 1
       751 +                      last7_path = None
       752 +                      if is_df and args.report_last7:
       753 +                          last7_path = _write_last7_by_reportdate_csv(df, csv_path, args.encoding,
           + args.bom)
       754 +                      manifest["sheets"].append({
       755 +                          "sheet_name": sheet_name,
       756 +                          "sanitized_name": safe,
       757 +                          "file": csv_path,
       758 +                          "file_sha256": get_file_hash(csv_path),
       759 +                          "rows": row_count,
       760 +                          "cols": col_count,
       761 +                          "last7_file": last7_path
       762 +                      })
       763 +                  except ExportTimeoutError:
       764 +                      logger.error(f"Timeout during export of {safe}")
       765 +                      raise ExportError(f"Timeout exporting {safe}")
       766 +                  except Exception as e:
       767 +                      logger.error(f"Error exporting {safe}: {str(e)}")
       768 +                      raise ExportError(f"Error exporting {safe}: {str(e)}")
       769 +
       770 +          # Benchmark summary
       771 +          if args.benchmark and manifest["benchmark_results"]:
       772 +              total_duration = 0
       773 +              total_rows = 0
       774 +              for benchmark in manifest["benchmark_results"]:
       775 +                  for engine, results in benchmark["results"].items():
       776 +                      total_duration += results.get("total_duration", 0)
       777 +                      if results.get("throughput_rows_sec", 0) > 0:
       778 +                          total_rows += results.get("throughput_rows_sec", 0) *
           + results.get("total_duration", 0)
       779 +
       780 +              manifest["benchmark_summary"] = {
       781 +                  "total_duration_seconds": round(total_duration, 3),
       782 +                  "total_rows_processed": round(total_rows, 0),
       783 +                  "average_rows_per_second": round(total_rows / total_duration, 1) if total_duration
           + > 0 else 0,
       784 +                  "sheets_benchmarked": len(manifest["benchmark_results"])
       785 +              }
       786 +
       787 +          with open(os.path.join(out_dir, "manifest.json"), "w", encoding="utf-8") as f:
       788 +              json.dump(manifest, f, indent=2)  # Write manifest
       789 +
       790 +          # --- Print statements (inside loop) ---
       791 +          print(f"--- Processed File: {os.path.basename(xlsx_path)} ---")
       792 +          print(f"Source workbook : {xlsx_path}")
       793 +          print(f"Output folder   : {out_dir}")
       794 +          print(f"Sheets exported : {exported}")
       795 +          print(f"Sheets skipped  : {skipped} (empty or no columns)")
       796 +          print(f"Manifest        : {os.path.join(out_dir, 'manifest.json')}")
       797 +          print("\n") # Add a newline for separation
       798 +          return True
       799 +
       800 +      except Exception as e:
       801 +          # If one file fails, log it and continue to the next
       802 +          logger.error(f"Failed to process file {xlsx_path_arg}: {e}")
       803 +          print(f"--- FAILED to process file: {os.path.basename(xlsx_path_arg)} ---")
       804 +          print(f"Error: {e}")
       805 +          print("\n")
       806 +          return False
       807 +
       808    def main():
       809        """
       810        Main function: Parses args, processes Excel, exports sheets, creates manifest. 811        """
       812        ap = argparse.ArgumentParser(description="Export all Excel sheets to individual CSV files.") 813 -
       813 +
       814        # --- THIS IS THE KEY CHANGE ---
       815        # It now accepts one or more files positionally, NOT with --xlsx
       816        ap.add_argument("xlsx_files", nargs='+', help="One or more paths to the source .xlsx files")
     ...
       632                        help="If set, only visible cells among --af-columns count toward the
             threshold.") 633        ap.add_argument("--disable-af-gate", action="store_true",
       634                        help="Disable A-F gate filtering (export all rows).")