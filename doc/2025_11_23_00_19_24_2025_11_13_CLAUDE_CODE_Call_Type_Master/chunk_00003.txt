● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 6 additions and 4 removals
       475            logger.error(f"Error processing sheet {sheet_name}: {str(e)}")
       476            return {"sheet_name": sheet_name, "status": "error", "error": str(e)}
       477
       478 -  def _iter_rows(ws, visible_only: bool, date_fmt="%Y-%m-%d", dt_fmt="%Y-%m-%d %H:%M:%S",
           -  t_fmt="%H:%M:%S", af_columns="A,B,C,D,E,F", min_af_nonempty=4, af_gate_visible_only=False):
       478 +  def _iter_rows(ws, visible_only: bool, date_fmt="%Y-%m-%d", dt_fmt="%Y-%m-%d %H:%M:%S",
           +  t_fmt="%H:%M:%S", af_columns="A,B,C,D,E,F", min_af_nonempty=4, af_gate_visible_only=False
           +  , show_progress=False):
       479        """
       480        Iterator for openpyxl worksheet rows, applying visibility, AF gate, and formatting. 481        :param ws: Worksheet
     ...
       486        :param af_columns: AF gate columns
       487        :param min_af_nonempty: Min non-empty
       488        :param af_gate_visible_only: Visible for gate
       489 +      :param show_progress: Show progress bar (disabled by default for performance)
       490        :yield: Formatted row lists
       491        """
       492        max_r, max_c = get_used(ws)  # Get range
     ...
       496                for rr in range(rng.min_row, rng.max_row + 1):
       497                    for cc in range(rng.min_col, rng.max_col + 1):
       498                        merged_top_left[(rr, cc)] = (tl_r, tl_c)
       499 -
       499 +
       500        af_cols = _parse_af_columns(af_columns)
       501 -
       502 -      for r in tqdm(range(1, max_r + 1), desc="Processing rows"):  # Progress bar
       501 +
       502 +      row_range = tqdm(range(1, max_r + 1), desc="Processing rows") if
           +  show_progress else range(1, max_r + 1)
       503 +      for r in row_range:  # Progress bar optional
       504            if visible_only and ws.row_dimensions[r].hidden:
       505                continue  # Skip hidden row
       506

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 4 additions and 3 removals
       537            if row_out and any(v != "" for v in row_out):
       538                yield row_out
       539
       540 -  def _stream_write_csv(path, rows_iterable, encoding: str, bom: bool):
       540 +  def _stream_write_csv(path, rows_iterable, encoding: str, bom: bool, buffer_size=65536):
       541        """
       542 -      Streams rows to CSV file. 542 +      Streams rows to CSV file with optimized buffering. 543        :param path: Output path
       544        :param rows_iterable: Iterable of rows
       545        :param encoding: Encoding
       546        :param bom: Add BOM for UTF-8
       547 +      :param buffer_size: Buffer size for file writing (default 64KB)
       548        :return: (row_count, max_cols)
       549        """
       550        os.makedirs(os.path.dirname(path), exist_ok=True)  # Ensure dir
       551        final_encoding = ("utf-8-sig" if bom and encoding.lower().replace("-", "") == "utf8" else
             encoding)
       552        row_count = 0
       553        max_cols_seen = 0
       554 -      with open(path, "w", encoding=final_encoding, newline="") as f:
       554 +      with open(path, "w", encoding=final_encoding, newline="", buffering=buffer_size) as f:
       555            writer = csv.writer(f)
       556            for row in rows_iterable:
       557                if row is None:

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 2 additions and 2 removals
       438            @timeout_handler(300)
       439            def export_with_timeout():
       440                if args.engine == "openpyxl":
       441 -                  return _stream_write_csv(csv_path, _iter_rows(ws, args.visible_only,
           -  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           -  args.af_gate_visible_only), args.encoding, args.bom)
       441 +                  return _stream_write_csv(csv_path, _iter_rows(ws, args.visible_only,
           +  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           +  args.af_gate_visible_only, show_progress=False), args.encoding, args.bom
           +  , buffer_size=getattr(args, 'buffer_size', 65536))
       442                else:
       443 -                  return _stream_write_csv(csv_path, _iter_rows_pandas(df, args.visible_only,
           -  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           -  args.af_gate_visible_only), args.encoding, args.bom)
       443 +                  return _stream_write_csv(csv_path, _iter_rows_pandas(df, args.visible_only,
           +  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           +  args.af_gate_visible_only), args.encoding, args.bom, buffer_size=getattr(args, 'buffer_size', 65536
           +  ))
       444
       445            row_count, col_count = export_with_timeout()  # Export with timeout
       446            last7_path = None

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 2 additions and 2 removals
       787                            @timeout_handler(300)
       788                            def export_with_timeout():
       789                                if args.engine == "openpyxl":
       790 -                                  return _stream_write_csv(csv_path, _iter_rows(ws,
           -  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           -  args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom)
       790 +                                  return _stream_write_csv(csv_path, _iter_rows(ws,
           +  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           +  args.min_af_nonempty, args.af_gate_visible_only, show_progress=False), args.encoding, args.bom
           +  , buffer_size=getattr(args, 'buffer_size', 65536))
       791                                else:
       792 -                                  return _stream_write_csv(csv_path, _iter_rows_pandas(df,
           -  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           -  args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom)
       792 +                                  return _stream_write_csv(csv_path, _iter_rows_pandas(df,
           +  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           +  args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom
           +  , buffer_size=getattr(args, 'buffer_size', 65536))
       793
       794                            row_count, col_count = export_with_timeout()
       795                            if row_count == 0:

∴ Thought for 3s (ctrl+o to show thinking)

· Optimizing export_sheets.py… (esc to interrupt · ctrl+t to show todos)
  ⎿  Next: Optimize Convert_to_CSV.bat for best performance settings

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
> 
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
  ⏵⏵ accept edits on (shift+tab to cycle)

╭─── Claude Code v2.0.35 ─────────────────────────────────────────────────────────────────────────╮
│                             │ Tips for getting started                                          │
│      Welcome back RAC! │ Run /init to create a CLAUDE.md file with instructions for Claude │
│                             │ ───────────────────────────────────────────────────────────────── │
│           ▐▛███▜▌           │ Recent activity                                                   │
│          ▝▜█████▛▘          │ No recent activity                                                │
│            ▘▘ ▝▝            │                                                                   │
│                             │                                                                   │
│   Sonnet 4.5 · Claude Pro   │                                                                   │
│       C:\Dev\scripts        │                                                                   │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯

> I need to optimize my Excel-to-CSV conversion workflow for better performance. I have two files:

1. C:\Dev\scripts\Convert_to_CSV.bat - A batch file that processes Excel files and passes them to Python
2. C:\Dev\scripts\export_sheets.py - A Python script that converts Excel files to CSV

Currently, the process works but is slower than I'd like, especially with large files. Here's my current batch file:
```batch
@echo off
title Convert to CSV

REM --- Enable delayed expansion for variables ---
setlocal enabledelayedexpansion

REM --- Debug information ---
echo Current directory: %CD%
echo Raw arguments: %*
echo. REM --- Check if any files were provided ---
if "%~1"=="" (
    echo No files provided. pause
    exit /b 1
)

REM --- Build file list properly ---
set "FILE_LIST="
:process_args
if "%~1"=="" goto done_args
    set "FILE_LIST=!FILE_LIST! "%~1""
    shift
    goto process_args
:done_args

REM --- Show what we're processing ---
echo Files to process: !FILE_LIST! echo. echo Running Python script...

REM --- Run Python with the file list ---
py "C:\Dev\scripts\export_sheets.py" !FILE_LIST! --engine calamine

if %ERRORLEVEL% NEQ 0 (
    echo. echo ----------------------------------------
    echo ERROR: Python script failed with code %ERRORLEVEL%
    echo ----------------------------------------
) else (
    echo. echo ----------------------------------------
    echo All files processed successfully. echo ----------------------------------------
)

pause
exit /b
```

Here's a portion of my export_sheets.py that shows the main functions:
```python
import argparse
import os
import sys
import pandas as pd
import json
import time
import logging
from datetime import datetime
from pathlib import Path
import concurrent.futures
from tqdm import tqdm

# Global variables and setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def export_sheet(xlsx_path, sheet_name, output_dir, engine='auto', chunksize=5000):
    """Export a single sheet to CSV.""" start_time = time.time()

    try:
        # Create output filename
        clean_sheet_name = "".join(c if c.isalnum() or c in "._- " else "_" for c in sheet_name)
        clean_sheet_name = clean_sheet_name.strip()
        output_file = os.path.join(output_dir, f"{clean_sheet_name}.csv")

        # Read the Excel sheet with specified engine
        if engine == 'calamine':
            # Using calamine approach
            import polars as pl
            df = pl.read_excel(xlsx_path, sheet_name=sheet_name)

            # Skip empty sheets or sheets with no columns
            if df.shape[0] == 0 or df.shape[1] == 0:
                return False, 0, 0, f"Skipped: Empty sheet or no columns"

            # Write to CSV
            df.write_csv(output_file)
            rows, cols = df.shape
        else:
            # Using pandas/openpyxl approach
            df = pd.read_excel(
                xlsx_path,
                sheet_name=sheet_name,
                engine='openpyxl' if engine == 'openpyxl' else None
            )

            # Skip empty sheets or sheets with no columns
            if df.shape[0] == 0 or df.shape[1] == 0:
                return False, 0, 0, f"Skipped: Empty sheet or no columns"

            # Write to CSV
            df.to_csv(output_file, index=False)
            rows, cols = df.shape

        duration = time.time() - start_time
        return True, rows, cols, f"Exported: {rows} rows, {cols} columns in {duration:.2f}s"

    except Exception as e:
        return False, 0, 0, f"Error: {str(e)}"

def get_sheet_names(xlsx_path, engine='auto'):
    """Get all sheet names from an Excel file.""" try:
        if engine == 'calamine':
            import polars as pl
            return pl.read_excel_schema(xlsx_path)
        else:
            return pd.ExcelFile(xlsx_path, engine='openpyxl' if engine == 'openpyxl' else None).sheet_names
    except Exception as e:
        logger.error(f"Failed to get sheet names: {e}")
        return []

def process_workbook(xlsx_path, engine='auto', chunksize=5000):
    """Process a whole Excel workbook and export all sheets.""" # Create output directory
    base_name = os.path.splitext(os.path.basename(xlsx_path))[0]
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M")
    output_dir = f"{os.path.dirname(xlsx_path)}/{base_name}_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)

    # Get all sheet names
    sheet_names = get_sheet_names(xlsx_path, engine)
    if not sheet_names:
        logger.error(f"No sheets found or error reading workbook: {xlsx_path}")
        return False

    # Process each sheet
    results = {}
    skipped = 0

    for sheet_name in tqdm(sheet_names, desc="Exporting sheets"):
        success, rows, cols, message = export_sheet(xlsx_path, sheet_name, output_dir, engine, chunksize)
        results[sheet_name] = {
            "success": success,
            "rows": rows,
            "columns": cols,
            "message": message
        }
        if not success:
            skipped += 1

    # Create manifest
    manifest = {
        "source": xlsx_path,
        "exported_at": datetime.now().isoformat(),
        "output_directory": output_dir,
        "sheets": results
    }

    with open(os.path.join(output_dir, "manifest.json"), "w") as f:
        json.dump(manifest, f, indent=2)

    # Report results
    print(f"--- Processed File: {os.path.basename(xlsx_path)} ---")
    print(f"Source workbook : {xlsx_path}")
    print(f"Output folder   : {output_dir}")
    print(f"Sheets exported : {len(sheet_names) - skipped}")
    print(f"Sheets skipped  : {skipped} (empty or no columns)")
    print(f"Manifest        : {os.path.join(output_dir, 'manifest.json')}")

    return True

def main():
    ap = argparse.ArgumentParser(description="Export all Excel sheets to individual CSV files.") ap.add_argument("xlsx_files", nargs='+', help="One or more paths to the source .xlsx files")
    ap.add_argument("--engine", default="auto", choices=["auto", "calamine", "openpyxl"],
                    help="Engine for reading Excel files")
    ap.add_argument("--chunksize", type=int, default=5000,
                    help="Chunk size for processing large files")

    args = ap.parse_args()

    # Check available memory and adjust chunksize if needed
    import psutil
    available_memory_mb = psutil.virtual_memory().available / (1024 * 1024)
    logger.info(f"Available memory: {available_memory_mb:.1f}MB")

    # Process each file
    for xlsx_path_arg in args.xlsx_files:
        try:
            xlsx_path = os.path.abspath(xlsx_path_arg)
            logger.info(f"Processing {os.path.basename(xlsx_path)} - Using engine: {args.engine}, chunksize:
{args.chunksize}, available memory: {available_memory_mb:.1f}MB")
            process_workbook(xlsx_path, args.engine, args.chunksize)
        except Exception as e:
            logger.error(f"Failed to process file: {os.path.basename(xlsx_path_arg)}")
            logger.error(f"Error: {e}")

if __name__ == "__main__":
    main()
```

Please enhance both files for maximum performance:

1. For the batch file (Convert_to_CSV.bat):
   - Optimize the command-line arguments to use the best performance settings
   - Keep the same basic structure and error handling
   - Don't change how it processes file arguments

2. For the Python script (export_sheets.py):
   - Add parallel processing capability for multi-file processing
   - Optimize memory usage and chunksize determination based on file size
   - Add multi-threaded sheet processing within a single workbook
   - Optimize the CSV writing process
   - Add command-line options for performance tuning
   - Keep the same output format and manifest generation

The main bottlenecks appear to be in:
- Sequential processing of multiple files
- Sequential processing of sheets within a workbook
- Sub-optimal memory usage and chunking strategy
- Inefficient data conversion for very large files

Please provide optimized versions of both files that maintain compatibility with my existing workflow but
significantly improve performance. ● I'll help you optimize both files for maximum performance. Let me first read the current files to understand the
  complete implementation, then create optimized versions. ● Read(export_sheets.py)
  ⎿  Read 853 lines

● Read(Convert_to_CSV.bat)
  ⎿  Read 49 lines

● Now I'll create optimized versions of both files. Let me update the todo list and proceed with the optimizations. ● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 6 additions and 4 removals
       475            logger.error(f"Error processing sheet {sheet_name}: {str(e)}")
       476            return {"sheet_name": sheet_name, "status": "error", "error": str(e)}
       477
       478 -  def _iter_rows(ws, visible_only: bool, date_fmt="%Y-%m-%d", dt_fmt="%Y-%m-%d %H:%M:%S",
           -  t_fmt="%H:%M:%S", af_columns="A,B,C,D,E,F", min_af_nonempty=4, af_gate_visible_only=False):
       478 +  def _iter_rows(ws, visible_only: bool, date_fmt="%Y-%m-%d", dt_fmt="%Y-%m-%d %H:%M:%S",
           +  t_fmt="%H:%M:%S", af_columns="A,B,C,D,E,F", min_af_nonempty=4, af_gate_visible_only=False
           +  , show_progress=False):
       479        """
       480        Iterator for openpyxl worksheet rows, applying visibility, AF gate, and formatting. 481        :param ws: Worksheet
     ...
       486        :param af_columns: AF gate columns
       487        :param min_af_nonempty: Min non-empty
       488        :param af_gate_visible_only: Visible for gate
       489 +      :param show_progress: Show progress bar (disabled by default for performance)
       490        :yield: Formatted row lists
       491        """
       492        max_r, max_c = get_used(ws)  # Get range
     ...
       496                for rr in range(rng.min_row, rng.max_row + 1):
       497                    for cc in range(rng.min_col, rng.max_col + 1):
       498                        merged_top_left[(rr, cc)] = (tl_r, tl_c)
       499 -
       499 +
       500        af_cols = _parse_af_columns(af_columns)
       501 -
       502 -      for r in tqdm(range(1, max_r + 1), desc="Processing rows"):  # Progress bar
       501 +
       502 +      row_range = tqdm(range(1, max_r + 1), desc="Processing rows") if
           +  show_progress else range(1, max_r + 1)
       503 +      for r in row_range:  # Progress bar optional
       504            if visible_only and ws.row_dimensions[r].hidden:
       505                continue  # Skip hidden row
       506

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 4 additions and 3 removals
       537            if row_out and any(v != "" for v in row_out):
       538                yield row_out
       539
       540 -  def _stream_write_csv(path, rows_iterable, encoding: str, bom: bool):
       540 +  def _stream_write_csv(path, rows_iterable, encoding: str, bom: bool, buffer_size=65536):
       541        """
       542 -      Streams rows to CSV file. 542 +      Streams rows to CSV file with optimized buffering. 543        :param path: Output path
       544        :param rows_iterable: Iterable of rows
       545        :param encoding: Encoding
       546        :param bom: Add BOM for UTF-8
       547 +      :param buffer_size: Buffer size for file writing (default 64KB)
       548        :return: (row_count, max_cols)
       549        """
       550        os.makedirs(os.path.dirname(path), exist_ok=True)  # Ensure dir
       551        final_encoding = ("utf-8-sig" if bom and encoding.lower().replace("-", "") == "utf8" else
             encoding)
       552        row_count = 0
       553        max_cols_seen = 0
       554 -      with open(path, "w", encoding=final_encoding, newline="") as f:
       554 +      with open(path, "w", encoding=final_encoding, newline="", buffering=buffer_size) as f:
       555            writer = csv.writer(f)
       556            for row in rows_iterable:
       557                if row is None:

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 2 additions and 2 removals
       438            @timeout_handler(300)
       439            def export_with_timeout():
       440                if args.engine == "openpyxl":
       441 -                  return _stream_write_csv(csv_path, _iter_rows(ws, args.visible_only,
           -  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           -  args.af_gate_visible_only), args.encoding, args.bom)
       441 +                  return _stream_write_csv(csv_path, _iter_rows(ws, args.visible_only,
           +  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           +  args.af_gate_visible_only, show_progress=False), args.encoding, args.bom
           +  , buffer_size=getattr(args, 'buffer_size', 65536))
       442                else:
       443 -                  return _stream_write_csv(csv_path, _iter_rows_pandas(df, args.visible_only,
           -  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           -  args.af_gate_visible_only), args.encoding, args.bom)
       443 +                  return _stream_write_csv(csv_path, _iter_rows_pandas(df, args.visible_only,
           +  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           +  args.af_gate_visible_only), args.encoding, args.bom, buffer_size=getattr(args, 'buffer_size', 65536
           +  ))
       444
       445            row_count, col_count = export_with_timeout()  # Export with timeout
       446            last7_path = None

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 2 additions and 2 removals
       787                            @timeout_handler(300)
       788                            def export_with_timeout():
       789                                if args.engine == "openpyxl":
       790 -                                  return _stream_write_csv(csv_path, _iter_rows(ws,
           -  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           -  args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom)
       790 +                                  return _stream_write_csv(csv_path, _iter_rows(ws,
           +  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           +  args.min_af_nonempty, args.af_gate_visible_only, show_progress=False), args.encoding, args.bom
           +  , buffer_size=getattr(args, 'buffer_size', 65536))
       791                                else:
       792 -                                  return _stream_write_csv(csv_path, _iter_rows_pandas(df,
           -  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           -  args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom)
       792 +                                  return _stream_write_csv(csv_path, _iter_rows_pandas(df,
           +  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           +  args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom
           +  , buffer_size=getattr(args, 'buffer_size', 65536))
       793
       794                            row_count, col_count = export_with_timeout()
       795                            if row_count == 0:

∴ Thought for 3s (ctrl+o to show thinking)

· Optimizing export_sheets.py… (esc to interrupt · ctrl+t to show todos)
  ⎿  Next: Optimize Convert_to_CSV.bat for best performance settings

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
> 
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
  ⏵⏵ accept edits on (shift+tab to cycle)

╭─── Claude Code v2.0.35 ─────────────────────────────────────────────────────────────────────────╮
│                             │ Tips for getting started                                          │
│      Welcome back RAC! │ Run /init to create a CLAUDE.md file with instructions for Claude │
│                             │ ───────────────────────────────────────────────────────────────── │
│           ▐▛███▜▌           │ Recent activity                                                   │
│          ▝▜█████▛▘          │ No recent activity                                                │
│            ▘▘ ▝▝            │                                                                   │
│                             │                                                                   │
│   Sonnet 4.5 · Claude Pro   │                                                                   │
│       C:\Dev\scripts        │                                                                   │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯

> I need to optimize my Excel-to-CSV conversion workflow for better performance. I have two files:

1. C:\Dev\scripts\Convert_to_CSV.bat - A batch file that processes Excel files and passes them to Python
2. C:\Dev\scripts\export_sheets.py - A Python script that converts Excel files to CSV

Currently, the process works but is slower than I'd like, especially with large files. Here's my current batch file:
```batch
@echo off
title Convert to CSV

REM --- Enable delayed expansion for variables ---
setlocal enabledelayedexpansion

REM --- Debug information ---
echo Current directory: %CD%
echo Raw arguments: %*
echo. REM --- Check if any files were provided ---
if "%~1"=="" (
    echo No files provided. pause
    exit /b 1
)

REM --- Build file list properly ---
set "FILE_LIST="
:process_args
if "%~1"=="" goto done_args
    set "FILE_LIST=!FILE_LIST! "%~1""
    shift
    goto process_args
:done_args

REM --- Show what we're processing ---
echo Files to process: !FILE_LIST! echo. echo Running Python script...

REM --- Run Python with the file list ---
py "C:\Dev\scripts\export_sheets.py" !FILE_LIST! --engine calamine

if %ERRORLEVEL% NEQ 0 (
    echo. echo ----------------------------------------
    echo ERROR: Python script failed with code %ERRORLEVEL%
    echo ----------------------------------------
) else (
    echo. echo ----------------------------------------
    echo All files processed successfully. echo ----------------------------------------
)

pause
exit /b
```

Here's a portion of my export_sheets.py that shows the main functions:
```python
import argparse
import os
import sys
import pandas as pd
import json
import time
import logging
from datetime import datetime
from pathlib import Path
import concurrent.futures
from tqdm import tqdm

# Global variables and setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def export_sheet(xlsx_path, sheet_name, output_dir, engine='auto', chunksize=5000):
    """Export a single sheet to CSV.""" start_time = time.time()

    try:
        # Create output filename
        clean_sheet_name = "".join(c if c.isalnum() or c in "._- " else "_" for c in sheet_name)
        clean_sheet_name = clean_sheet_name.strip()
        output_file = os.path.join(output_dir, f"{clean_sheet_name}.csv")

        # Read the Excel sheet with specified engine
        if engine == 'calamine':
            # Using calamine approach
            import polars as pl
            df = pl.read_excel(xlsx_path, sheet_name=sheet_name)

            # Skip empty sheets or sheets with no columns
            if df.shape[0] == 0 or df.shape[1] == 0:
                return False, 0, 0, f"Skipped: Empty sheet or no columns"

            # Write to CSV
            df.write_csv(output_file)
            rows, cols = df.shape
        else:
            # Using pandas/openpyxl approach
            df = pd.read_excel(
                xlsx_path,
                sheet_name=sheet_name,
                engine='openpyxl' if engine == 'openpyxl' else None
            )

            # Skip empty sheets or sheets with no columns
            if df.shape[0] == 0 or df.shape[1] == 0:
                return False, 0, 0, f"Skipped: Empty sheet or no columns"

            # Write to CSV
            df.to_csv(output_file, index=False)
            rows, cols = df.shape

        duration = time.time() - start_time
        return True, rows, cols, f"Exported: {rows} rows, {cols} columns in {duration:.2f}s"

    except Exception as e:
        return False, 0, 0, f"Error: {str(e)}"

def get_sheet_names(xlsx_path, engine='auto'):
    """Get all sheet names from an Excel file.""" try:
        if engine == 'calamine':
            import polars as pl
            return pl.read_excel_schema(xlsx_path)
        else:
            return pd.ExcelFile(xlsx_path, engine='openpyxl' if engine == 'openpyxl' else None).sheet_names
    except Exception as e:
        logger.error(f"Failed to get sheet names: {e}")
        return []

def process_workbook(xlsx_path, engine='auto', chunksize=5000):
    """Process a whole Excel workbook and export all sheets."""