543        :param path: Output path
       544        :param rows_iterable: Iterable of rows
       545        :param encoding: Encoding
       546        :param bom: Add BOM for UTF-8
       547 +      :param buffer_size: Buffer size for file writing (default 64KB)
       548        :return: (row_count, max_cols)
       549        """
       550        os.makedirs(os.path.dirname(path), exist_ok=True)  # Ensure dir
       551        final_encoding = ("utf-8-sig" if bom and encoding.lower().replace("-", "") == "utf8" else
             encoding)
       552        row_count = 0
       553        max_cols_seen = 0
       554 -      with open(path, "w", encoding=final_encoding, newline="") as f:
       554 +      with open(path, "w", encoding=final_encoding, newline="", buffering=buffer_size) as f:
       555            writer = csv.writer(f)
       556            for row in rows_iterable:
       557                if row is None:

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 2 additions and 2 removals
       438            @timeout_handler(300)
       439            def export_with_timeout():
       440                if args.engine == "openpyxl":
       441 -                  return _stream_write_csv(csv_path, _iter_rows(ws, args.visible_only,
           -  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           -  args.af_gate_visible_only), args.encoding, args.bom)
       441 +                  return _stream_write_csv(csv_path, _iter_rows(ws, args.visible_only,
           +  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           +  args.af_gate_visible_only, show_progress=False), args.encoding, args.bom
           +  , buffer_size=getattr(args, 'buffer_size', 65536))
       442                else:
       443 -                  return _stream_write_csv(csv_path, _iter_rows_pandas(df, args.visible_only,
           -  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           -  args.af_gate_visible_only), args.encoding, args.bom)
       443 +                  return _stream_write_csv(csv_path, _iter_rows_pandas(df, args.visible_only,
           +  args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           +  args.af_gate_visible_only), args.encoding, args.bom, buffer_size=getattr(args, 'buffer_size', 65536
           +  ))
       444
       445            row_count, col_count = export_with_timeout()  # Export with timeout
       446            last7_path = None

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 2 additions and 2 removals
       787                            @timeout_handler(300)
       788                            def export_with_timeout():
       789                                if args.engine == "openpyxl":
       790 -                                  return _stream_write_csv(csv_path, _iter_rows(ws,
           -  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           -  args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom)
       790 +                                  return _stream_write_csv(csv_path, _iter_rows(ws,
           +  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           +  args.min_af_nonempty, args.af_gate_visible_only, show_progress=False), args.encoding, args.bom
           +  , buffer_size=getattr(args, 'buffer_size', 65536))
       791                                else:
       792 -                                  return _stream_write_csv(csv_path, _iter_rows_pandas(df,
           -  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           -  args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom)
       792 +                                  return _stream_write_csv(csv_path, _iter_rows_pandas(df,
           +  args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           +  args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom
           +  , buffer_size=getattr(args, 'buffer_size', 65536))
       793
       794                            row_count, col_count = export_with_timeout()
       795                            if row_count == 0:

● Update(export_sheets.py)
  ⎿  Updated export_sheets.py with 217 additions and 6 removals
       598        _stream_write_csv(out_path, _iter_filtered_rows(), encoding, bom)
       599        return out_path
       600
       601 +  def process_single_file(xlsx_path_arg, args):
       602 +      """
       603 +      Process a single Excel file with all the export logic. 604 +      :param xlsx_path_arg: Path to Excel file
       605 +      :param args: Parsed arguments
       606 +      :return: Success status
       607 +      """
       608 +      try:
       609 +          xlsx_path = os.path.abspath(xlsx_path_arg)  # Absolute path
       610 +
       611 +          # Auto engine and chunksize
       612 +          file_size = os.path.getsize(xlsx_path)
       613 +          available_memory = psutil.virtual_memory().available / (1024 * 1024)  # MB
       614 +
       615 +          if args.engine == "auto":
       616 +              args.engine = "calamine"  # Default to calamine
       617 +
       618 +          if args.chunksize is None:
       619 +              args.chunksize = get_adaptive_chunksize(file_size, available_memory)
       620 +              if file_size > 50 * 1024 * 1024:
       621 +                  logger.info(f"Large file detected ({file_size / (1024*1024):.1f}MB), enabling
           + adaptive chunking with size {args.chunksize}")
       622 +
       623 +          logger.info(f"Processing {os.path.basename(xlsx_path)} - Using engine: {args.engine},
           + chunksize: {args.chunksize}, available memory: {available_memory:.1f}MB")
       624 +          if not os.path.isfile(xlsx_path):
       625 +              logger.error(f"File not found: {xlsx_path}")
       626 +              raise FileNotFoundError(f"File not found: {xlsx_path}")
       627 +
       628 +          # Set output dir: subfolder in source dir with timestamp
       629 +          source_dir = os.path.dirname(xlsx_path)
       630 +          wb_stem = os.path.splitext(os.path.basename(xlsx_path))[0]
       631 +          stamp = datetime.now().strftime("%Y-%m-%d_%H%M")
       632 +          out_dir = os.path.join(source_dir, f"{wb_stem}_{stamp}")
       633 +          os.makedirs(out_dir, exist_ok=True)
       634 +
       635 +          # Load sheet names based on engine
       636 +          wb = None
       637 +          if args.engine == "openpyxl":
       638 +              wb = load_workbook(xlsx_path, read_only=True, data_only=True)
       639 +              sheet_names = wb.sheetnames
       640 +          else:
       641 +              excel_file = pd.ExcelFile(xlsx_path, engine='calamine')
       642 +              sheet_names = excel_file.sheet_names
       643 +
       644 +          used_names = set()  # For unique filenames
       645 +          exported = 0
       646 +          skipped = 0
       647 +          stats = {"rows_skipped_af_gate": 0}
       648 +          benchmark_results = []  # List for benchmarks
       649 +
       650 +          # Manifest dict for output JSON
       651 +          manifest = {
       652 +              "workbook": xlsx_path,
       653 +              "timestamp": datetime.now().isoformat(),
       654 +              "output_dir": out_dir,
       655 +              "workbook_sha256": get_file_hash(xlsx_path),
       656 +              "engine": args.engine,
       657 +              "chunksize": args.chunksize,
       658 +              "benchmark_enabled": bool(args.benchmark),
       659 +              "parallel_enabled": bool(args.parallel),
       660 +              "max_parallel_sheets": args.max_parallel_sheets,
       661 +              "file_size_mb": round(file_size / (1024 * 1024), 2),
       662 +              "available_memory_mb": round(available_memory, 2),
       663 +              "rows_skipped_af_gate": stats.get("rows_skipped_af_gate", 0),
       664 +              "af_gate": {
       665 +                  "columns": args.af_columns,
       666 +                  "min_nonempty": args.min_af_nonempty,
       667 +                  "visible_only": bool(args.af_gate_visible_only),
       668 +                  "disabled": bool(args.disable_af_gate)
       669 +              },
       670 +              "sheets": [],
       671 +              "benchmark_results": []
       672 +          }
       673 +
       674 +          # Filter sheets
       675 +          filtered_sheets = []
       676 +          for sheet_name in sheet_names:
       677 +              if not args.all_sheets:
       678 +                  if args.exclude and any(pat.lower() in sheet_name.lower() for pat in args.exclude):
       679 +                      continue
       680 +                  if args.include and not any(pat.lower() in sheet_name.lower() for pat in
           + args.include):
       681 +                      continue
       682 +              filtered_sheets.append(sheet_name)
       683 +
       684 +          if args.parallel and len(filtered_sheets) > 1:
       685 +              # Parallel
       686 +              logger.info(f"Processing {len(filtered_sheets)} sheets in parallel (max
           + {args.max_parallel_sheets} workers)")
       687 +              with ThreadPoolExecutor(max_workers=args.max_parallel_sheets) as executor:
       688 +                  future_to_sheet = {
       689 +                      executor.submit(process_sheet_parallel, sheet_name, xlsx_path, out_dir, args,
           + args.engine, used_names, benchmark_results, None if len(filtered_sheets) != 1 else
           + os.path.splitext(os.path.basename(xlsx_path))[0]): sheet_name
       690 +                      for sheet_name in filtered_sheets
       691 +                  }
       692 +
       693 +                  for future in tqdm(as_completed(future_to_sheet), total=len(filtered_sheets),
           + desc="Processing sheets"):
       694 +                      sheet_name = future_to_sheet[future]
       695 +                      try:
       696 +                          result = future.result()
       697 +                          if result["status"] == "success":
       698 +                              exported += 1
       699 +                              manifest["sheets"].append(result)
       700 +                          elif result["status"] == "skipped":
       701 +                              skipped += 1
       702 +                          else:
       703 +                              logger.error(f"Error processing {sheet_name}: {result.get('error',
           + 'Unknown error')}")
       704 +                      except Exception as e:
       705 +                          logger.error(f"Exception processing {sheet_name}: {str(e)}")
       706 +          else:
       707 +              # Sequential
       708 +              for sheet_name in tqdm(filtered_sheets, desc="Exporting sheets"):
       709 +                  if args.benchmark:
       710 +                      benchmark_results = {}  # Should be list? Inconsistent with parallel
       711 +                      for eng in ['calamine', 'openpyxl']:
       712 +                          duration = benchmark_engine_performance(xlsx_path, sheet_name, eng,
           + is_detailed=True)
       713 +                          benchmark_results[eng] = duration
       714 +                          logger.info(f"Benchmark: {sheet_name} with {eng} - {json.dumps(duration)}")
       715 +                      manifest["benchmark_results"].append({"sheet_name": sheet_name, "results":
           + benchmark_results})
       716 +
       717 +                  # Load data
       718 +                  if args.engine == "openpyxl":
       719 +                      ws = wb[sheet_name]
       720 +                      max_r, max_c = _get_used_range(ws)
       721 +                      is_df = False
       722 +                  else:
       723 +                      df = pd.read_excel(xlsx_path, sheet_name=sheet_name, engine='calamine',
           + keep_default_na=False)
       724 +                      setattr(df, "_include_headers", True)
       725 +                      max_r, max_c = _get_used_range(df)
       726 +                      is_df = True
       727 +
       728 +                  if not args.include_empty and (max_r == 0 or max_c == 0):
       729 +                      skipped += 1
       730 +                      continue
       731 +
       732 +                  safe = sanitize_filename(sheet_name, used_names)
       733 +                  if len(filtered_sheets) == 1:
       734 +                      csv_name = os.path.splitext(os.path.basename(xlsx_path))[0] + ".csv"
       735 +                  else:
       736 +                      csv_name = f"{safe}.csv"
       737 +                  csv_path = os.path.join(out_dir, csv_name)
       738 +                  try:
       739 +                      @timeout_handler(300)
       740 +                      def export_with_timeout():
       741 +                          if args.engine == "openpyxl":
       742 +                              return _stream_write_csv(csv_path, _iter_rows(ws, args.visible_only,
           + args.date_format, args.datetime_format, args.time_format, args.af_columns, args.min_af_nonempty,
           + args.af_gate_visible_only, show_progress=False), args.encoding, args.bom, buffer_size=getattr(args,
           + 'buffer_size', 65536))
       743 +                          else:
       744 +                              return _stream_write_csv(csv_path, _iter_rows_pandas(df,
           + args.visible_only, args.date_format, args.datetime_format, args.time_format, args.af_columns,
           + args.min_af_nonempty, args.af_gate_visible_only), args.encoding, args.bom, buffer_size=getattr(args,
           +  'buffer_size', 65536))
       745 +
       746 +                      row_count, col_count = export_with_timeout()
       747 +                      if row_count == 0:
       748 +                          skipped += 1
       749 +                          continue
       750 +                      exported += 1
       751 +                      last7_path = None
       752 +                      if is_df and args.report_last7:
       753 +                          last7_path = _write_last7_by_reportdate_csv(df, csv_path, args.encoding,
           + args.bom)
       754 +                      manifest["sheets"].append({
       755 +                          "sheet_name": sheet_name,
       756 +                          "sanitized_name": safe,
       757 +                          "file": csv_path,
       758 +                          "file_sha256": get_file_hash(csv_path),
       759 +                          "rows": row_count,
       760 +                          "cols": col_count,
       761 +                          "last7_file": last7_path
       762 +                      })
       763 +                  except ExportTimeoutError:
       764 +                      logger.error(f"Timeout during export of {safe}")
       765 +                      raise ExportError(f"Timeout exporting {safe}")
       766 +                  except Exception as e:
       767 +                      logger.error(f"Error exporting {safe}: {str(e)}")
       768 +                      raise ExportError(f"Error exporting {safe}: {str(e)}")
       769 +
       770 +          # Benchmark summary
       771 +          if args.benchmark and manifest["benchmark_results"]:
       772 +              total_duration = 0
       773 +              total_rows = 0
       774 +              for benchmark in manifest["benchmark_results"]:
       775 +                  for engine, results in benchmark["results"].items():
       776 +                      total_duration += results.get("total_duration", 0)
       777 +                      if results.get("throughput_rows_sec", 0) > 0:
       778 +                          total_rows += results.get("throughput_rows_sec", 0) *
           + results.get("total_duration", 0)
       779 +
       780 +              manifest["benchmark_summary"] = {
       781 +                  "total_duration_seconds": round(total_duration, 3),
       782 +                  "total_rows_processed": round(total_rows, 0),
       783 +                  "average_rows_per_second": round(total_rows / total_duration, 1) if total_duration
           + > 0 else 0,
       784 +                  "sheets_benchmarked": len(manifest["benchmark_results"])
       785 +              }
       786 +
       787 +          with open(os.path.join(out_dir, "manifest.json"), "w", encoding="utf-8") as f:
       788 +              json.dump(manifest, f, indent=2)  # Write manifest
       789 +
       790 +          # --- Print statements (inside loop) ---
       791 +          print(f"--- Processed File: {os.path.basename(xlsx_path)} ---")
       792 +          print(f"Source workbook : {xlsx_path}")
       793 +          print(f"Output folder   : {out_dir}")
       794 +          print(f"Sheets exported : {exported}")
       795 +          print(f"Sheets skipped  : {skipped} (empty or no columns)")
       796 +          print(f"Manifest        : {os.path.join(out_dir, 'manifest.json')}")
       797 +          print("\n") # Add a newline for separation
       798 +          return True
       799 +
       800 +      except Exception as e:
       801 +          # If one file fails, log it and continue to the next
       802 +          logger.error(f"Failed to process file {xlsx_path_arg}: {e}")
       803 +          print(f"--- FAILED to process file: {os.path.basename(xlsx_path_arg)} ---")
       804 +          print(f"Error: {e}")
       805 +          print("\n")
       806 +          return False
       807 +
       808    def main():
       809        """
       810        Main function: Parses args, processes Excel, exports sheets, creates manifest. 811        """
       812        ap = argparse.ArgumentParser(description="Export all Excel sheets to individual CSV files.") 813 -
       813 +
       814        # --- THIS IS THE KEY CHANGE ---
       815        # It now accepts one or more files positionally, NOT with --xlsx
       816        ap.add_argument("xlsx_files", nargs='+', help="One or more paths to the source .xlsx files")
     ...
       632                        help="If set, only visible cells among --af-columns count toward the
             threshold.") 633        ap.add_argument("--disable-af-gate", action="store_true",
       634                        help="Disable A-F gate filtering (export all rows).") 635 -      ap.add_argument("--engine", default="auto", choices=["auto", "calamine", "openpyxl"],
       635 +      ap.add_argument("--engine", default="auto", choices=["auto", "calamine", "openpyxl"],
       636                        help="Engine for reading Excel files (auto selects based on file size)")
       637 -      ap.add_argument("--chunksize", type=int, default=None,
       637 +      ap.add_argument("--chunksize", type=int, default=None,
       638                        help="Read file in chunks of this size for large files (auto-enabled for 50MB+
             files)")
       639 -      ap.add_argument("--benchmark", action="store_true",
       639 +      ap.add_argument("--benchmark", action="store_true",
       640                        help="Enable performance benchmarking and include results in manifest")
       641 -      ap.add_argument("--parallel", action="store_true",
       641 +      ap.add_argument("--parallel", action="store_true",
       642                        help="Enable parallel sheet processing for better performance")
       643 -      ap.add_argument("--max-parallel-sheets", type=int, default=4,
       643 +      ap.add_argument("--max-parallel-sheets", type=int, default=4,
       644                        help="Maximum number of sheets to process in parallel (default: 4)")
       645 +      ap.add_argument("--max-parallel-files", type=int, default=2,
       646 +                      help="Maximum number of files to process in parallel (default: 2)")
       647 +      ap.add_argument("--buffer-size", type=int, default=65536,
       648 +                      help="Buffer size for CSV writing in bytes (default: 64KB)")
       649        args = ap.parse_args()
       650
       651        # Disable AF gate if flagged

  Update(export_sheets.py)

∴ Thought for 1s (ctrl+o to show thinking)

* Optimizing export_sheets.py… (esc to interrupt · ctrl+t to show todos)
  ⎿  Next: Optimize Convert_to_CSV.bat for best performance settings

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
> 
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
  ⏵⏵ accept edits on (shift+tab to cycle)

╭─── Claude Code v2.0.35 ─────────────────────────────────────────────────────────────────────────╮
│                             │ Tips for getting started                                          │
│      Welcome back RAC! │ Run /init to create a CLAUDE.md file with instructions for Claude │
│                             │ ───────────────────────────────────────────────────────────────── │
│           ▐▛███▜▌           │ Recent activity                                                   │
│          ▝▜█████▛▘          │ No recent activity                                                │
│            ▘▘ ▝▝            │                                                                   │
│                             │                                                                   │
│   Sonnet 4.5 · Claude Pro   │                                                                   │
│       C:\Dev\scripts        │                                                                   │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯

> I need to optimize my Excel-to-CSV conversion workflow for better performance. I have two files:

1. C:\Dev\scripts\Convert_to_CSV.bat - A batch file that processes Excel files and passes them to Python
2. C:\Dev\scripts\export_sheets.py - A Python script that converts Excel files to CSV

Currently, the process works but is slower than I'd like, especially with large files. Here's my current batch file:
```batch
@echo off
title Convert to CSV

REM --- Enable delayed expansion for variables ---
setlocal enabledelayedexpansion

REM --- Debug information ---
echo Current directory: %CD%
echo Raw arguments: %*
echo. REM --- Check if any files were provided ---
if "%~1"=="" (
    echo No files provided. pause
    exit /b 1
)

REM --- Build file list properly ---
set "FILE_LIST="
:process_args
if "%~1"=="" goto done_args
    set "FILE_LIST=!FILE_LIST! "%~1""
    shift
    goto process_args
:done_args

REM --- Show what we're processing ---
echo Files to process: !FILE_LIST! echo. echo Running Python script...

REM --- Run Python with the file list ---
py "C:\Dev\scripts\export_sheets.py" !FILE_LIST! --engine calamine

if %ERRORLEVEL% NEQ 0 (
    echo. echo ----------------------------------------
    echo ERROR: Python script failed with code %ERRORLEVEL%
    echo ----------------------------------------
) else (
    echo. echo ----------------------------------------
    echo All files processed successfully. echo ----------------------------------------
)

pause
exit /b
```

Here's a portion of my export_sheets.py that shows the main functions:
```python
import argparse
import os
import sys
import pandas as pd
import json
import time
import logging
from datetime import datetime
from pathlib import Path
import concurrent.futures
from tqdm import tqdm

# Global variables and setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def export_sheet(xlsx_path, sheet_name, output_dir, engine='auto', chunksize=5000):
    """Export a single sheet to CSV.""" start_time = time.time()

    try:
        # Create output filename
        clean_sheet_name = "".join(c if c.isalnum() or c in "._- " else "_" for c in sheet_name)
        clean_sheet_name = clean_sheet_name.strip()
        output_file = os.path.join(output_dir, f"{clean_sheet_name}.csv")

        # Read the Excel sheet with specified engine
        if engine == 'calamine':
            # Using calamine approach
            import polars as pl
            df = pl.read_excel(xlsx_path, sheet_name=sheet_name)

            # Skip empty sheets or sheets with no columns
            if df.shape[0] == 0 or df.shape[1] == 0:
                return False, 0, 0, f"Skipped: Empty sheet or no columns"

            # Write to CSV
            df.write_csv(output_file)
            rows, cols = df.shape
        else:
            # Using pandas/openpyxl approach
            df = pd.read_excel(
                xlsx_path,
                sheet_name=sheet_name,
                engine='openpyxl' if engine == 'openpyxl' else None
            )

            # Skip empty sheets or sheets with no columns
            if df.shape[0] == 0 or df.shape[1] == 0:
                return False, 0, 0, f"Skipped: Empty sheet or no columns"

            # Write to CSV
            df.to_csv(output_file, index=False)
            rows, cols = df.shape

        duration = time.time() - start_time
        return True, rows, cols, f"Exported: {rows} rows, {cols} columns in {duration:.2f}s"

    except Exception as e:
        return False, 0, 0, f"Error: {str(e)}"

def get_sheet_names(xlsx_path, engine='auto'):
    """Get all sheet names from an Excel file.""" try:
        if engine == 'calamine':
            import polars as pl
            return pl.read_excel_schema(xlsx_path)
        else:
            return pd.ExcelFile(xlsx_path, engine='openpyxl' if engine == 'openpyxl' else None).sheet_names
    except Exception as e:
        logger.error(f"Failed to get sheet names: {e}")
        return []

def process_workbook(xlsx_path, engine='auto', chunksize=5000):
    """Process a whole Excel workbook and export all sheets.""" # Create output directory
    base_name = os.path.splitext(os.path.basename(xlsx_path))[0]
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M")
    output_dir = f"{os.path.dirname(xlsx_path)}/{base_name}_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)

    # Get all sheet names
    sheet_names = get_sheet_names(xlsx_path, engine)
    if not sheet_names:
        logger.error(f"No sheets found or error reading workbook: {xlsx_path}")
        return False

    # Process each sheet
    results = {}
    skipped = 0

    for sheet_name in tqdm(sheet_names, desc="Exporting sheets"):
        success, rows, cols, message = export_sheet(xlsx_path, sheet_name, output_dir, engine, chunksize)
        results[sheet_name] = {
            "success": success,
            "rows": rows,
            "columns": cols,
            "message": message
        }
        if not success:
            skipped += 1

    # Create manifest
    manifest = {
        "source": xlsx_path,
        "exported_at": datetime.now().isoformat(),
        "output_directory": output_dir,
        "sheets": results
    }

    with open(os.path.join(output_dir, "manifest.json"), "w") as f:
        json.dump(manifest, f, indent=2)

    # Report results
    print(f"--- Processed File: {os.path.basename(xlsx_path)} ---")
    print(f"Source workbook : {xlsx_path}")
    print(f"Output folder   : {output_dir}")
    print(f"Sheets exported : {len(sheet_names) - skipped}")
    print(f"Sheets skipped  : {skipped} (empty or no columns)")
    print(f"Manifest        : {os.path.join(output_dir, 'manifest.json')}")

    return True

def main():
    ap = argparse.ArgumentParser(description="Export all Excel sheets to individual CSV files.") ap.add_argument("xlsx_files", nargs='+', help="One or more paths to the source .xlsx files")
    ap.add_argument("--engine", default="auto", choices=["auto", "calamine", "openpyxl"],
                    help="Engine for reading Excel files")
    ap.add_argument("--chunksize", type=int, default=5000,
                    help="Chunk size for processing large files")

    args = ap.parse_args()

    # Check available memory and adjust chunksize if needed
    import psutil
    available_memory_mb = psutil.virtual_memory().available / (1024 * 1024)
    logger.info(f"Available memory: {available_memory_mb:.1f}MB")

    # Process each file
    for xlsx_path_arg in args.xlsx_files:
        try:
            xlsx_path = os.path.abspath(xlsx_path_arg)
            logger.info(f"Processing {os.path.basename(xlsx_path)} - Using engine: {args.engine}, chunksize:
{args.chunksize}, available memory: {available_memory_mb:.1f}MB")
            process_workbook(xlsx_path, args.engine, args.chunksize)
        except Exception as e:
            logger.error(f"Failed to process file: {os.path.basename(xlsx_path_arg)}")
            logger.error(f"Error: {e}")

if __name__ == "__main__":
    main()
```

Please enhance both files for maximum performance:

1. For the batch file (Convert_to_CSV.bat):
   - Optimize the command-line arguments to use the best performance settings
   - Keep the same basic structure and error handling
   - Don't change how it processes file arguments

2.